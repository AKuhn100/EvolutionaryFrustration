{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ee163b",
   "metadata": {},
   "source": [
    "# Proviz 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820eb570",
   "metadata": {},
   "source": [
    "This jupyter notebook contains all of the code used to process evolutionary coupling scores generated by EVcouplings (https://github.com/debbiemarkslab/EVcouplings) for the calculation of evolutionary frustration and comparison of evolutionary frustration to structure based frustration derived from AlphaFold predicted structures and/or experimental structures from the RCSB PDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0d1b2",
   "metadata": {},
   "source": [
    "Given a directory with a PDB ID labeled Fasta File for a monomeric RCSB PDB entry, pull the biological assembly, filter down to the monomer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3635adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pymolPy3\n",
    "\n",
    "# Initialize pymolPy3\n",
    "pm = pymolPy3.pymolPy3(0)  # Launch PyMOL without GUI\n",
    "\n",
    "def fetch_pdb_id_from_fasta(fasta_path):\n",
    "    \"\"\"Extract PDB ID from FASTA file.\"\"\"\n",
    "    if not os.path.exists(fasta_path):\n",
    "        print(f\"FASTA file not found: {fasta_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(fasta_path, 'r') as fasta_file:\n",
    "            for line in fasta_file:\n",
    "                if line.startswith('>'):\n",
    "                    parts = line.strip().split('|')\n",
    "                    if len(parts) > 0:\n",
    "                        pdb_id = parts[0][1:].split('_')[0]  # Remove '>' and extract the part before '_'\n",
    "                        return pdb_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading FASTA file: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_pdb_file(pdb_id, assembly_directory):\n",
    "    \"\"\"Download the biological assembly PDB file.\"\"\"\n",
    "    print(f\"Downloading PDB file for PDB ID: {pdb_id}\")\n",
    "    os.makedirs(assembly_directory, exist_ok=True)\n",
    "\n",
    "    biological_assembly_url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "    biological_assembly_path = os.path.join(assembly_directory, f\"{pdb_id}_biological_assembly.pdb\")\n",
    "    response = requests.get(biological_assembly_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(biological_assembly_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Biological assembly PDB downloaded to {biological_assembly_path}\")\n",
    "        return biological_assembly_path\n",
    "    else:\n",
    "        print(f\"Failed to download biological assembly PDB file for {pdb_id}. HTTP Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_monomer(biological_assembly_path, monomer_output_path):\n",
    "    \"\"\"Extract the monomer from the biological assembly using the available chain.\"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting monomer from biological assembly: {biological_assembly_path}\")\n",
    "        \n",
    "        # First, try to find the chain ID from the COMPND record\n",
    "        compnd_chain = None\n",
    "        with open(biological_assembly_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"COMPND\"):\n",
    "                    if \"CHAIN:\" in line:\n",
    "                        # Extract chain ID after \"CHAIN:\"\n",
    "                        chain_part = line.split(\"CHAIN:\")[1].strip()\n",
    "                        # Remove any trailing semicolon and whitespace\n",
    "                        chain_id = chain_part.rstrip(';').strip()\n",
    "                        compnd_chain = chain_id\n",
    "                        print(f\"Found chain {chain_id} in COMPND record\")\n",
    "                        break\n",
    "                if line.startswith(\"ATOM\"):  # Stop reading if we hit ATOM records\n",
    "                    break\n",
    "        \n",
    "        # Load structure into PyMOL\n",
    "        pm(f\"load {biological_assembly_path}\")\n",
    "        pm(\"remove not polymer.protein\")  # Remove everything that's not a protein polymer\n",
    "        pm(\"remove resn HOH\")  # Remove water molecules\n",
    "        \n",
    "        # Create a selection of all protein atoms\n",
    "        pm(\"select protein_chains, polymer.protein\")\n",
    "        \n",
    "        # Get all chains from structure\n",
    "        chain_identifiers = set()\n",
    "        with open(biological_assembly_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"ATOM  \") or line.startswith(\"HETATM\"):\n",
    "                    chain_id = line[21]  # Chain identifier is in column 22\n",
    "                    if chain_id.strip():  # Only add non-empty chain IDs\n",
    "                        chain_identifiers.add(chain_id)\n",
    "        \n",
    "        print(f\"Found chains in structure: {sorted(chain_identifiers)}\")\n",
    "        \n",
    "        if not chain_identifiers:\n",
    "            print(\"No protein chains found in the structure\")\n",
    "            return\n",
    "            \n",
    "        # Select chain based on priority:\n",
    "        # 1. Use COMPND chain if it exists in structure\n",
    "        # 2. Otherwise use first alphabetical chain\n",
    "        if compnd_chain and compnd_chain in chain_identifiers:\n",
    "            selected_chain = compnd_chain\n",
    "        else:\n",
    "            selected_chain = sorted(chain_identifiers)[0]\n",
    "            \n",
    "        print(f\"Selected chain {selected_chain} for monomer extraction\")\n",
    "        \n",
    "        # Select and save the chosen chain\n",
    "        pm(f\"select monomer, chain {selected_chain}\")\n",
    "        pm(f\"save {monomer_output_path}, monomer\")\n",
    "        pm(\"delete monomer\")\n",
    "        pm(\"delete protein_chains\")\n",
    "        pm(\"delete all\")\n",
    "        print(f\"Monomer extracted and saved to {monomer_output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting monomer for {biological_assembly_path}: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    base_directory = \"\" #Path to directory containing the directory with the fasta file. Can you be used for multiple subdirectories simultaneously.\n",
    "\n",
    "    # Iterate through each protein directory in base_directory\n",
    "    for protein_dir in os.listdir(base_directory):\n",
    "        protein_path = os.path.join(base_directory, protein_dir)\n",
    "        if os.path.isdir(protein_path):  # Ensure it's a directory\n",
    "            fasta_file = os.path.join(protein_path, f\"{protein_dir}.fasta\")\n",
    "            experimental_data_dir = os.path.join(protein_path, \"experimental_data\")\n",
    "            monomer_output_path = os.path.join(experimental_data_dir, \"monomer.pdb\")\n",
    "\n",
    "            # Process each FASTA file\n",
    "            pdb_id = fetch_pdb_id_from_fasta(fasta_file)\n",
    "            if pdb_id:\n",
    "                biological_assembly_path = download_pdb_file(pdb_id, experimental_data_dir)\n",
    "                if biological_assembly_path:\n",
    "                    extract_monomer(biological_assembly_path, monomer_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813eeea",
   "metadata": {},
   "source": [
    "Calculate average B-factor per Residue for filtered monomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import Bio.PDB\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_b_factors(pdb_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Calculate the average B-factors for each residue in the PDB structure and save them to a file,\n",
    "    including the one-letter residue name, indexed starting from 1.\n",
    "\n",
    "    Parameters:\n",
    "    - pdb_path: str, path to the input PDB file.\n",
    "    - output_txt_path: str, path to the output text file.\n",
    "    \"\"\"\n",
    "    # Mapping three-letter residue names to one-letter codes\n",
    "    one_letter_code = {\n",
    "        \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "        \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "        \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "        \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\",\n",
    "        # Handle uncommon residues with a placeholder\n",
    "        \"UNK\": \"X\"\n",
    "    }\n",
    "\n",
    "    parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure(\"protein\", pdb_path)\n",
    "        b_factors = []\n",
    "        residue_names = []\n",
    "\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    res_id = residue.get_id()[1]\n",
    "                    res_name = residue.get_resname()\n",
    "                    b_factor_list = [atom.get_bfactor() for atom in residue]\n",
    "                    average_b_factor = np.mean(b_factor_list)\n",
    "                    b_factors.append(average_b_factor)\n",
    "                    residue_names.append(one_letter_code.get(res_name, \"X\"))  # Default to 'X' for unknown residues\n",
    "\n",
    "        # Write the output file with re-indexed residue numbers starting from 1\n",
    "        with open(output_txt_path, 'w') as file:\n",
    "            file.write(\"Residue\\tResidueAA\\tAverage_B_Factor\\n\")\n",
    "            for idx, (aa, b_factor) in enumerate(zip(residue_names, b_factors), start=1):\n",
    "                file.write(f\"{idx}\\t{aa}\\t{b_factor:.3f}\\n\")\n",
    "\n",
    "        print(f\"Average B-factors with residue names saved to {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDB file {pdb_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_directory = \"\" #Path to directory containing the directory with the fasta file. Can you be used for multiple subdirectories simultaneously.\n",
    "\n",
    "    # Iterate through each protein directory in base_directory\n",
    "    for protein_dir in os.listdir(base_directory):\n",
    "        protein_path = os.path.join(base_directory, protein_dir)\n",
    "        if os.path.isdir(protein_path):  # Ensure it's a directory\n",
    "            experimental_data_dir = os.path.join(protein_path, \"experimental_data\")\n",
    "            pdb_path = os.path.join(experimental_data_dir, \"monomer.pdb\")\n",
    "            output_txt_path = os.path.join(experimental_data_dir, \"average_b_factors.txt\")\n",
    "\n",
    "            # Ensure the monomer PDB file exists before processing\n",
    "            if os.path.exists(pdb_path):\n",
    "                calculate_average_b_factors(pdb_path, output_txt_path)\n",
    "            else:\n",
    "                print(f\"Monomer PDB file not found: {pdb_path}. Skipping {protein_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c536fb",
   "metadata": {},
   "source": [
    "Pull AlphaFold structure for uniprot ID corresponding to the PDB ID in the input fasta file, trim to relevant section via pairwise sequence alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, pairwise2\n",
    "from Bio.PDB import *\n",
    "\n",
    "def get_pdb_directories(root_path):\n",
    "    \"\"\"Return a list of directories that contain FASTA files.\"\"\"\n",
    "    valid_dirs = []\n",
    "    \n",
    "    try:\n",
    "        for entry in os.scandir(root_path):\n",
    "            if entry.is_dir():\n",
    "                fasta_files = [f for f in os.scandir(entry.path) if f.name.endswith('.fasta')]\n",
    "                if len(fasta_files) == 1:\n",
    "                    valid_dirs.append(entry.path)\n",
    "                elif len(fasta_files) > 1:\n",
    "                    print(f\"[WARNING] Directory {entry.path} contains multiple FASTA files - skipping\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to scan directory {root_path}: {e}\")\n",
    "        return []\n",
    "        \n",
    "    return sorted(valid_dirs)\n",
    "\n",
    "def get_sequence_from_pdb(pdb_path):\n",
    "    \"\"\"Extract sequence from PDB file using Bio.PDB\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('structure', pdb_path)\n",
    "        ppb = PPBuilder()\n",
    "        seq = \"\"\n",
    "        for pp in ppb.build_peptides(structure):\n",
    "            seq += str(pp.get_sequence())\n",
    "        return seq\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to extract sequence from {pdb_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_matching_residues(exp_seq, af_seq):\n",
    "    \"\"\"Find residue positions that match between sequences.\"\"\"\n",
    "    alignment = pairwise2.align.globalxx(exp_seq, af_seq)[0]\n",
    "    exp_aligned, af_aligned = alignment[0], alignment[1]\n",
    "    \n",
    "    matching_positions = []\n",
    "    exp_pos = 0\n",
    "    af_pos = 0\n",
    "    \n",
    "    for i in range(len(exp_aligned)):\n",
    "        if exp_aligned[i] != '-' and af_aligned[i] != '-':\n",
    "            if exp_aligned[i] == af_aligned[i]:\n",
    "                matching_positions.append((exp_pos + 1, af_pos + 1))\n",
    "        \n",
    "        if exp_aligned[i] != '-':\n",
    "            exp_pos += 1\n",
    "        if af_aligned[i] != '-':\n",
    "            af_pos += 1\n",
    "    \n",
    "    return matching_positions\n",
    "\n",
    "def parse_pdb_and_chain(fasta_file_path):\n",
    "    \"\"\"\n",
    "    Parse PDB code & chain from the FASTA header.\n",
    "    Expected formats:\n",
    "    >4AKE_1|Chains A, B|ADENYLATE KINASE|Escherichia coli (562)\n",
    "    >pdb|4AKE|A Chain A, molecule 1|ADENYLATE KINASE|Escherichia coli\n",
    "    >1234_1|Chain A|Description\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(fasta_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\">\"):\n",
    "                    header = line.strip()[1:]\n",
    "                    print(f\"[DEBUG] Parsing FASTA header: {header}\")\n",
    "                    \n",
    "                    parts = [p.strip() for p in header.split(\"|\")]\n",
    "                    \n",
    "                    # Try to find PDB code\n",
    "                    pdb_code = None\n",
    "                    if parts[0]:\n",
    "                        pdb_match = re.search(r'([0-9][A-Za-z0-9]{3})(?:_|$)', parts[0])\n",
    "                        if pdb_match:\n",
    "                            pdb_code = pdb_match.group(1).upper()\n",
    "                    \n",
    "                    # Try to find chain\n",
    "                    chain = None\n",
    "                    for part in parts:\n",
    "                        chain_match = re.search(r'Chain[s]?\\s+([A-Za-z])(?:\\s|$|,)', part)\n",
    "                        if chain_match:\n",
    "                            chain = chain_match.group(1).upper()\n",
    "                            break\n",
    "                    \n",
    "                    if pdb_code:\n",
    "                        print(f\"[INFO] Found PDB: {pdb_code}, Chain: {chain}\")\n",
    "                        return (pdb_code, chain)\n",
    "                    else:\n",
    "                        raise ValueError(\"Could not find valid PDB code in FASTA header\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to parse FASTA file {fasta_file_path}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    raise ValueError(\"No FASTA header found in file\")\n",
    "\n",
    "def get_uniprot_id_for_pdb_chain(pdb_code, chain):\n",
    "    \"\"\"Query the PDBe SIFTS API to find the UniProt ID.\"\"\"\n",
    "    url = f\"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot/{pdb_code.lower()}\"\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        data = resp.json()\n",
    "        if pdb_code.lower() not in data:\n",
    "            raise ValueError(f\"No data returned from SIFTS for PDB {pdb_code}\")\n",
    "            \n",
    "        sifts_info = data[pdb_code.lower()]\n",
    "        if \"UniProt\" not in sifts_info or not sifts_info[\"UniProt\"]:\n",
    "            raise ValueError(f\"No UniProt mappings found for {pdb_code}\")\n",
    "            \n",
    "        for uniprot_id, uniprot_dict in sifts_info[\"UniProt\"].items():\n",
    "            chain_mappings = uniprot_dict.get(\"mappings\", [])\n",
    "            for mapping in chain_mappings:\n",
    "                if chain is None or mapping.get(\"chain_id\", \"\") == chain:\n",
    "                    return uniprot_id\n",
    "                    \n",
    "        raise ValueError(f\"Could not find UniProt ID for {pdb_code} chain {chain}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] SIFTS API request failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_alphafold_structure(uniprot_id, output_pdb_path):\n",
    "    \"\"\"Download structure from AlphaFold DB.\"\"\"\n",
    "    url = f\"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v4.pdb\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if \"ATOM\" not in response.text:\n",
    "            raise ValueError(\"Downloaded file does not appear to be a valid PDB\")\n",
    "            \n",
    "        with open(output_pdb_path, 'w') as f:\n",
    "            f.write(response.text)\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to download AlphaFold structure: {e}\")\n",
    "        raise\n",
    "\n",
    "def trim_alphafold_structure(exp_pdb_path, af_pdb_path, output_path, pymol=None):\n",
    "    \"\"\"Trim AlphaFold structure to match experimental sequence.\"\"\"\n",
    "    exp_seq = get_sequence_from_pdb(exp_pdb_path)\n",
    "    af_seq = get_sequence_from_pdb(af_pdb_path)\n",
    "    \n",
    "    if not exp_seq or not af_seq:\n",
    "        return False\n",
    "        \n",
    "    matching_positions = find_matching_residues(exp_seq, af_seq)\n",
    "    if not matching_positions:\n",
    "        print(\"[WARNING] No matching residues found between sequences\")\n",
    "        return False\n",
    "        \n",
    "    if pymol:\n",
    "        try:\n",
    "            af_residues = [af_pos for _, af_pos in matching_positions]\n",
    "            resi_str = \"+\".join(str(pos) for pos in af_residues)\n",
    "            \n",
    "            pymol(f\"load {af_pdb_path}, af_struct\")\n",
    "            pymol(f\"select matching_residues, af_struct and resi {resi_str}\")\n",
    "            pymol(f\"save {output_path}, matching_residues\")\n",
    "            pymol(\"delete all\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] PyMOL trimming failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"[WARNING] PyMOL not available - skipping structure trimming\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Try to import PyMOL\n",
    "    try:\n",
    "        import pymolPy3\n",
    "        pm = pymolPy3.pymolPy3(0)\n",
    "        print(\"[INFO] PyMOL available - will perform structure trimming\")\n",
    "    except ImportError:\n",
    "        pm = None\n",
    "        print(\"[WARNING] PyMOL not available - structure trimming will be skipped\")\n",
    "    \n",
    "    root_directory = input(\"Enter the root directory path: \")\n",
    "    if not os.path.isdir(root_directory):\n",
    "        print(f\"[ERROR] Invalid directory path: {root_directory}\")\n",
    "        return\n",
    "        \n",
    "    pdb_dirs = get_pdb_directories(root_directory)\n",
    "    if not pdb_dirs:\n",
    "        print(\"[ERROR] No valid directories with FASTA files found\")\n",
    "        return\n",
    "\n",
    "    for pdb_dir in pdb_dirs:\n",
    "        print(f\"\\n[INFO] Processing: {pdb_dir}\")\n",
    "        \n",
    "        try:\n",
    "            fasta_files = [f for f in os.scandir(pdb_dir) if f.name.endswith('.fasta')]\n",
    "            if not fasta_files:  # This shouldn't happen due to our directory validation\n",
    "                continue\n",
    "            \n",
    "            fasta_path = fasta_files[0].path\n",
    "            print(f\"\\n[DEBUG] Processing FASTA file: {fasta_path}\")\n",
    "            \n",
    "            pdb_code, chain = parse_pdb_and_chain(fasta_path)\n",
    "            print(f\"[INFO] Working with PDB code: {pdb_code}, chain: {chain}\")\n",
    "            \n",
    "            # Set up output directories\n",
    "            exp_data_dir = os.path.join(pdb_dir, \"experimental_data\")\n",
    "            alphafold_dir = os.path.join(pdb_dir, \"alphafold_structure\")\n",
    "            frustratometer_dir = os.path.join(pdb_dir, \"frustratometer_af\")\n",
    "            \n",
    "            os.makedirs(alphafold_dir, exist_ok=True)\n",
    "            os.makedirs(frustratometer_dir, exist_ok=True)\n",
    "            \n",
    "            # Check for experimental structure\n",
    "            exp_pdb_path = os.path.join(exp_data_dir, \"monomer.pdb\")\n",
    "            has_exp_structure = os.path.exists(exp_pdb_path)\n",
    "            \n",
    "            # Get UniProt ID and download AlphaFold structure\n",
    "            uniprot_id = get_uniprot_id_for_pdb_chain(pdb_code, chain)\n",
    "            full_af_path = os.path.join(alphafold_dir, \"AF_structure_full.pdb\")\n",
    "            \n",
    "            if download_alphafold_structure(uniprot_id, full_af_path):\n",
    "                # Determine which structure to use for frustratometer\n",
    "                if has_exp_structure and pm:\n",
    "                    trimmed_af_path = os.path.join(alphafold_dir, \"AF_structure_trimmed.pdb\")\n",
    "                    if trim_alphafold_structure(exp_pdb_path, full_af_path, trimmed_af_path, pm):\n",
    "                        analysis_pdb = trimmed_af_path\n",
    "                    else:\n",
    "                        analysis_pdb = full_af_path\n",
    "                else:\n",
    "                    analysis_pdb = full_af_path\n",
    "                \n",
    "                print(f\"[INFO] Successfully processed {pdb_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {pdb_dir}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if pm:\n",
    "        pm(\"quit\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b783",
   "metadata": {},
   "source": [
    "Calculate the Evolutionary coupling based energy difference between the native sequence and each single amino acid substitution mutant(Parallelized Version Exists for Larger Proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "# Load the protein sequence from a fasta file\n",
    "def load_fasta(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        for record in SeqIO.parse(file, \"fasta\"):\n",
    "            return str(record.seq)\n",
    "\n",
    "# Load the coupling scores from the provided file\n",
    "def load_coupling_scores(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Normalize cn between 0 and 1\n",
    "    cn_min = df['cn'].min()\n",
    "    cn_max = df['cn'].max()\n",
    "    df['cn_normalized'] = (df['cn'] - cn_min) / (cn_max - cn_min)\n",
    "    \n",
    "    # Multiply cn by its respective probability\n",
    "    df['weighted_cn'] = df['cn_normalized'] * df['probability']\n",
    "    \n",
    "    coupling_scores = {}\n",
    "    for _, row in df.iterrows():\n",
    "        i, A_i, j, A_j, weighted_cn = int(row['i']), row['A_i'], int(row['j']), row['A_j'], row['weighted_cn']\n",
    "        coupling_scores[(i, j)] = weighted_cn\n",
    "        coupling_scores[(j, i)] = weighted_cn  # Ensure symmetry\n",
    "    return coupling_scores, df\n",
    "\n",
    "\n",
    "# Define the MJ coupling score matrix scaffold\n",
    "amino_acids = ['C', 'M', 'F', 'I', 'L', 'V', 'W', 'Y', 'A', 'G', 'T', 'S', 'N', 'Q', 'D', 'E', 'H', 'R', 'K', 'P']\n",
    "\n",
    "# Initialize the MJ matrix (example, fill with actual data)\n",
    "mj_matrix = {(aa1, aa2): 0.0 for aa1 in amino_acids for aa2 in amino_acids}\n",
    "\n",
    "# Fill in the MJ matrix \n",
    "mj_matrix[('C', 'C')] = -5.44\n",
    "mj_matrix[('C', 'M')] = -4.99\n",
    "mj_matrix[('C', 'F')] = -5.80\n",
    "mj_matrix[('C', 'I')] = -5.50\n",
    "mj_matrix[('C', 'L')] = -5.83\n",
    "mj_matrix[('C', 'V')] = -4.96\n",
    "mj_matrix[('C', 'W')] = -4.95\n",
    "mj_matrix[('C', 'Y')] = -4.16\n",
    "mj_matrix[('C', 'A')] = -3.57\n",
    "mj_matrix[('C', 'G')] = -3.16\n",
    "mj_matrix[('C', 'T')] = -3.11\n",
    "mj_matrix[('C', 'S')] = -2.86\n",
    "mj_matrix[('C', 'N')] = -2.59\n",
    "mj_matrix[('C', 'Q')] = -2.85\n",
    "mj_matrix[('C', 'D')] = -2.41\n",
    "mj_matrix[('C', 'E')] = -2.27\n",
    "mj_matrix[('C', 'H')] = -3.60\n",
    "mj_matrix[('C', 'R')] = -2.57\n",
    "mj_matrix[('C', 'K')] = -1.95\n",
    "mj_matrix[('C', 'P')] = -3.07\n",
    "\n",
    "mj_matrix[('M', 'M')] = -5.46\n",
    "mj_matrix[('M', 'F')] = -6.56\n",
    "mj_matrix[('M', 'I')] = -6.02\n",
    "mj_matrix[('M', 'L')] = -6.41\n",
    "mj_matrix[('M', 'V')] = -5.32\n",
    "mj_matrix[('M', 'W')] = -5.55\n",
    "mj_matrix[('M', 'Y')] = -4.91\n",
    "mj_matrix[('M', 'A')] = -3.94\n",
    "mj_matrix[('M', 'G')] = -3.39\n",
    "mj_matrix[('M', 'T')] = -3.51\n",
    "mj_matrix[('M', 'S')] = -3.03\n",
    "mj_matrix[('M', 'N')] = -2.95\n",
    "mj_matrix[('M', 'Q')] = -3.30\n",
    "mj_matrix[('M', 'D')] = -2.57\n",
    "mj_matrix[('M', 'E')] = -2.89\n",
    "mj_matrix[('M', 'H')] = -3.98\n",
    "mj_matrix[('M', 'R')] = -3.12\n",
    "mj_matrix[('M', 'K')] = -2.48\n",
    "mj_matrix[('M', 'P')] = -3.45\n",
    "\n",
    "mj_matrix[('F', 'F')] = -7.26\n",
    "mj_matrix[('F', 'I')] = -6.84\n",
    "mj_matrix[('F', 'L')] = -7.28\n",
    "mj_matrix[('F', 'V')] = -6.29\n",
    "mj_matrix[('F', 'W')] = -6.16\n",
    "mj_matrix[('F', 'Y')] = -5.66\n",
    "mj_matrix[('F', 'A')] = -4.81\n",
    "mj_matrix[('F', 'G')] = -4.13\n",
    "mj_matrix[('F', 'T')] = -4.28\n",
    "mj_matrix[('F', 'S')] = -4.02\n",
    "mj_matrix[('F', 'N')] = -3.75\n",
    "mj_matrix[('F', 'Q')] = -4.10\n",
    "mj_matrix[('F', 'D')] = -3.48\n",
    "mj_matrix[('F', 'E')] = -3.56\n",
    "mj_matrix[('F', 'H')] = -4.77\n",
    "mj_matrix[('F', 'R')] = -3.98\n",
    "mj_matrix[('F', 'K')] = -3.36\n",
    "mj_matrix[('F', 'P')] = -4.25\n",
    "\n",
    "mj_matrix[('I', 'I')] = -6.54\n",
    "mj_matrix[('I', 'L')] = -7.04\n",
    "mj_matrix[('I', 'V')] = -6.05\n",
    "mj_matrix[('I', 'W')] = -5.78\n",
    "mj_matrix[('I', 'Y')] = -5.25\n",
    "mj_matrix[('I', 'A')] = -4.58\n",
    "mj_matrix[('I', 'G')] = -3.78\n",
    "mj_matrix[('I', 'T')] = -4.03\n",
    "mj_matrix[('I', 'S')] = -3.52\n",
    "mj_matrix[('I', 'N')] = -3.24\n",
    "mj_matrix[('I', 'Q')] = -3.67\n",
    "mj_matrix[('I', 'D')] = -3.17\n",
    "mj_matrix[('I', 'E')] = -3.27\n",
    "mj_matrix[('I', 'H')] = -4.14\n",
    "mj_matrix[('I', 'R')] = -3.63\n",
    "mj_matrix[('I', 'K')] = -3.01\n",
    "mj_matrix[('I', 'P')] = -3.76\n",
    "\n",
    "mj_matrix[('L', 'L')] = -7.37\n",
    "mj_matrix[('L', 'V')] = -6.48\n",
    "mj_matrix[('L', 'W')] = -6.14\n",
    "mj_matrix[('L', 'Y')] = -5.67\n",
    "mj_matrix[('L', 'A')] = -4.91\n",
    "mj_matrix[('L', 'G')] = -4.16\n",
    "mj_matrix[('L', 'T')] = -4.34\n",
    "mj_matrix[('L', 'S')] = -3.92\n",
    "mj_matrix[('L', 'N')] = -3.74\n",
    "mj_matrix[('L', 'Q')] = -4.04\n",
    "mj_matrix[('L', 'D')] = -3.40\n",
    "mj_matrix[('L', 'E')] = -3.59\n",
    "mj_matrix[('L', 'H')] = -4.54\n",
    "mj_matrix[('L', 'R')] = -4.03\n",
    "mj_matrix[('L', 'K')] = -3.37\n",
    "mj_matrix[('L', 'P')] = -4.20\n",
    "\n",
    "mj_matrix[('V', 'V')] = -5.52\n",
    "mj_matrix[('V', 'W')] = -5.18\n",
    "mj_matrix[('V', 'Y')] = -4.62\n",
    "mj_matrix[('V', 'A')] = -4.04\n",
    "mj_matrix[('V', 'G')] = -3.38\n",
    "mj_matrix[('V', 'T')] = -3.46\n",
    "mj_matrix[('V', 'S')] = -3.05\n",
    "mj_matrix[('V', 'N')] = -2.83\n",
    "mj_matrix[('V', 'Q')] = -3.07\n",
    "mj_matrix[('V', 'D')] = -2.48\n",
    "mj_matrix[('V', 'E')] = -2.67\n",
    "mj_matrix[('V', 'H')] = -3.58\n",
    "mj_matrix[('V', 'R')] = -3.07\n",
    "mj_matrix[('V', 'K')] = -2.49\n",
    "mj_matrix[('V', 'P')] = -3.32\n",
    "\n",
    "mj_matrix[('W', 'W')] = -5.06\n",
    "mj_matrix[('W', 'Y')] = -4.66\n",
    "mj_matrix[('W', 'A')] = -3.82\n",
    "mj_matrix[('W', 'G')] = -3.42\n",
    "mj_matrix[('W', 'T')] = -3.22\n",
    "mj_matrix[('W', 'S')] = -2.99\n",
    "mj_matrix[('W', 'N')] = -3.07\n",
    "mj_matrix[('W', 'Q')] = -3.11\n",
    "mj_matrix[('W', 'D')] = -2.84\n",
    "mj_matrix[('W', 'E')] = -2.99\n",
    "mj_matrix[('W', 'H')] = -3.98\n",
    "mj_matrix[('W', 'R')] = -3.41\n",
    "mj_matrix[('W', 'K')] = -2.69\n",
    "mj_matrix[('W', 'P')] = -3.73\n",
    "\n",
    "mj_matrix[('Y', 'Y')] = -4.17\n",
    "mj_matrix[('Y', 'A')] = -3.36\n",
    "mj_matrix[('Y', 'G')] = -3.01\n",
    "mj_matrix[('Y', 'T')] = -3.01\n",
    "mj_matrix[('Y', 'S')] = -2.78\n",
    "mj_matrix[('Y', 'N')] = -2.76\n",
    "mj_matrix[('Y', 'Q')] = -2.97\n",
    "mj_matrix[('Y', 'D')] = -2.76\n",
    "mj_matrix[('Y', 'E')] = -2.79\n",
    "mj_matrix[('Y', 'H')] = -3.52\n",
    "mj_matrix[('Y', 'R')] = -3.16\n",
    "mj_matrix[('Y', 'K')] = -2.60\n",
    "mj_matrix[('Y', 'P')] = -3.19\n",
    "\n",
    "mj_matrix[('A', 'A')] = -2.72\n",
    "mj_matrix[('A', 'G')] = -2.31\n",
    "mj_matrix[('A', 'T')] = -2.32\n",
    "mj_matrix[('A', 'S')] = -2.01\n",
    "mj_matrix[('A', 'N')] = -1.84\n",
    "mj_matrix[('A', 'Q')] = -1.89\n",
    "mj_matrix[('A', 'D')] = -1.70\n",
    "mj_matrix[('A', 'E')] = -1.51\n",
    "mj_matrix[('A', 'H')] = -2.41\n",
    "mj_matrix[('A', 'R')] = -1.83\n",
    "mj_matrix[('A', 'K')] = -1.31\n",
    "mj_matrix[('A', 'P')] = -2.03\n",
    "\n",
    "mj_matrix[('G', 'G')] = -2.24\n",
    "mj_matrix[('G', 'T')] = -2.08\n",
    "mj_matrix[('G', 'S')] = -1.82\n",
    "mj_matrix[('G', 'N')] = -1.74\n",
    "mj_matrix[('G', 'Q')] = -1.66\n",
    "mj_matrix[('G', 'D')] = -1.59\n",
    "mj_matrix[('G', 'E')] = -1.22\n",
    "mj_matrix[('G', 'H')] = -2.15\n",
    "mj_matrix[('G', 'R')] = -1.72\n",
    "mj_matrix[('G', 'K')] = -1.15\n",
    "mj_matrix[('G', 'P')] = -1.87\n",
    "\n",
    "mj_matrix[('T', 'T')] = -2.12\n",
    "mj_matrix[('T', 'S')] = -1.96\n",
    "mj_matrix[('T', 'N')] = -1.88\n",
    "mj_matrix[('T', 'Q')] = -1.90\n",
    "mj_matrix[('T', 'D')] = -1.80\n",
    "mj_matrix[('T', 'E')] = -1.74\n",
    "mj_matrix[('T', 'H')] = -2.42\n",
    "mj_matrix[('T', 'R')] = -1.90\n",
    "mj_matrix[('T', 'K')] = -1.31\n",
    "mj_matrix[('T', 'P')] = -1.90\n",
    "\n",
    "mj_matrix[('S', 'S')] = -1.67\n",
    "mj_matrix[('S', 'N')] = -1.58\n",
    "mj_matrix[('S', 'Q')] = -1.49\n",
    "mj_matrix[('S', 'D')] = -1.63\n",
    "mj_matrix[('S', 'E')] = -1.48\n",
    "mj_matrix[('S', 'H')] = -2.11\n",
    "mj_matrix[('S', 'R')] = -1.62\n",
    "mj_matrix[('S', 'K')] = -1.05\n",
    "mj_matrix[('S', 'P')] = -1.57\n",
    "\n",
    "mj_matrix[('N', 'N')] = -1.68\n",
    "mj_matrix[('N', 'Q')] = -1.71\n",
    "mj_matrix[('N', 'D')] = -1.68\n",
    "mj_matrix[('N', 'E')] = -1.51\n",
    "mj_matrix[('N', 'H')] = -2.08\n",
    "mj_matrix[('N', 'R')] = -1.64\n",
    "mj_matrix[('N', 'K')] = -1.21\n",
    "mj_matrix[('N', 'P')] = -1.53\n",
    "\n",
    "mj_matrix[('Q', 'Q')] = -1.54\n",
    "mj_matrix[('Q', 'D')] = -1.46\n",
    "mj_matrix[('Q', 'E')] = -1.42\n",
    "mj_matrix[('Q', 'H')] = -1.98\n",
    "mj_matrix[('Q', 'R')] = -1.80\n",
    "mj_matrix[('Q', 'K')] = -1.29\n",
    "mj_matrix[('Q', 'P')] = -1.73\n",
    "\n",
    "mj_matrix[('D', 'D')] = -1.21\n",
    "mj_matrix[('D', 'E')] = -1.02\n",
    "mj_matrix[('D', 'H')] = -2.32\n",
    "mj_matrix[('D', 'R')] = -2.29\n",
    "mj_matrix[('D', 'K')] = -1.68\n",
    "mj_matrix[('D', 'P')] = -1.33\n",
    "\n",
    "mj_matrix[('E', 'E')] = -0.91\n",
    "mj_matrix[('E', 'H')] = -2.15\n",
    "mj_matrix[('E', 'R')] = -2.27\n",
    "mj_matrix[('E', 'K')] = -1.80\n",
    "mj_matrix[('E', 'P')] = -1.26\n",
    "\n",
    "mj_matrix[('H', 'H')] = -3.05\n",
    "mj_matrix[('H', 'R')] = -2.16\n",
    "mj_matrix[('H', 'K')] = -1.35\n",
    "mj_matrix[('H', 'P')] = -2.25\n",
    "\n",
    "mj_matrix[('R', 'R')] = -1.55\n",
    "mj_matrix[('R', 'K')] = -0.59\n",
    "mj_matrix[('R', 'P')] = -1.70\n",
    "\n",
    "mj_matrix[('K', 'K')] = -0.12\n",
    "mj_matrix[('K', 'P')] = -0.97\n",
    "\n",
    "mj_matrix[('P', 'P')] = -1.75\n",
    "\n",
    "# Ensure the MJ matrix is symmetric\n",
    "def ensure_mj_matrix_symmetric(mj_matrix, amino_acids):\n",
    "    for aa1 in amino_acids:\n",
    "        for aa2 in amino_acids:\n",
    "            if mj_matrix[(aa1, aa2)] == 0.0 and mj_matrix[(aa2, aa1)] != 0.0:\n",
    "                mj_matrix[(aa1, aa2)] = mj_matrix[(aa2, aa1)]\n",
    "            elif mj_matrix[(aa2, aa1)] == 0.0 and mj_matrix[(aa1, aa2)] != 0.0:\n",
    "                mj_matrix[(aa2, aa1)] = mj_matrix[(aa1, aa2)]\n",
    "    return mj_matrix\n",
    "\n",
    "# Calculate the MJ score for a given sequence considering coupling scores\n",
    "def calculate_mj_score(sequence, mj_matrix, coupling_scores):\n",
    "    score = 0.0\n",
    "    num_residues = len(sequence)\n",
    "    for i in range(num_residues):\n",
    "        for j in range(i+1, num_residues):\n",
    "            coupling_score = coupling_scores.get((i+1, j+1), 0)  # Default to 0 if no score is found\n",
    "            if coupling_score != 0:\n",
    "                score += mj_matrix[(sequence[i], sequence[j])] * coupling_score # Weight by coupling score\n",
    "    return score\n",
    "\n",
    "# Generate all possible single amino acid mutations\n",
    "def generate_mutations(sequence, amino_acids):\n",
    "    mutations = []\n",
    "    for i in range(len(sequence)):\n",
    "        for aa in amino_acids:\n",
    "            if aa != sequence[i]:\n",
    "                mutated_seq = sequence[:i] + aa + sequence[i+1:]\n",
    "                mutations.append((sequence[i], i+1, aa, mutated_seq))\n",
    "    return mutations\n",
    "\n",
    "# Calculate the change in MJ score for each mutation\n",
    "def calculate_mutation_scores(sequence, mj_matrix, coupling_scores, amino_acids):\n",
    "    original_score = calculate_mj_score(sequence, mj_matrix, coupling_scores)\n",
    "    mutations = generate_mutations(sequence, amino_acids)\n",
    "    \n",
    "    mutation_scores = {\"wt\": original_score}\n",
    "    for wt_residue, index, mutant_residue, mutated_seq in mutations:\n",
    "        mutated_score = calculate_mj_score(mutated_seq, mj_matrix, coupling_scores)\n",
    "        mutation_label = f\"{wt_residue}{index}{mutant_residue}\"\n",
    "        mutation_scores[mutation_label] = mutated_score\n",
    "    \n",
    "    return mutation_scores, original_score\n",
    "\n",
    "# Save the matrix to a file\n",
    "def save_matrix(matrix, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for key, value in matrix.items():\n",
    "            f.write(f\"{key}\\t{value}\\n\")\n",
    "\n",
    "# Save the MJ matrix\n",
    "def save_mj_matrix(mj_matrix, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for (aa1, aa2), value in mj_matrix.items():\n",
    "            f.write(f\"{aa1}-{aa2}\\t{value}\\n\")\n",
    "\n",
    "# Save the coupling score matrix as a sorted list\n",
    "def save_coupling_matrix(coupling_scores, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for (i, j), score in sorted(coupling_scores.items()):\n",
    "            f.write(f\"{i}-{j}\\t{score}\\n\")\n",
    "\n",
    "# Save the weighted MJ scores\n",
    "def save_weighted_scores(scores, original_score, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Label\\tScore\\tDifference\\n\")\n",
    "        for label, score in scores.items():\n",
    "            difference = (original_score - score) if label != \"wt\" else 0\n",
    "            f.write(f\"{label}\\t{score}\\t{difference}\\n\")\n",
    "\n",
    "# Main function to run the analysis\n",
    "def main(fasta_file, coupling_file, output_dir):\n",
    "    sequence = load_fasta(fasta_file)\n",
    "    print(f\"Original Sequence: {sequence}\")\n",
    "    \n",
    "    coupling_scores, coupling_df = load_coupling_scores(coupling_file)\n",
    "    \n",
    "    # Save the coupling score matrix as a sorted list\n",
    "    save_coupling_matrix(coupling_scores, f\"{output_dir}/coupling_scores_matrix.txt\")\n",
    "    \n",
    "    # Ensure the MJ matrix is symmetric\n",
    "    global mj_matrix\n",
    "    mj_matrix = ensure_mj_matrix_symmetric(mj_matrix, amino_acids)\n",
    "    \n",
    "    # Save the MJ matrix\n",
    "    save_mj_matrix(mj_matrix, f\"{output_dir}/mj_matrix.txt\")\n",
    "    \n",
    "    # Calculate mutation scores\n",
    "    mutation_scores, original_score = calculate_mutation_scores(sequence, mj_matrix, coupling_scores, amino_acids)\n",
    "    \n",
    "    # Save the weighted MJ scores\n",
    "    save_weighted_scores(mutation_scores, original_score, f\"{output_dir}/stability_scores.txt\")\n",
    "    \n",
    "    # Print results\n",
    "    for label, score in mutation_scores.items():\n",
    "        difference = original_score - score if label != \"wt\" else 0\n",
    "        print(f\"{label}: {score}, Difference: {difference}\")\n",
    "\n",
    "# Run the main function with your fasta file and coupling scores file\n",
    "fasta_file = \"\"  # Change this to your fasta file name\n",
    "coupling_file = \"\"  # Change this to your EVcouplings coupling scores file name\n",
    "output_dir = \"\"  # Change this to your desired output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "main(fasta_file, coupling_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25b37e",
   "metadata": {},
   "source": [
    "# The following section contains the scripts used to analyze the 20 randomly selected high quality monomeric proteins (20R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cc32e",
   "metadata": {},
   "source": [
    "Summarize (compress) mutational frustration output from the Frustratometer (20R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53357fb1",
   "metadata": {},
   "source": [
    "Script expects the subdirectories containing the .rar frustratometer output to be called frustratometer and frustratometer_af (if you have an AlphaFold structure based frustratometer output). This script was used for the 20 random (20R) monomers from the PDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import patoolib\n",
    "import pandas as pd\n",
    "\n",
    "# Function to clean directories by deleting all files except .rar and removing subdirectories\n",
    "def clean_directory(directory):\n",
    "    \"\"\"\n",
    "    Deletes all files in the directory except .rar files.\n",
    "    Deletes all subdirectories and their contents.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory to clean.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning directory: {directory}\")\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        \n",
    "        # If it's a file\n",
    "        if os.path.isfile(item_path):\n",
    "            if not item.lower().endswith('.rar'):\n",
    "                print(f\"Deleting file: {item_path}\")\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                print(f\"Keeping .rar file: {item_path}\")\n",
    "        \n",
    "        # If it's a directory\n",
    "        elif os.path.isdir(item_path):\n",
    "            print(f\"Deleting directory and its contents: {item_path}\")\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "# Function to extract .rar files from directories\n",
    "def extract_rar_from_directory(input_dir):\n",
    "    \"\"\"\n",
    "    Extracts the first .rar file found in the specified directory.\n",
    "    \"\"\"\n",
    "    print(f\"Scanning directory for .rar files: {input_dir}\")\n",
    "    rar_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.rar')]\n",
    "    if not rar_files:\n",
    "        print(f\"No .rar files found in {input_dir}.\")\n",
    "        return\n",
    "    \n",
    "    rar_file = os.path.join(input_dir, rar_files[0])\n",
    "    extracted_folder = os.path.join(input_dir, \"extracted\")\n",
    "    \n",
    "    try:\n",
    "        patoolib.extract_archive(rar_file, outdir=extracted_folder)\n",
    "        print(f\"Extracted {rar_file} to {extracted_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {rar_file}: {e}\")\n",
    "\n",
    "# Function to locate the .pdb_mutational file\n",
    "def locate_pdb_mutational_file(extracted_folder):\n",
    "    print(f\"Searching for FrustrationData directory in: {extracted_folder}\")\n",
    "    frustration_dirs = [\n",
    "        os.path.join(root, dir)\n",
    "        for root, dirs, files in os.walk(extracted_folder)\n",
    "        for dir in dirs if \"FrustrationData\" in dir\n",
    "    ]\n",
    "    if not frustration_dirs:\n",
    "        raise FileNotFoundError(f\"No FrustrationData directory found in {extracted_folder}\")\n",
    "    \n",
    "    pdb_files = [\n",
    "        os.path.join(frustration_dirs[0], f)\n",
    "        for f in os.listdir(frustration_dirs[0])\n",
    "        if f.endswith('.pdb_mutational')\n",
    "    ]\n",
    "    if not pdb_files:\n",
    "        raise FileNotFoundError(f\"No .pdb_mutational files found in {frustration_dirs[0]}\")\n",
    "    \n",
    "    return pdb_files[0]\n",
    "\n",
    "# Function to process the .pdb_mutational file\n",
    "def process_frustration_file(input_file, output_file):\n",
    "    print(f\"Processing .pdb_mutational file: {input_file}\")\n",
    "    col_names = [\n",
    "        \"Res1\", \"Res2\", \"ChainRes1\", \"ChainRes2\",\n",
    "        \"DensityRes1\", \"DensityRes2\", \"AA1\", \"AA2\",\n",
    "        \"NativeEnergy\", \"DecoyEnergy\", \"SDEnergy\",\n",
    "        \"FrstIndex\", \"Welltype\", \"FrstState\"\n",
    "    ]\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, sep=r'\\s+', comment=\"#\", names=col_names)\n",
    "        residue_data = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            energy_diff = row[\"NativeEnergy\"] - row[\"DecoyEnergy\"]\n",
    "            residues = [(row[\"Res1\"], row[\"AA1\"]), (row[\"Res2\"], row[\"AA2\"])]\n",
    "            for res, aa in residues:\n",
    "                if res not in residue_data:\n",
    "                    residue_data[res] = {\"aa\": aa, \"sum_diff\": 0.0, \"count\": 0}\n",
    "                residue_data[res][\"sum_diff\"] += energy_diff\n",
    "                residue_data[res][\"count\"] += 1\n",
    "\n",
    "        output_data = []\n",
    "        for res, data in sorted(residue_data.items()):\n",
    "            average_diff = data[\"sum_diff\"] / data[\"count\"] if data[\"count\"] > 0 else 0.0\n",
    "            output_data.append([res, data[\"aa\"], average_diff])\n",
    "\n",
    "        reindexed_data = []\n",
    "        for new_residue_index, row in enumerate(output_data, start=1):\n",
    "            reindexed_data.append([new_residue_index, row[1], row[2]])\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"Residue# ResidueAA Difference\\n\")\n",
    "            for row in reindexed_data:\n",
    "                f.write(f\"{row[0]} {row[1]} {row[2]:.4f}\\n\")\n",
    "        print(f\"Output written to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {input_file}: {e}\")\n",
    "\n",
    "# Function to process extracted directories\n",
    "def process_subdirectory(test_dir, sub_dir, output_filename):\n",
    "    subdirectory_path = os.path.join(test_dir, sub_dir)\n",
    "    extracted_folder = os.path.join(subdirectory_path, \"extracted\")\n",
    "    output_file = os.path.join(subdirectory_path, output_filename)\n",
    "\n",
    "    if not os.path.isdir(extracted_folder):\n",
    "        print(f\"Extracted folder not found: {extracted_folder}. Skipping {sub_dir} in {test_dir}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pdb_mutational_file = locate_pdb_mutational_file(extracted_folder)\n",
    "        process_frustration_file(pdb_mutational_file, output_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error processing {sub_dir} in {test_dir}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {sub_dir} in {test_dir}: {e}\")\n",
    "\n",
    "# Main function\n",
    "def main(root_directory):\n",
    "    for dirpath, dirnames, _ in os.walk(root_directory):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.lower() in ['frustratometer', 'frustratometer_af']:\n",
    "                target_dir = os.path.join(dirpath, dirname)\n",
    "                print(f\"\\nFound target directory: {target_dir}\")\n",
    "                \n",
    "                # **Clean the target directory before extraction**\n",
    "                clean_directory(target_dir)\n",
    "                \n",
    "                # Proceed with extraction\n",
    "                extract_rar_from_directory(target_dir)\n",
    "                \n",
    "                # Determine output filename based on directory type\n",
    "                if 'frustratometer_af' in dirname.lower():\n",
    "                    output_filename = \"frustration_af_summary.txt\"\n",
    "                elif 'frustratometer' in dirname.lower():\n",
    "                    output_filename = \"frustration_summary.txt\"\n",
    "                else:\n",
    "                    print(f\"Unknown directory type: {dirname}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Process the subdirectory\n",
    "                process_subdirectory(dirpath, dirname, output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # **Specify your root directory here**\n",
    "    root_directory = \"\" \n",
    "    main(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beedb22",
   "metadata": {},
   "source": [
    "Combine and allign data for each protein (20R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf297bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "from collections import defaultdict\n",
    "from Bio.PDB.Polypeptide import is_aa\n",
    "from Bio.PDB import PDBParser, DSSP\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Logging Setup\n",
    "# -------------------------------------------------------------------------\n",
    "def setup_logging(debug=False):\n",
    "    level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging(debug=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) Directory Validation\n",
    "# -------------------------------------------------------------------------\n",
    "def validate_directory_structure(dir_path):\n",
    "    \"\"\"\n",
    "    Checks which data sources are available in the directory.\n",
    "    Returns a tuple (has_any_data, available_sources).\n",
    "    \"\"\"\n",
    "    possible_sources = {\n",
    "        'frustratometer': ['frustration_summary.txt'],\n",
    "        'frustratometer_af': ['frustration_af_summary.txt'],\n",
    "        'experimental_data': ['average_b_factors.txt'],  \n",
    "        'mj_analysis': ['stability_scores.txt']\n",
    "    }\n",
    "    \n",
    "    available_sources = []\n",
    "    dir_path = Path(dir_path)\n",
    "    \n",
    "    for subdir, required_files in possible_sources.items():\n",
    "        subdir_path = dir_path / subdir\n",
    "        if subdir_path.is_dir():\n",
    "            # All \"required_files\" must be present\n",
    "            all_files_exist = all((subdir_path / file).is_file() for file in required_files)\n",
    "            if all_files_exist:\n",
    "                available_sources.append(subdir)\n",
    "    \n",
    "    return len(available_sources) > 0, available_sources\n",
    "\n",
    "def get_valid_directories(root_path):\n",
    "    \"\"\"\n",
    "    Finds all subdirectories that have at least one valid data source.\n",
    "    Returns a list of tuples (directory_path, available_sources).\n",
    "    \"\"\"\n",
    "    valid_dirs = []\n",
    "    root_path = Path(root_path)\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        logger.error(f\"Root directory does not exist: {root_path}\")\n",
    "        return []\n",
    "        \n",
    "    for entry in root_path.iterdir():\n",
    "        if entry.is_dir():\n",
    "            has_data, available_sources = validate_directory_structure(entry)\n",
    "            if has_data:\n",
    "                valid_dirs.append((entry, available_sources))\n",
    "                logger.info(f\"Found directory {entry} with data sources: {', '.join(available_sources)}\")\n",
    "            else:\n",
    "                logger.warning(f\"Directory {entry} has no valid data sources, skipping\")\n",
    "    \n",
    "    return sorted(valid_dirs, key=lambda x: str(x[0]))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Parsing Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def parse_frustration_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses frustration_summary.txt or frustration_af_summary.txt.\n",
    "    \n",
    "    Format:\n",
    "        Residue# ResidueAA Difference\n",
    "        1 A -0.1234\n",
    "        2 G 0.5678\n",
    "        3 S -1.2345\n",
    "        ...\n",
    "        \n",
    "    Returns:\n",
    "        sequence_str (str): Amino acid sequence in 1-letter codes.\n",
    "        pos_values (dict): Dictionary mapping 1-based residue positions to frustration differences.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_frustration_file: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"Residue#\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    res_num = int(parts[0])\n",
    "                    aa = parts[1].upper()\n",
    "                    difference = float(parts[2])\n",
    "                    \n",
    "                    if aa not in \"ACDEFGHIKLMNPQRSTVWY\":\n",
    "                        logger.warning(f\"Line {line_number}: Unknown amino acid code '{aa}'. Assigned as 'X'.\")\n",
    "                        aa = 'X'\n",
    "\n",
    "                    lines.append((res_num, aa, difference))\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"parse_frustration_file: Skipping line {line_number} due to ValueError: {line}\")\n",
    "                    continue\n",
    "            else:\n",
    "                logger.warning(f\"parse_frustration_file: Line {line_number} does not have enough parts: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(f\"parse_frustration_file: No valid data found in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines_sorted = sorted(lines, key=lambda x: x[0])\n",
    "    sequence = ''.join([aa for (_, aa, _) in lines_sorted])\n",
    "    pos_values = {res_num: difference for (res_num, _, difference) in lines_sorted}\n",
    "\n",
    "    return sequence, pos_values\n",
    "\n",
    "def parse_b_factor(file_path):\n",
    "    \"\"\"\n",
    "    Parses average_b_factors.txt\n",
    "    Format:\n",
    "      Residue ResidueAA Average_B_Factor\n",
    "      1 K 84.683\n",
    "      ...\n",
    "    Returns:\n",
    "        sequence_str (str): Amino acid sequence from file.\n",
    "        pos_values (dict): Dictionary {1-based_pos: B-factor}.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_b_factor: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"Residue\"):\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            try:\n",
    "                idx = int(parts[0])\n",
    "                aa = parts[1].upper()\n",
    "                bfact = float(parts[2])\n",
    "                lines.append((idx, aa, bfact))\n",
    "            except ValueError as ve:\n",
    "                logger.warning(f\"parse_b_factor: Skipping line due to ValueError: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(f\"parse_b_factor: No lines parsed in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    data_dict = {r[0]: (r[1], r[2]) for r in lines}\n",
    "    sorted_indices = sorted(data_dict.keys())\n",
    "\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for pos, idx in enumerate(sorted_indices, start=1):\n",
    "        aa, bfact = data_dict[idx]\n",
    "        seq_builder.append(aa)\n",
    "        pos_values[pos] = bfact\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_evolutionary(file_path):\n",
    "    \"\"\"\n",
    "    Parses stability_scores.txt\n",
    "    Format:\n",
    "      Label Score Difference\n",
    "      M1C -456.89159 -0.0\n",
    "      ...\n",
    "    Returns (sequence_str, pos_values)\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_evolutionary: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    data_map = defaultdict(list)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"Label\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            label = parts[0]\n",
    "            try:\n",
    "                diff = float(parts[2])\n",
    "            except ValueError:\n",
    "                logger.warning(f\"parse_evolutionary: Invalid difference value in line: {line}\")\n",
    "                continue\n",
    "\n",
    "            if label.lower() == \"wt\":\n",
    "                continue\n",
    "\n",
    "            m = re.match(r'([A-Z])(\\d+)([A-Z])', label, re.IGNORECASE)\n",
    "            if m:\n",
    "                native_aa = m.group(1).upper()\n",
    "                idx = int(m.group(2))\n",
    "                data_map[(native_aa, idx)].append(diff)\n",
    "\n",
    "    if not data_map:\n",
    "        logger.debug(f\"parse_evolutionary: No valid lines in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    index_to_aa = {}\n",
    "    index_to_diff = {}\n",
    "\n",
    "    used_positions = set()\n",
    "    sorted_keys = sorted(data_map.keys(), key=lambda x: x[1])\n",
    "    for (aa, i) in sorted_keys:\n",
    "        if i in used_positions:\n",
    "            continue\n",
    "        used_positions.add(i)\n",
    "        diffs = data_map[(aa, i)]\n",
    "        avg_diff = sum(diffs)/len(diffs) if diffs else 0.0\n",
    "        index_to_aa[i] = aa\n",
    "        index_to_diff[i] = avg_diff\n",
    "\n",
    "    sorted_indices = sorted(index_to_aa.keys())\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for pos, idx in enumerate(sorted_indices, start=1):\n",
    "        seq_builder.append(index_to_aa[idx])\n",
    "        pos_values[pos] = index_to_diff[idx]\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_rmsf(file_path):\n",
    "    \"\"\"\n",
    "    Parses experimental_data/rmsf.csv of the form:\n",
    "      26,A,2.472\n",
    "      27,A,2.308\n",
    "      28,A,2.657\n",
    "      ...\n",
    "    We reindex so that the first residue (e.g. 26) -> 1, second (27) -> 2, etc.\n",
    "    Returns (sequence_str, pos_values) => {1-based_pos: RMSF}.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_rmsf: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(',')\n",
    "            if len(parts) < 3:\n",
    "                logger.warning(f\"parse_rmsf: Skipping malformed line {line_number}: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                old_idx = int(parts[0])\n",
    "                # chain = parts[1]  # Chain letter (not essential for alignment here)\n",
    "                rmsf_val = float(parts[2])\n",
    "                lines.append((old_idx, rmsf_val))\n",
    "            except ValueError:\n",
    "                logger.warning(f\"parse_rmsf: Skipping invalid line {line_number}: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(\"parse_rmsf: No valid lines in RMSF file.\")\n",
    "        return \"\", {}\n",
    "\n",
    "    # Sort by the original residue index just in case\n",
    "    lines_sorted = sorted(lines, key=lambda x: x[0])\n",
    "\n",
    "    # Determine offset\n",
    "    offset = lines_sorted[0][0] - 1  # e.g., if first index is 26, offset is 25\n",
    "    pos_values = {}\n",
    "    seq_builder = []\n",
    "    for (old_idx, val) in lines_sorted:\n",
    "        new_idx = old_idx - offset\n",
    "        pos_values[new_idx] = val\n",
    "        # Use a dummy 'A' (or any single-letter code) for alignment\n",
    "        seq_builder.append(\"A\")\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_mutation_scores(file_path):\n",
    "    \"\"\"\n",
    "    Parses mutation_scores.txt which is expected to have the following format:\n",
    "    \n",
    "       segment mutant  pos wt subs    frequency  column_conservation  effect_prediction_epistatic\n",
    "           NaN   G12A   12  G    A 2.533944e-02             0.319221                    -2.248724\n",
    "           NaN   G12C   12  G    C 5.786646e-04             0.319221                    -5.606657\n",
    "           ...\n",
    "    \n",
    "    For each residue position, this function calculates the negative average of the \n",
    "    effect_prediction_epistatic scores and builds a wt sequence (using the wt residue).\n",
    "    \n",
    "    Returns:\n",
    "       sequence_str (str): The wt amino acid sequence (ordered by position).\n",
    "       pos_values (dict): Dictionary mapping 1-based residue positions to the negative average effect prediction.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_mutation_scores: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "    \n",
    "    pos_effects = defaultdict(list)\n",
    "    pos_wt = {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        header = f.readline()  # Skip header line\n",
    "        for line_number, line in enumerate(f, start=2):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 8:\n",
    "                logger.warning(f\"parse_mutation_scores: Skipping malformed line {line_number}: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                # parts: [segment, mutant, pos, wt, subs, frequency, column_conservation, effect_prediction_epistatic]\n",
    "                pos = int(parts[2])\n",
    "                wt_res = parts[3].upper()\n",
    "                effect = float(parts[7])\n",
    "            except ValueError:\n",
    "                logger.warning(f\"parse_mutation_scores: Skipping line {line_number} due to conversion error: {line}\")\n",
    "                continue\n",
    "            \n",
    "            pos_effects[pos].append(effect)\n",
    "            # Record wt residue (assuming consistent for a given pos)\n",
    "            if pos not in pos_wt:\n",
    "                pos_wt[pos] = wt_res\n",
    "    \n",
    "    if not pos_effects:\n",
    "        logger.debug(f\"parse_mutation_scores: No valid data parsed from {file_path}\")\n",
    "        return \"\", {}\n",
    "    \n",
    "    sorted_positions = sorted(pos_effects.keys())\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for new_pos, pos in enumerate(sorted_positions, start=1):\n",
    "        wt = pos_wt.get(pos, 'X')\n",
    "        avg_effect = sum(pos_effects[pos]) / len(pos_effects[pos])\n",
    "        # Negative average as requested\n",
    "        pos_values[new_pos] = -avg_effect\n",
    "        seq_builder.append(wt)\n",
    "    \n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Alignment Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def align_two(seqA, seqB, gap_open=-2, gap_extend=-0.5):\n",
    "    \"\"\"\n",
    "    Attempt a global alignment with Biopython pairwise2.\n",
    "    Returns (alignedA, alignedB).\n",
    "    \"\"\"\n",
    "    seqA = seqA.upper()\n",
    "    seqB = seqB.upper()\n",
    "\n",
    "    if len(seqA) == 0 and len(seqB) == 0:\n",
    "        logger.debug(\"align_two: Both sequences empty => '' \")\n",
    "        return \"\", \"\"\n",
    "    if len(seqA) == 0:\n",
    "        logger.debug(f\"align_two: SeqA empty, SeqB length={len(seqB)} => trivial alignment\")\n",
    "        return \"-\" * len(seqB), seqB\n",
    "    if len(seqB) == 0:\n",
    "        logger.debug(f\"align_two: SeqB empty, SeqA length={len(seqA)} => trivial alignment\")\n",
    "        return seqA, \"-\" * len(seqA)\n",
    "\n",
    "    logger.debug(f\"align_two: Attempting global alignment: len(seqA)={len(seqA)}, len(seqB)={len(seqB)}\")\n",
    "    alignments = pairwise2.align.globalms(seqA, seqB, 2, -1, gap_open, gap_extend)\n",
    "    if not alignments:\n",
    "        logger.warning(\"align_two: No alignment from Biopython => trivial fallback.\")\n",
    "        max_len = max(len(seqA), len(seqB))\n",
    "        if len(seqA) == max_len:\n",
    "            return seqA, seqB + \"-\"*(len(seqA)-len(seqB))\n",
    "        else:\n",
    "            return seqA + \"-\"*(len(seqB)-len(seqA)), seqB\n",
    "\n",
    "    best = alignments[0]\n",
    "    return best[0], best[1]\n",
    "\n",
    "def merge_val_alignment(alnA, alnB, valA, valB):\n",
    "    \"\"\"\n",
    "    Merge aligned sequences with their values.\n",
    "    Returns (aligned_valsA, aligned_valsB) as lists.\n",
    "    \"\"\"\n",
    "    aligned_valsA = []\n",
    "    aligned_valsB = []\n",
    "    origA_pos = 1\n",
    "    origB_pos = 1\n",
    "    \n",
    "    for i in range(len(alnA)):\n",
    "        cA = alnA[i]\n",
    "        cB = alnB[i]\n",
    "\n",
    "        if cA == '-':\n",
    "            aligned_valsA.append('n/a')\n",
    "        else:\n",
    "            aligned_valsA.append(valA.get(origA_pos, 'n/a'))\n",
    "            origA_pos += 1\n",
    "\n",
    "        if cB == '-':\n",
    "            aligned_valsB.append('n/a')\n",
    "        else:\n",
    "            aligned_valsB.append(valB.get(origB_pos, 'n/a'))\n",
    "            origB_pos += 1\n",
    "            \n",
    "    return aligned_valsA, aligned_valsB\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) Multiple Sequence Alignment\n",
    "# -------------------------------------------------------------------------\n",
    "def multi_align_sequences(seq_list):\n",
    "    \"\"\"\n",
    "    Progressive multiple sequence alignment.\n",
    "    seq_list: [(name, seq_str, val_dict), ...]\n",
    "    Returns a list of (name, final_aln_seq, final_aln_vals).\n",
    "    \"\"\"\n",
    "    seq_list.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    recs = []\n",
    "    for (nm, s, v) in seq_list:\n",
    "        arr = [v.get(i, 'n/a') for i in range(1, len(s)+1)]\n",
    "        recs.append([nm, s, v, s, arr])\n",
    "\n",
    "    if not recs:\n",
    "        logger.debug(\"multi_align_sequences: No sequences to align.\")\n",
    "        return []\n",
    "\n",
    "    # The first record is the \"master\" to start\n",
    "    master_aln_seq = recs[0][3]\n",
    "    master_aln_vals = recs[0][4]\n",
    "\n",
    "    for i in range(1, len(recs)):\n",
    "        nameB, seqB_str, valB, alnB_str, alnB_vals = recs[i]\n",
    "        logger.debug(f\"multi_align: Aligning MASTER({recs[0][0]}) with {nameB}\")\n",
    "\n",
    "        # Re-build dictionary for master positions\n",
    "        master_dict = {}\n",
    "        real_pos = 1\n",
    "        for idx_char, char in enumerate(master_aln_seq):\n",
    "            if char != '-':\n",
    "                master_dict[real_pos] = master_aln_vals[idx_char]\n",
    "                real_pos += 1\n",
    "\n",
    "        alnA, alnB = align_two(master_aln_seq, seqB_str)\n",
    "        newA_vals, newB_vals = merge_val_alignment(alnA, alnB, master_dict, valB)\n",
    "\n",
    "        master_aln_seq = alnA\n",
    "        master_aln_vals = newA_vals\n",
    "        recs[0][3] = alnA\n",
    "        recs[0][4] = newA_vals\n",
    "\n",
    "        recs[i][3] = alnB\n",
    "        recs[i][4] = newB_vals\n",
    "\n",
    "    final_len = len(recs[0][3])\n",
    "    logger.debug(f\"multi_align_sequences: final alignment length={final_len}\")\n",
    "\n",
    "    # Pad/truncate all alignments to the final length\n",
    "    for i in range(len(recs)):\n",
    "        seq_aln = recs[i][3]\n",
    "        vals_aln = recs[i][4]\n",
    "        diff = final_len - len(seq_aln)\n",
    "        if diff > 0:\n",
    "            logger.debug(f\"Padding {recs[i][0]} from length={len(seq_aln)} to {final_len}\")\n",
    "            seq_aln += '-'*diff\n",
    "            vals_aln += ['n/a']*diff\n",
    "            recs[i][3] = seq_aln\n",
    "            recs[i][4] = vals_aln\n",
    "        elif diff < 0:\n",
    "            logger.warning(f\"{recs[i][0]} alignment is longer than master!? Truncating.\")\n",
    "            recs[i][3] = seq_aln[:final_len]\n",
    "            recs[i][4] = vals_aln[:final_len]\n",
    "\n",
    "    final_data = []\n",
    "    for r in recs:\n",
    "        final_data.append((r[0], r[3], r[4]))\n",
    "    return final_data\n",
    "\n",
    "def map_ss_to_alignment(ss_map, exp_seq, residue_seq, aligned_dict):\n",
    "    \"\"\"Maps secondary structure assignments to aligned sequence positions.\"\"\"\n",
    "    if not ss_map or 'B_FACTOR' not in aligned_dict:\n",
    "        return ['n/a'] * len(residue_seq)\n",
    "        \n",
    "    # Get B-factor alignment\n",
    "    bf_aln_seq, bf_aln_vals = aligned_dict['B_FACTOR']\n",
    "    \n",
    "    # Create raw SS array\n",
    "    raw_ss = ['n/a'] * len(residue_seq)\n",
    "    \n",
    "    # Track position in original SS map\n",
    "    ss_pos = 1\n",
    "    \n",
    "    # Map through alignment\n",
    "    for i, (master_res, bf_res) in enumerate(zip(residue_seq, bf_aln_seq)):\n",
    "        if bf_res == '-' or bf_aln_vals[i] == 'n/a':\n",
    "            raw_ss[i] = 'n/a'\n",
    "        else:\n",
    "            raw_ss[i] = ss_map.get(ss_pos, 'o')\n",
    "            ss_pos += 1\n",
    "            \n",
    "    return raw_ss\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) PDB Parsing Function\n",
    "# -------------------------------------------------------------------------\n",
    "def parse_secondary_structure_from_pdb(pdb_file_path):\n",
    "    \"\"\"\n",
    "    Parses a PDB file to extract per-residue secondary structure assignments using DSSP.\n",
    "    Returns:\n",
    "        ss_map (dict): Mapping from sequential position (1-based) to 'A', 'B', or 'o'.\n",
    "    \"\"\"\n",
    "    ss_map = {}\n",
    "    pdb_file = Path(pdb_file_path)\n",
    "    \n",
    "    if not pdb_file.is_file():\n",
    "        logger.error(f\"PDB file not found: {pdb_file_path}\")\n",
    "        return ss_map\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('protein', pdb_file)\n",
    "        model = structure[0]\n",
    "        chains = list(model.get_chains())\n",
    "        logger.debug(f\"Found chains: {[chain.id for chain in chains]}\")\n",
    "        \n",
    "        dssp = DSSP(model, str(pdb_file), dssp='/opt/homebrew/bin/mkdssp')\n",
    "        logger.debug(f\"DSSP successful, found {len(dssp.keys())} residues\")\n",
    "        \n",
    "        # Get all keys and sort by residue number\n",
    "        keys = sorted(dssp.keys(), key=lambda x: x[1][1])\n",
    "        \n",
    "        # Assign sequential positions\n",
    "        for seq_pos, key in enumerate(keys, start=1):\n",
    "            ss = dssp[key][2]\n",
    "            if ss in ('H', 'G', 'I'):\n",
    "                ss_code = 'A'\n",
    "            elif ss in ('E', 'B'):\n",
    "                ss_code = 'B'\n",
    "            else:\n",
    "                ss_code = 'o'\n",
    "                \n",
    "            ss_map[seq_pos] = ss_code\n",
    "            logger.debug(f\"Assigned SS for sequential position {seq_pos}: {ss} -> {ss_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing PDB: {str(e)}\")\n",
    "        return ss_map\n",
    "\n",
    "    logger.info(f\"Successfully parsed {len(ss_map)} residues with SS assignments\")\n",
    "    return ss_map\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7) Main Processing Function\n",
    "# -------------------------------------------------------------------------\n",
    "def process_directory(root_directory, output_dir=None):\n",
    "    \"\"\"\n",
    "    Process a directory containing subdirectories with data.\n",
    "    \"\"\"\n",
    "    root_directory = Path(root_directory)\n",
    "    \n",
    "    # Set up summary data directory\n",
    "    if output_dir:\n",
    "        summary_data_dir = Path(output_dir)\n",
    "    else:\n",
    "        summary_data_dir = root_directory / \"summary_data\"\n",
    "        \n",
    "    try:\n",
    "        summary_data_dir.mkdir(exist_ok=True)\n",
    "        logger.info(f\"Using summary directory: {summary_data_dir}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create summary directory: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get valid directories and their available sources\n",
    "    dir_info = get_valid_directories(root_directory)\n",
    "\n",
    "    if not dir_info:\n",
    "        logger.error(\"No directories with valid data sources found.\")\n",
    "        return\n",
    "\n",
    "    # Process each directory\n",
    "    for dir_path, available_sources in dir_info:\n",
    "        protein_id = dir_path.name\n",
    "        logger.info(f\"Processing: {dir_path}\")\n",
    "        \n",
    "        # Initialize paths and data containers\n",
    "        seq_exp, vals_exp = \"\", {}\n",
    "        seq_af, vals_af = \"\", {}\n",
    "        seq_bf, vals_bf = \"\", {}\n",
    "        seq_ev, vals_ev = \"\", {}\n",
    "        seq_rmsf, vals_rmsf = \"\", {}\n",
    "        seq_mut, vals_mut = \"\", {}  # For mutation scores\n",
    "        ss_map = {}  # Secondary Structure mapping\n",
    "\n",
    "        try:\n",
    "            # 1) Experimental frustration\n",
    "            if 'frustratometer' in available_sources:\n",
    "                exp_path = dir_path / \"frustratometer\" / \"frustration_summary.txt\"\n",
    "                seq_exp, vals_exp = parse_frustration_file(str(exp_path))\n",
    "                logger.debug(f\"EXP_FRUST: length={len(seq_exp)} from {exp_path}\")\n",
    "            \n",
    "            # 2) AlphaFold frustration\n",
    "            if 'frustratometer_af' in available_sources:\n",
    "                af_path = dir_path / \"frustratometer_af\" / \"frustration_af_summary.txt\"\n",
    "                seq_af, vals_af = parse_frustration_file(str(af_path))\n",
    "                logger.debug(f\"AF_FRUST: length={len(seq_af)} from {af_path}\")\n",
    "            \n",
    "            # 3) Experimental data (B-factor, RMSF)\n",
    "            if 'experimental_data' in available_sources:\n",
    "                # B-factor\n",
    "                bf_path = dir_path / \"experimental_data\" / \"average_b_factors.txt\"\n",
    "                seq_bf, vals_bf = parse_b_factor(str(bf_path))\n",
    "                logger.debug(f\"B_FACTOR: length={len(seq_bf)} from {bf_path}\")\n",
    "                \n",
    "                # RMSF\n",
    "                rmsf_path = dir_path / \"experimental_data\" / \"rmsf.csv\"\n",
    "                if rmsf_path.is_file():\n",
    "                    seq_rmsf, vals_rmsf = parse_rmsf(str(rmsf_path))\n",
    "                    logger.debug(f\"RMSF: length={len(seq_rmsf)} from {rmsf_path}\")\n",
    "\n",
    "            # 4) Evolutionary data (MJ analysis)\n",
    "            if 'mj_analysis' in available_sources:\n",
    "                evol_path = dir_path / \"mj_analysis\" / \"stability_scores.txt\"\n",
    "                seq_ev, vals_ev = parse_evolutionary(str(evol_path))\n",
    "                logger.debug(f\"EVOL: length={len(seq_ev)} from {evol_path}\")\n",
    "\n",
    "            # 5) Mutation scores (Mutability)\n",
    "            ms_path = dir_path / \"evc_output\" / \"couplings\" / \"mutation_scores.txt\"\n",
    "            if ms_path.is_file():\n",
    "                seq_mut, vals_mut = parse_mutation_scores(str(ms_path))\n",
    "                logger.debug(f\"MUTATION: length={len(seq_mut)} from {ms_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"No mutation_scores.txt found at {ms_path}\")\n",
    "\n",
    "            # 6) Secondary Structure\n",
    "            # Locate 'monomer.pdb' within 'experimental_data' subdirectory\n",
    "            pdb_file_path = dir_path / \"experimental_data\" / \"monomer.pdb\"\n",
    "            if not pdb_file_path.is_file():\n",
    "                logger.warning(f\"No PDB file found at {pdb_file_path}. Assigning 'n/a' to all residues.\")\n",
    "            else:\n",
    "                ss_map = parse_secondary_structure_from_pdb(pdb_file_path)\n",
    "                if ss_map:\n",
    "                    logger.info(f\"Secondary structure information extracted for protein {protein_id}.\")\n",
    "                else:\n",
    "                    logger.warning(f\"Secondary structure information could not be extracted for protein {protein_id}. Assigning 'n/a' to all residues.\")\n",
    "\n",
    "            # Collect available data sources for alignment\n",
    "            data_sources = []\n",
    "            if len(seq_exp) > 0:\n",
    "                data_sources.append((\"EXP_FRUST\", seq_exp, vals_exp))\n",
    "            if len(seq_af) > 0:\n",
    "                data_sources.append((\"AF_FRUST\", seq_af, vals_af))\n",
    "            if len(seq_bf) > 0:\n",
    "                data_sources.append((\"B_FACTOR\", seq_bf, vals_bf))\n",
    "            if len(seq_ev) > 0:\n",
    "                data_sources.append((\"EVOL\", seq_ev, vals_ev))\n",
    "            if len(seq_rmsf) > 0:\n",
    "                data_sources.append((\"RMSF\", seq_rmsf, vals_rmsf))\n",
    "            if len(seq_mut) > 0:\n",
    "                data_sources.append((\"MUTATION\", seq_mut, vals_mut))\n",
    "\n",
    "            if not data_sources:\n",
    "                logger.warning(f\"No valid sequence data found in {dir_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Found {len(data_sources)} valid data sources for alignment\")\n",
    "\n",
    "            # Perform progressive multiple alignment\n",
    "            aligned = multi_align_sequences(data_sources)\n",
    "            if not aligned:\n",
    "                logger.warning(f\"Alignment failed for {dir_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Successfully aligned {len(aligned)} sequences\")\n",
    "\n",
    "            # The reference alignment is the first entry, typically the longest sequence\n",
    "            residue_seq = aligned[0][1]\n",
    "            final_len = len(residue_seq)\n",
    "            logger.debug(f\"Final alignment length: {final_len}\")\n",
    "\n",
    "            # Build a lookup for name -> (aln_seq, aln_vals)\n",
    "            aligned_dict = {x[0]: (x[1], x[2]) for x in aligned}\n",
    "            logger.debug(f\"Available data types in alignment: {list(aligned_dict.keys())}\")\n",
    "\n",
    "            # Prepare columns\n",
    "            raw_index = list(range(1, final_len+1))\n",
    "            raw_res = list(residue_seq)\n",
    "            raw_exp = ['n/a'] * final_len\n",
    "            raw_af  = ['n/a'] * final_len\n",
    "            raw_bf  = ['n/a'] * final_len\n",
    "            raw_ev  = ['n/a'] * final_len\n",
    "            raw_rmsf = ['n/a'] * final_len\n",
    "            raw_mut = ['n/a'] * final_len  # For mutation scores\n",
    "            raw_ss = ['n/a'] * final_len  # Initialize with 'n/a'\n",
    "\n",
    "            # Fill columns if available\n",
    "            if \"EXP_FRUST\" in aligned_dict:\n",
    "                _, exp_aln_vals = aligned_dict[\"EXP_FRUST\"]\n",
    "                raw_exp = exp_aln_vals\n",
    "            if \"AF_FRUST\" in aligned_dict:\n",
    "                _, af_aln_vals = aligned_dict[\"AF_FRUST\"]\n",
    "                raw_af = af_aln_vals\n",
    "            if \"B_FACTOR\" in aligned_dict:\n",
    "                _, bf_aln_vals = aligned_dict[\"B_FACTOR\"]\n",
    "                raw_bf = bf_aln_vals\n",
    "            if \"EVOL\" in aligned_dict:\n",
    "                _, ev_aln_vals = aligned_dict[\"EVOL\"]\n",
    "                raw_ev = ev_aln_vals\n",
    "            if \"RMSF\" in aligned_dict:\n",
    "                _, rmsf_aln_vals = aligned_dict[\"RMSF\"]\n",
    "                raw_rmsf = rmsf_aln_vals\n",
    "            if \"MUTATION\" in aligned_dict:\n",
    "                _, mut_aln_vals = aligned_dict[\"MUTATION\"]\n",
    "                raw_mut = mut_aln_vals\n",
    "\n",
    "            # Assign Secondary Structure based on alignment\n",
    "            raw_ss = map_ss_to_alignment(ss_map, seq_exp, residue_seq, aligned_dict)\n",
    "\n",
    "            # --- Modified output: Write CSV file instead of a tab-delimited text file ---\n",
    "            df = pd.DataFrame({\n",
    "                'AlnIndex': raw_index,\n",
    "                'Residue': raw_res,\n",
    "                'SecondaryStructure': raw_ss,\n",
    "                'B_Factor': raw_bf,\n",
    "                'ExpFrust': raw_exp,\n",
    "                'AFFrust': raw_af,\n",
    "                'EvolFrust': raw_ev,\n",
    "                'RMSF': raw_rmsf,\n",
    "                'Mutability': raw_mut\n",
    "            })\n",
    "\n",
    "            # Write CSV file in the current directory\n",
    "            out_path = dir_path / \"summary.csv\"\n",
    "            df.to_csv(out_path, index=False)\n",
    "            logger.info(f\"Wrote CSV summary to {out_path}\")\n",
    "\n",
    "            # Also write CSV file in the central summary_data directory\n",
    "            summary_filename = f\"summary_{dir_path.name}.csv\"\n",
    "            summary_path = summary_data_dir / summary_filename\n",
    "            df.to_csv(summary_path, index=False)\n",
    "            logger.info(f\"Wrote central CSV summary to {summary_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dir_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return summary_data_dir\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 9) Example Usage\n",
    "# -------------------------------------------------------------------------\n",
    "# Example usage (uncomment to run):\n",
    "root_dir = \"\"\n",
    "summary_dir = process_directory(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb81d1f",
   "metadata": {},
   "source": [
    "Generate violin plots for spearman correlation between each frustration type and B-factor for the set of proteins (20R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, levene, bartlett, kruskal\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def compute_spearman(x, y):\n",
    "    \"\"\"\n",
    "    Compute Spearman correlation with improved handling of constant arrays.\n",
    "    Returns None if correlation cannot be computed.\n",
    "    \"\"\"\n",
    "    mask = x.notna() & y.notna()\n",
    "    if mask.sum() > 1:\n",
    "        x_valid = x[mask]\n",
    "        y_valid = y[mask]\n",
    "        \n",
    "        # Check for constant arrays\n",
    "        if x_valid.std() == 0 or y_valid.std() == 0:\n",
    "            return None\n",
    "        try:\n",
    "            corr, _ = spearmanr(x_valid, y_valid)\n",
    "            return corr if not np.isnan(corr) else None\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_data(data_dir):\n",
    "    \"\"\"\n",
    "    Process all data files in the specified directory and compute Spearman correlations.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # Define frustration types\n",
    "    frust_types = ['ExpFrust', 'AFFrust', 'EvolFrust']\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(('.txt', '.csv')):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                # Determine separator based on file extension\n",
    "                sep = '\\t' if filename.endswith('.txt') else ','\n",
    "                df = pd.read_csv(filepath, sep=sep, na_values=['n/a', 'N/A'])\n",
    "                \n",
    "                # Skip if B_Factor column is missing\n",
    "                if 'B_Factor' not in df.columns:\n",
    "                    print(f\"Skipping {filename}: Missing 'B_Factor' column.\")\n",
    "                    continue\n",
    "                \n",
    "                # Convert columns to numeric\n",
    "                df['B_Factor'] = pd.to_numeric(df['B_Factor'], errors='coerce')\n",
    "                \n",
    "                # Process each frustration type\n",
    "                for frust_type in frust_types:\n",
    "                    if frust_type in df.columns:\n",
    "                        df[frust_type] = pd.to_numeric(df[frust_type], errors='coerce')\n",
    "                        corr = compute_spearman(df['B_Factor'], df[frust_type])\n",
    "                        \n",
    "                        if corr is not None:\n",
    "                            results.append({\n",
    "                                'Protein': filename,\n",
    "                                'Frustration_Type': frust_type,\n",
    "                                'Spearman_Correlation': corr\n",
    "                            })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def pairwise_levene_corrected(df, group_col, value_col, alpha=0.05, correction='bonferroni'):\n",
    "    \"\"\"\n",
    "    Perform pairwise Levene's tests between all group combinations with Bonferroni correction.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - group_col: Column name representing group labels.\n",
    "    - value_col: Column name containing the numerical values to compare.\n",
    "    - alpha: Desired overall significance level (default is 0.05).\n",
    "    - correction: Multiple testing correction method (default is 'bonferroni').\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with pairwise comparison results including adjusted p-values and significance.\n",
    "    \"\"\"\n",
    "    groups = df[group_col].unique()\n",
    "    pairwise = list(combinations(groups, 2))\n",
    "    \n",
    "    results = []\n",
    "    p_values = []\n",
    "    pair_names = []\n",
    "    \n",
    "    # Perform pairwise Levene's tests\n",
    "    for (group1, group2) in pairwise:\n",
    "        data1 = df[df[group_col] == group1][value_col].dropna()\n",
    "        data2 = df[df[group_col] == group2][value_col].dropna()\n",
    "        stat, p = levene(data1, data2)\n",
    "        results.append({'Group1': group1, 'Group2': group2, 'Statistic': stat, 'p-value': p})\n",
    "        p_values.append(p)\n",
    "        pair_names.append(f\"{group1} vs {group2}\")\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    adjusted = multipletests(p_values, alpha=alpha, method=correction)\n",
    "    adjusted_pvals = adjusted[1]\n",
    "    reject = adjusted[0]\n",
    "    \n",
    "    # Compile results\n",
    "    for i, pair in enumerate(results):\n",
    "        pair['Adjusted p-value'] = adjusted_pvals[i]\n",
    "        pair['Reject H0'] = reject[i]\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def add_annotations(ax, pairwise_df, order, value_col, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Add non-overlapping bracket annotations with vertical p-values for all pairwise comparisons.\n",
    "    Includes aligned shorter brackets with spacing where they meet.\n",
    "    \"\"\"\n",
    "    def get_pvalue(group1, group2, df):\n",
    "        \"\"\"Helper function to safely get p-value for a pair of groups\"\"\"\n",
    "        mask = ((df['Group1'] == group1) & (df['Group2'] == group2)) | \\\n",
    "               ((df['Group1'] == group2) & (df['Group2'] == group1))\n",
    "        matched = df[mask]['Adjusted p-value']\n",
    "        if len(matched) == 0:\n",
    "            print(f\"Warning: No p-value found for comparison between {group1} and {group2}\")\n",
    "            return None\n",
    "        return matched.values[0]\n",
    "    \n",
    "    # Determine the y positions based on the order\n",
    "    y_positions = {group: idx for idx, group in enumerate(order)}\n",
    "    \n",
    "    # Get the current x-axis limits to determine placement\n",
    "    xlim = ax.get_xlim()\n",
    "    max_x = xlim[1]\n",
    "    x_offset = max_x * 0.25  # Base offset for spacing\n",
    "    y_spacing = 0.1  # Small y-direction spacing where brackets meet\n",
    "    \n",
    "    # Calculate shared x positions for aligned shorter brackets\n",
    "    shared_x_start = max_x \n",
    "    shared_x_end = max_x + 0.03\n",
    "    long_x_start = max_x + 0.08\n",
    "    long_x_end = max_x + 0.11\n",
    "    \n",
    "    # Add top short bracket (AFFrust to ExpFrust)\n",
    "    group1 = 'AFFrust'\n",
    "    group2 = 'ExpFrust'\n",
    "    y1 = y_positions[group1]\n",
    "    y2 = y_positions[group2]\n",
    "    \n",
    "    ax.plot([shared_x_start, shared_x_end], [y1, y1], lw=1.5, c='black')\n",
    "    ax.plot([shared_x_start, shared_x_end], [y2 - y_spacing, y2 - y_spacing], lw=1.5, c='black')\n",
    "    ax.plot([shared_x_end, shared_x_end], [y1, y2 - y_spacing], lw=1.5, c='black')\n",
    "    \n",
    "    p_value = get_pvalue(group1, group2, pairwise_df)\n",
    "    if p_value is not None:\n",
    "        asterisk = '*' if p_value < significance_level else ''\n",
    "        ax.text(shared_x_end + 0.02, (y1 + y2) / 2, f'p = {p_value:.3f}{asterisk}', \n",
    "                ha='left', va='center', fontsize=10, color='black', rotation=270)\n",
    "    \n",
    "    # Add bottom short bracket (ExpFrust to EvolFrust)\n",
    "    group1 = 'ExpFrust'\n",
    "    group2 = 'EvolFrust'\n",
    "    y1 = y_positions[group1]\n",
    "    y2 = y_positions[group2]\n",
    "    \n",
    "    ax.plot([shared_x_start, shared_x_end], [y1 + y_spacing, y1 + y_spacing], lw=1.5, c='black')\n",
    "    ax.plot([shared_x_start, shared_x_end], [y2, y2], lw=1.5, c='black')\n",
    "    ax.plot([shared_x_end, shared_x_end], [y1 + y_spacing, y2], lw=1.5, c='black')\n",
    "    \n",
    "    p_value = get_pvalue(group1, group2, pairwise_df)\n",
    "    if p_value is not None:\n",
    "        asterisk = '*' if p_value < significance_level else ''\n",
    "        ax.text(shared_x_end + 0.02, (y1 + y2) / 2, f'p = {p_value:.3f}{asterisk}', \n",
    "                ha='left', va='center', fontsize=10, color='black', rotation=270)\n",
    "    \n",
    "    # Add long bracket (AFFrust to EvolFrust)\n",
    "    group1 = 'AFFrust'\n",
    "    group2 = 'EvolFrust'\n",
    "    y1 = y_positions[group1]\n",
    "    y2 = y_positions[group2]\n",
    "    \n",
    "    ax.plot([long_x_start, long_x_end], [y1, y1], lw=1.5, c='black')\n",
    "    ax.plot([long_x_start, long_x_end], [y2, y2], lw=1.5, c='black')\n",
    "    ax.plot([long_x_end, long_x_end], [y1, y2], lw=1.5, c='black')\n",
    "    \n",
    "    p_value = get_pvalue(group1, group2, pairwise_df)\n",
    "    if p_value is not None:\n",
    "        asterisk = '*' if p_value < significance_level else ''\n",
    "        ax.text(long_x_end + 0.02, (y1 + y2) / 2, f'p = {p_value:.3f}{asterisk}', \n",
    "                ha='left', va='center', fontsize=10, color='black', rotation=270)\n",
    "    \n",
    "    # Update the figure's right margin to accommodate vertical text\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "\n",
    "def create_violin_plot(df, pairwise_results, kruskal_stat, kruskal_p, group_col='Frustration_Type', value_col='Spearman_Correlation'):\n",
    "    \"\"\"\n",
    "    Create a publication-quality horizontal violin plot showing the distribution of \n",
    "    Spearman correlations for each frustration type, with annotations for significant variance differences\n",
    "    and Kruskal-Wallis test results.\n",
    "    \"\"\"\n",
    "    # Set up the figure with high DPI for publication quality\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=600)\n",
    "    \n",
    "    # Set publication-quality style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_context(\"paper\", font_scale=1.5)\n",
    "    \n",
    "    # Create a custom order for the frustration types (bottom to top)\n",
    "    order = ['AFFrust', 'ExpFrust', 'EvolFrust']\n",
    "    \n",
    "    # Create a custom color palette in the desired order\n",
    "    palette = {'AFFrust': '#377eb8',    # Blue (AlphaFold)\n",
    "              'ExpFrust': '#e41a1c',    # Red (Experimental)\n",
    "              'EvolFrust': '#4daf4a'}   # Green (Evolutionary)\n",
    "    \n",
    "    # Create custom labels\n",
    "    labels = {'AFFrust': 'AlphaFold',\n",
    "             'ExpFrust': 'Experimental',\n",
    "             'EvolFrust': 'Evolutionary'}\n",
    "    \n",
    "    # Set the gridlines to be behind everything\n",
    "    ax.grid(True, axis='x', linestyle='--', alpha=0.7, zorder=0)\n",
    "    \n",
    "    # Add vertical dotted lines for means (behind the violins)\n",
    "    for frust_type in order:\n",
    "        mean_val = df[df[group_col] == frust_type][value_col].mean()\n",
    "        ax.axvline(x=mean_val, ymin=0, ymax=1, color=palette[frust_type], \n",
    "                  linestyle=':', linewidth=2, alpha=0.8, zorder=1)\n",
    "    \n",
    "    # Create violin plot (in front of the mean lines)\n",
    "    sns.violinplot(\n",
    "        data=df,\n",
    "        x=value_col,\n",
    "        y=group_col,\n",
    "        order=order,\n",
    "        inner='box',\n",
    "        palette=palette,\n",
    "        linewidth=1.5,\n",
    "        zorder=2,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('Distribution of Spearman Correlations\\nby Frustration Type', \n",
    "                pad=20, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Set axis labels with custom font properties\n",
    "    ax.set_xlabel('Spearman Correlation Between Frustration and B-factor', labelpad=15, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Frustration Metric', labelpad=15, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Customize tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Customize y-axis labels\n",
    "    ax.set_yticklabels([labels[tick] for tick in order])\n",
    "    \n",
    "    # Set spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "    \n",
    "    # Adjust plot margins to make room for annotations outside the plot area\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.15, right=0.85, top=0.85)\n",
    "    \n",
    "    # Add annotations for significant variance differences\n",
    "    add_annotations(ax, pairwise_results, order, value_col)\n",
    "    \n",
    "    # Add note about statistical test as a single line in bottom left\n",
    "    fig.text(0.01, 0.01, \n",
    "                'p-values are from pairwise Levene tests for equality of variance with Bonferroni correction.',\n",
    "                ha='left', va='bottom', fontsize=8, style='italic')\n",
    "    \n",
    "    # Prepare Kruskal-Wallis test annotation with significance comment\n",
    "    if kruskal_p < 0.05:\n",
    "        significance_comment = \"Distributions are significantly different.\"\n",
    "    else:\n",
    "        significance_comment = \"No significant differences in distributions.\"\n",
    "    \n",
    "    kruskal_text = f'Kruskal-Wallis Test:\\nH = {kruskal_stat:.2f}, p = {kruskal_p:.3e}\\n{significance_comment}'\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.0, linewidth=1.5)\n",
    "    \n",
    "    # Add Kruskal-Wallis test results as a text box in the top-left corner of the figure\n",
    "    fig.text(0.00, 0.95, kruskal_text, fontsize=12,\n",
    "             verticalalignment='top', bbox=props, ha='left')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def test_variance_equality(df):\n",
    "    \"\"\"\n",
    "    Perform Levene's test to assess the equality of variances across frustration types.\n",
    "    \"\"\"\n",
    "    groups = [group['Spearman_Correlation'].dropna() for name, group in df.groupby('Frustration_Type')]\n",
    "    \n",
    "    stat, p_value = levene(*groups)\n",
    "    \n",
    "    print(\"Levenes Test for Equality of Variances\")\n",
    "    print(f\"Statistic: {stat:.4f}, p-value: {p_value:.4e}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: Significant differences in variances (reject H0)\")\n",
    "    else:\n",
    "        print(\"Result: No significant differences in variances (fail to reject H0)\")\n",
    "    \n",
    "def test_bartlett_variance(df):\n",
    "    \"\"\"\n",
    "    Perform Bartlett's test to assess the equality of variances across frustration types.\n",
    "    \"\"\"\n",
    "    groups = [group['Spearman_Correlation'].dropna() for name, group in df.groupby('Frustration_Type')]\n",
    "    \n",
    "    stat, p_value = bartlett(*groups)\n",
    "    \n",
    "    print(\"Bartletts Test for Equality of Variances\")\n",
    "    print(f\"Statistic: {stat:.4f}, p-value: {p_value:.4e}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: Significant differences in variances (reject H0)\")\n",
    "    else:\n",
    "        print(\"Result: No significant differences in variances (fail to reject H0)\")\n",
    "    \n",
    "def test_distribution_equality(df):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test to assess the equality of distributions across frustration types.\n",
    "    Returns the test statistic and p-value.\n",
    "    \"\"\"\n",
    "    groups = [group['Spearman_Correlation'].dropna() for name, group in df.groupby('Frustration_Type')]\n",
    "    \n",
    "    stat, p_value = kruskal(*groups)\n",
    "    \n",
    "    print(\"Kruskal-Wallis H-Test for Equality of Distributions\")\n",
    "    print(f\"Statistic: {stat:.4f}, p-value: {p_value:.4e}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: Significant differences in distributions (reject H0)\")\n",
    "    else:\n",
    "        print(\"Result: No significant differences in distributions (fail to reject H0)\")\n",
    "    \n",
    "    return stat, p_value\n",
    "\n",
    "def main():\n",
    "    # **Set your data directory**\n",
    "    DATA_DIR = ''\n",
    "    \n",
    "    # Process the data\n",
    "    results_df = process_data(DATA_DIR)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No valid data found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    summary_stats = results_df.groupby('Frustration_Type')['Spearman_Correlation'].agg(['mean', 'std', 'count'])\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Perform Levene's Test for equality of variances\n",
    "    print(\"\\n--- Variance Equality Test (Levene's Test) ---\")\n",
    "    test_variance_equality(results_df)\n",
    "    \n",
    "    # Perform Bartlett's Test for equality of variances\n",
    "    print(\"\\n--- Variance Equality Test (Bartlett's Test) ---\")\n",
    "    test_bartlett_variance(results_df)\n",
    "    \n",
    "    # Perform Kruskal-Wallis Test for equality of distributions\n",
    "    print(\"\\n--- Distribution Equality Test (Kruskal-Wallis Test) ---\")\n",
    "    kruskal_stat, kruskal_p = test_distribution_equality(results_df)\n",
    "    \n",
    "    # Perform pairwise Levene's Tests with Bonferroni correction\n",
    "    print(\"\\n--- Pairwise Variance Equality Tests (Levene's) with Bonferroni Correction ---\")\n",
    "    pairwise_results = pairwise_levene_corrected(\n",
    "        df=results_df,\n",
    "        group_col='Frustration_Type',\n",
    "        value_col='Spearman_Correlation',\n",
    "        alpha=0.05,\n",
    "        correction='bonferroni'\n",
    "    )\n",
    "    print(pairwise_results)\n",
    "    \n",
    "    # Create and display the violin plot with annotations\n",
    "    plt_obj = create_violin_plot(results_df, pairwise_results, kruskal_stat, kruskal_p)\n",
    "    plt_obj.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e454",
   "metadata": {},
   "source": [
    "Script to plot the single example plot used for Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fde9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator \n",
    "\n",
    "# Set Seaborn and Matplotlib styles to match the violin plot's aesthetics\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"deep\")\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 600,             # High resolution\n",
    "    'font.size': 25,               # Increased font size for better readability\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 18,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20,\n",
    "    'figure.figsize': (20, 15),    # Large figure size\n",
    "    'axes.linewidth': 1.5,\n",
    "    'lines.linewidth': 2.5,\n",
    "    'grid.linewidth': 1.0\n",
    "})\n",
    "\n",
    "\n",
    "def lowess_smoothing(x, y, frac=0.1, it=3):\n",
    "    \"\"\"\n",
    "    Applies LOWESS smoothing to the data.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array-like): Independent variable data.\n",
    "        y (array-like): Dependent variable data.\n",
    "        frac (float): The fraction of the data used when estimating each y-value.\n",
    "        it (int): The number of robustifying iterations.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Smoothed y-values.\n",
    "    \"\"\"\n",
    "    lowess = sm.nonparametric.lowess\n",
    "    z = lowess(y, x, frac=frac, it=it, return_sorted=False)\n",
    "    return z\n",
    "\n",
    "def parse_summary_file(file_path, window_size=5, frac=0.1, it=3):\n",
    "    \"\"\"\n",
    "    Parses the summary file and processes the data\n",
    "    \"\"\"\n",
    "    required_cols = [\"AlnIndex\", \"Residue\", \"SecondaryStructure\", \"B_Factor\", \"ExpFrust\", \"AFFrust\", \"EvolFrust\"]\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse data from {file_path}. Error: {e}\")\n",
    "    \n",
    "    if not set(required_cols).issubset(df.columns):\n",
    "        missing = set(required_cols) - set(df.columns)\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # Convert 'n/a' to NaN\n",
    "    for col in [\"B_Factor\", \"ExpFrust\", \"AFFrust\", \"EvolFrust\"]:\n",
    "        df[col] = pd.to_numeric(df[col].replace('n/a', np.nan), errors='coerce')\n",
    "    \n",
    "    df_original = df.copy()\n",
    "    df_for_plot = df.copy()\n",
    "    \n",
    "    # Apply LOWESS smoothing to each metric\n",
    "    for col in [\"B_Factor\", \"ExpFrust\", \"AFFrust\", \"EvolFrust\"]:\n",
    "        x = df_for_plot[\"AlnIndex\"].values\n",
    "        y = df_for_plot[col].values\n",
    "        mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "        if mask.sum() > 0:\n",
    "            y_smooth = lowess_smoothing(x[mask], y[mask], frac=frac, it=it)\n",
    "            df_for_plot.loc[mask, col] = y_smooth\n",
    "        else:\n",
    "            df_for_plot[col] = np.nan\n",
    "    \n",
    "    # Only normalize B-Factor\n",
    "    valid = ~df_for_plot['B_Factor'].isna()\n",
    "    if valid.any():\n",
    "        col_min = df_for_plot.loc[valid, 'B_Factor'].min()\n",
    "        col_max = df_for_plot.loc[valid, 'B_Factor'].max()\n",
    "        if col_max > col_min:\n",
    "            df_for_plot['B_Factor'] = (df_for_plot['B_Factor'] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            df_for_plot['B_Factor'] = 0.0\n",
    "    \n",
    "    # Compute Spearman correlations\n",
    "    corrs = {}\n",
    "    sub = df_original.dropna(subset=[\"B_Factor\",\"ExpFrust\",\"AFFrust\",\"EvolFrust\"])\n",
    "    if not sub.empty:\n",
    "        combos = [\n",
    "            (\"B_Factor\", \"ExpFrust\"),\n",
    "            (\"B_Factor\", \"AFFrust\"),\n",
    "            (\"B_Factor\", \"EvolFrust\")\n",
    "        ]\n",
    "        for (mA, mB) in combos:\n",
    "            if sub[mA].nunique() < 2 or sub[mB].nunique() < 2:\n",
    "                rho, pval = np.nan, np.nan\n",
    "            else:\n",
    "                rho, pval = spearmanr(sub[mA], sub[mB])\n",
    "            corrs[(mA, mB)] = (rho, pval)\n",
    "    \n",
    "    return df_original, df_for_plot, corrs\n",
    "\n",
    "def create_seaborn_figure(df_original, df_plot, corrs):\n",
    "    \"\"\"\n",
    "    Creates a Seaborn figure with:\n",
    "      1) A main line plot (smoothed metrics vs. AlnIndex) on the top row,\n",
    "         with full-height background highlighting and secondary-structure shapes.\n",
    "      2) Three scatter plots in the second row, each having:\n",
    "         - A one-line title for \"B_Factor Rank vs. Frust Metric\"\n",
    "         - A separate Spearman correlation line, placed just below the title\n",
    "           (no overlap, custom color for each frustration metric).\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib import gridspec\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from scipy.stats import spearmanr, linregress\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    ########################################\n",
    "    # HELPER FUNCTIONS FOR HELIX/ARROW SHAPES\n",
    "    ########################################\n",
    "    def create_helix(x_start, width, height=0.25, frequency=2):\n",
    "        \"\"\"Returns (x, y) for a sinusoidal helix from x_start to x_start + width.\"\"\"\n",
    "        num_points = max(int(width * 20), 2)\n",
    "        x = np.linspace(x_start, x_start + width, num_points)\n",
    "        y = height * np.sin(2 * np.pi * frequency * (x - x_start) / width)\n",
    "        return x, y\n",
    "\n",
    "    def create_arrow(x_start, width, height=0.25):\n",
    "        \"\"\"Returns (x, y) outline for a beta-strand arrow shape.\"\"\"\n",
    "        x = [\n",
    "            x_start, x_start,\n",
    "            x_start, x_start + 0.7 * width,\n",
    "            x_start + 0.7 * width, x_start + width,\n",
    "            x_start + 0.7 * width,\n",
    "            x_start + 0.7 * width, x_start,\n",
    "            x_start\n",
    "        ]\n",
    "        y = [\n",
    "            -height / 2,  height / 2,\n",
    "            -height / 2, -height / 2,\n",
    "            -height / 2,  0,\n",
    "             height / 2,\n",
    "             height / 2,  height / 2,\n",
    "            -height / 2\n",
    "        ]\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    ########################################\n",
    "    # CREATE FIGURE WITH GRIDSPEC\n",
    "    ########################################\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(2, 3, figure=fig, height_ratios=[3, 2])\n",
    "    \n",
    "    # Top row (full width) -> main axis\n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    ########################################\n",
    "    # (A) PLOT SMOOTHED LINES\n",
    "    ########################################\n",
    "    metrics = [\"B_Factor\", \"ExpFrust\", \"AFFrust\", \"EvolFrust\"]\n",
    "    colors = {\n",
    "        \"B_Factor\": \"#FF7F00\",\n",
    "        \"ExpFrust\": \"#E41A1C\",\n",
    "        \"AFFrust\": \"#377EB8\",\n",
    "        \"EvolFrust\": \"#4DAF4A\"\n",
    "    }\n",
    "    \n",
    "    for metric in metrics:\n",
    "        label = metric if metric != \"B_Factor\" else \"B-Factor (Normalized)\"\n",
    "        sns.lineplot(\n",
    "            x=\"AlnIndex\",\n",
    "            y=metric,\n",
    "            data=df_plot,\n",
    "            label=label,\n",
    "            color=colors.get(metric, \"black\"),\n",
    "            ax=ax_main,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    # Let Matplotlib calculate initial limits\n",
    "    fig.canvas.draw()\n",
    "    x_min, x_max = ax_main.get_xlim()\n",
    "    y_min, y_max = ax_main.get_ylim()\n",
    "\n",
    "    # Add extra space at the top for SS annotation\n",
    "    extra_top_space = 0.75\n",
    "    ax_main.set_ylim(y_min, y_max + extra_top_space)\n",
    "    # Re-fetch final y-limits after expanding top\n",
    "    y_min, y_max = ax_main.get_ylim()\n",
    "\n",
    "    ########################################\n",
    "    # (B) BACKGROUND HIGHLIGHT BY SECONDARY STRUCTURE\n",
    "    ########################################\n",
    "    if 'SecondaryStructure' in df_plot.columns:\n",
    "        ss_colors = {\n",
    "            'A': ('#800080', 0.1),  # alpha-helix\n",
    "            'B': ('#008080', 0.1),  # beta-sheet\n",
    "            'O': ('#808080', 0.1)   # other\n",
    "        }\n",
    "\n",
    "        # Sort by AlnIndex to ensure left -> right\n",
    "        df_plot_sorted = df_plot.sort_values(by='AlnIndex').reset_index(drop=True)\n",
    "\n",
    "        prev_ss = None\n",
    "        start_x = None\n",
    "\n",
    "        for i in range(len(df_plot_sorted)):\n",
    "            current_ss = df_plot_sorted.loc[i, 'SecondaryStructure']\n",
    "            current_x = df_plot_sorted.loc[i, 'AlnIndex']\n",
    "\n",
    "            # Draw a rectangle whenever the SS type changes\n",
    "            if current_ss != prev_ss:\n",
    "                if prev_ss is not None and prev_ss in ss_colors and start_x is not None:\n",
    "                    color, alpha = ss_colors[prev_ss]\n",
    "                    width = current_x - start_x\n",
    "                    ax_main.add_patch(\n",
    "                        plt.Rectangle(\n",
    "                            (start_x, y_min),\n",
    "                            width,\n",
    "                            y_max - y_min,  # full vertical extent\n",
    "                            facecolor=color,\n",
    "                            alpha=alpha,\n",
    "                            zorder=1\n",
    "                        )\n",
    "                    )\n",
    "                start_x = current_x\n",
    "                prev_ss = current_ss\n",
    "\n",
    "        # Close out the final segment\n",
    "        if prev_ss in ss_colors and start_x is not None:\n",
    "            color, alpha = ss_colors[prev_ss]\n",
    "            last_x = df_plot_sorted['AlnIndex'].iloc[-1]\n",
    "            width = last_x - start_x  # Removed the +1e-9\n",
    "            ax_main.add_patch(\n",
    "                plt.Rectangle(\n",
    "                    (start_x, y_min),\n",
    "                    width,\n",
    "                    y_max - y_min,\n",
    "                    facecolor=color,\n",
    "                    alpha=alpha,\n",
    "                    zorder=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "    ########################################\n",
    "    # (C) SECONDARY STRUCTURE SHAPES (TOP)\n",
    "    ########################################\n",
    "    box_height = 0.6\n",
    "    box_bottom = y_max - box_height\n",
    "    ax_main.add_patch(plt.Rectangle(\n",
    "        (ax_main.get_xlim()[0], box_bottom),\n",
    "        ax_main.get_xlim()[1] - ax_main.get_xlim()[0],\n",
    "        box_height,\n",
    "        facecolor='#f5f5f5',\n",
    "        edgecolor='#d3d3d3',\n",
    "        alpha=0.5,\n",
    "        zorder=2\n",
    "    ))\n",
    "\n",
    "    y_pos = box_bottom + box_height / 2  # midline for shapes\n",
    "\n",
    "    if 'SecondaryStructure' in df_plot.columns:\n",
    "        df_plot_sorted = df_plot.sort_values(by='AlnIndex').reset_index(drop=True)\n",
    "        prev_ss = None\n",
    "        start_x = None\n",
    "\n",
    "        for i in range(len(df_plot_sorted)):\n",
    "            current_ss = df_plot_sorted.loc[i, 'SecondaryStructure']\n",
    "            current_x = df_plot_sorted.loc[i, 'AlnIndex']\n",
    "\n",
    "            if current_ss != prev_ss:\n",
    "                # Draw shape for the old region\n",
    "                if prev_ss is not None and start_x is not None:\n",
    "                    width = current_x - start_x\n",
    "                    if prev_ss == 'A':  # alpha-helix\n",
    "                        x_helix, y_helix = create_helix(start_x, width)\n",
    "                        ax_main.plot(x_helix, y_helix + y_pos,\n",
    "                                     color='#800080', linewidth=2, zorder=3)\n",
    "                    elif prev_ss == 'B':  # beta-strand\n",
    "                        x_arrow, y_arrow = create_arrow(start_x, width)\n",
    "                        ax_main.plot(x_arrow, y_arrow + y_pos,\n",
    "                                     color='#008080', linewidth=2, zorder=3)\n",
    "                    else:\n",
    "                        # \"Other\"\n",
    "                        ax_main.plot([start_x, start_x + width],\n",
    "                                     [y_pos, y_pos],\n",
    "                                     color='#808080', linewidth=2, zorder=3)\n",
    "\n",
    "                start_x = current_x\n",
    "                prev_ss = current_ss\n",
    "\n",
    "        # Final shape\n",
    "        if prev_ss is not None and start_x is not None:\n",
    "            last_x = df_plot_sorted['AlnIndex'].iloc[-1]\n",
    "            width = last_x - start_x\n",
    "            if prev_ss == 'A':\n",
    "                x_helix, y_helix = create_helix(start_x, width)\n",
    "                ax_main.plot(x_helix, y_helix + y_pos,\n",
    "                             color='#800080', linewidth=2, zorder=3)\n",
    "            elif prev_ss == 'B':\n",
    "                x_arrow, y_arrow = create_arrow(start_x, width)\n",
    "                ax_main.plot(x_arrow, y_arrow + y_pos,\n",
    "                             color='#008080', linewidth=2, zorder=3)\n",
    "            else:\n",
    "                ax_main.plot([start_x, start_x + width],\n",
    "                             [y_pos, y_pos],\n",
    "                             color='#808080', linewidth=2, zorder=3)\n",
    "\n",
    "    ########################################\n",
    "    # (D) MAIN PLOT LABELS & LEGENDS\n",
    "    ########################################\n",
    "    ax_main.set_title(\"Smoothed Frustration and B-Factor vs Residue Index\", \n",
    "                      fontsize=24, fontweight='bold', pad=20)\n",
    "    ax_main.set_xlabel(\"Residue Index\", fontsize=20, fontweight='bold')\n",
    "    ax_main.set_ylabel(\"Frustration & Normalized B-Factor\", fontsize=20, fontweight='bold')\n",
    "\n",
    "    metrics_legend = ax_main.legend(title=\"Metrics\", fontsize=14, title_fontsize=16, \n",
    "                                    loc='lower left', frameon=True)\n",
    "    frame = metrics_legend.get_frame()\n",
    "    frame.set_facecolor('white')\n",
    "    frame.set_edgecolor('black')\n",
    "    frame.set_alpha(1)\n",
    "    metrics_legend.set_zorder(10)\n",
    "\n",
    "    ss_legend_elements = [\n",
    "        plt.Line2D([0], [0], color='#800080', linewidth=2, label='-helix'),\n",
    "        plt.Line2D([0], [0], color='#008080', linewidth=2, label='-sheet'),\n",
    "        plt.Line2D([0], [0], color='#808080', linewidth=2, label='other')\n",
    "    ]\n",
    "    ss_legend = ax_main.legend(handles=ss_legend_elements, loc='lower right',\n",
    "                               title='Secondary Structure', fontsize=14, title_fontsize=16,\n",
    "                               frameon=True)\n",
    "    ss_frame = ss_legend.get_frame()\n",
    "    ss_frame.set_facecolor('white')\n",
    "    ss_frame.set_edgecolor('black')\n",
    "    ss_frame.set_alpha(1)\n",
    "    ss_legend.set_zorder(10)\n",
    "    \n",
    "    ax_main.add_artist(metrics_legend)  # Ensure both legends remain visible\n",
    "\n",
    "    ########################################\n",
    "    # (E) SCATTER PLOTS (BOTTOM ROW)\n",
    "    ########################################\n",
    "    scatter_metrics = [\"ExpFrust\", \"AFFrust\", \"EvolFrust\"]\n",
    "    scatter_colors = {\n",
    "        \"ExpFrust\": \"#E41A1C\",\n",
    "        \"AFFrust\": \"#377EB8\",\n",
    "        \"EvolFrust\": \"#4DAF4A\"\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(scatter_metrics):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        \n",
    "        sub = df_original.dropna(subset=[\"B_Factor\", metric])\n",
    "        if sub.empty:\n",
    "            ax.text(0.5, 0.5, \"No Data Available\",\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    transform=ax.transAxes, \n",
    "                    fontsize=14, color='red')\n",
    "            continue\n",
    "        \n",
    "        sub = sub.copy()\n",
    "        sub['B_Factor_Rank'] = sub['B_Factor'].rank(method='average')\n",
    "        sub[f'{metric}_Rank'] = sub[metric].rank(method='average')\n",
    "        \n",
    "        # Spearman correlation\n",
    "        rho, pval = spearmanr(sub['B_Factor'], sub[metric])\n",
    "\n",
    "        # Scatter\n",
    "        sns.scatterplot(\n",
    "            x='B_Factor_Rank',\n",
    "            y=f'{metric}_Rank',\n",
    "            data=sub,\n",
    "            color=scatter_colors.get(metric, \"black\"),\n",
    "            alpha=0.7,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # Optional regression line\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(\n",
    "            sub['B_Factor_Rank'], sub[f'{metric}_Rank']\n",
    "        )\n",
    "        x_vals = np.array(ax.get_xlim())\n",
    "        y_vals = intercept + slope * x_vals\n",
    "        ax.plot(x_vals, y_vals, '--', color='gray', linewidth=2)\n",
    "\n",
    "        # -------------\n",
    "        # 1) Normal one-line title\n",
    "        # -------------\n",
    "        main_title = f\"B_Factor Rank vs {metric} Rank\"\n",
    "        ax.set_title(main_title, fontsize=18, fontweight='bold', pad=35)\n",
    "\n",
    "        # -------------\n",
    "        # 2) Spearman correlation text, BELOW the title\n",
    "        # -------------\n",
    "        corr_text = f\"Spearman  = {rho:.3f} (p={pval:.2e})\"\n",
    "        ax.text(\n",
    "            0.5,   # x in axes coords\n",
    "            1.05,  # y in axes coords (just above the plot area)\n",
    "            corr_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha='center',\n",
    "            va='top',\n",
    "            color=scatter_colors[metric],  # match metric color\n",
    "            fontsize=16,\n",
    "            fontweight='bold',\n",
    "            clip_on=False,\n",
    "            zorder=5\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"B-Factor Rank\", fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel(f\"{metric} Rank\", fontsize=16, fontweight='bold')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    \n",
    "    ########################################\n",
    "    # (F) FORCE THE X-LIMITS TO THE DATA RANGE\n",
    "    ########################################\n",
    "    data_x_min = df_plot[\"AlnIndex\"].min()\n",
    "    data_x_max = df_plot[\"AlnIndex\"].max()\n",
    "    ax_main.set_xlim(data_x_min, data_x_max)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Specify the path to your summary file\n",
    "summary_file_path = \"\"\n",
    "\n",
    "# Example:\n",
    "# summary_file_path = \"data/summary_test001.txt\"\n",
    "\n",
    "# **Parse and Process the Data**\n",
    "\n",
    "\n",
    "# Define the LOWESS parameters\n",
    "window_size = 5   # Not used in LOWESS but kept for compatibility\n",
    "frac = 0.1        # The fraction of the data used when estimating each y-value\n",
    "it = 3            # The number of robustifying iterations\n",
    "\n",
    "# Parse the summary file\n",
    "try:\n",
    "    df_original, df_plot, corrs = parse_summary_file(summary_file_path, window_size=window_size, frac=frac, it=it)\n",
    "    print(\"Data parsing and processing completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Check if data was successfully parsed\n",
    "if 'df_original' in locals() and not df_original.empty:\n",
    "    fig = create_seaborn_figure(df_original, df_plot, corrs)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available to plot.\")\n",
    "\n",
    "\n",
    "#**(Optional) Save the Figure**\n",
    "\n",
    "# Uncomment the lines below to save the figure as a PNG file\n",
    "# output_image_path = \"path/to/save/figure.png\"\n",
    "# fig.savefig(output_image_path, dpi=300)\n",
    "# print(f\"Figure saved to {output_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a7eab",
   "metadata": {},
   "source": [
    "Script used to generate supplimental figures S1-S20, Figure 3, and Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages  \n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "########################################\n",
    "# GLOBAL MAPPINGS AND HELPER FUNCTIONS #\n",
    "########################################\n",
    "\n",
    "# Map internal keys to display names\n",
    "DISPLAY_MAP = {\n",
    "    'ExpFrust_Experimental': 'Experimental Frustration',\n",
    "    'ExpFrust_AlphaFold':   'AlphaFold Frustration',\n",
    "    'EvolFrust':            'Evolutionary Frustration'\n",
    "}\n",
    "\n",
    "def remap_legend(ax, mapping, **legend_kwargs):\n",
    "    \"\"\"\n",
    "    Re-labels the legend entries on `ax` according to `mapping`.\n",
    "    Keeps any suffix (e.g., \" (AUC=0.82)\") intact.\n",
    "    \"\"\"\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_labels = []\n",
    "    for lbl in labels:\n",
    "        key = lbl.split()[0]\n",
    "        rest = lbl[len(key):]\n",
    "        new_labels.append(mapping.get(key, key) + rest)\n",
    "    ax.legend(handles, new_labels, **legend_kwargs)\n",
    "\n",
    "########################################\n",
    "# 1) BASIC SETUP AND HELPER FUNCTIONS  #\n",
    "########################################\n",
    "\n",
    "def read_frustration_file(filepath, file_type='summary'):\n",
    "    \"\"\"\n",
    "    Read and process frustration data from a summary file in the new format.\n",
    "    \n",
    "    New summary file format (tab-separated):\n",
    "        AlnIndex, Residue, SecondaryStructure, B_Factor, ExpFrust, AFFrust, EvolFrust, ...\n",
    "    \n",
    "    For plotting purposes:\n",
    "      - Experimental Frustration is taken from the ExpFrust column.\n",
    "      - AlphaFold Frustration is taken from the AFFrust column (and mapped to ExpFrust).\n",
    "      - Evolutionary Frustration is taken from the EvolFrust column.\n",
    "      \n",
    "    Both frustration DataFrames use the same B_Factor.\n",
    "    \"\"\"\n",
    "    if file_type == 'summary':\n",
    "        df = pd.read_csv(filepath, sep=',', na_values=['n/a'])\n",
    "        \n",
    "        # For Experimental Frustration: use columns: AlnIndex, Residue, SecondaryStructure, B_Factor, ExpFrust\n",
    "        exp_columns = {\n",
    "            'AlnIndex': 'AlnIndex',\n",
    "            'Residue': 'Residue',\n",
    "            'SecondaryStructure': 'SecondaryStructure',\n",
    "            'B_Factor': 'B_Factor',\n",
    "            'ExpFrust': 'ExpFrust'\n",
    "        }\n",
    "        exp_present = [col for col in exp_columns if col in df.columns]\n",
    "        exp_df = df[exp_present].rename(columns=exp_columns)\n",
    "        for v in exp_columns.values():\n",
    "            if v not in exp_df.columns:\n",
    "                exp_df[v] = np.nan\n",
    "        \n",
    "        # For AlphaFold Frustration: use columns: AlnIndex, Residue, SecondaryStructure, B_Factor, AFFrust (mapped to ExpFrust)\n",
    "        af_columns = {\n",
    "            'AlnIndex': 'AlnIndex',\n",
    "            'Residue': 'Residue',\n",
    "            'SecondaryStructure': 'SecondaryStructure',\n",
    "            'B_Factor': 'B_Factor',\n",
    "            'AFFrust': 'ExpFrust'\n",
    "        }\n",
    "        af_present = [col for col in af_columns if col in df.columns]\n",
    "        af_df = df[af_present].rename(columns={k: v for k, v in af_columns.items() if k in af_present})\n",
    "        for v in af_columns.values():\n",
    "            if v not in af_df.columns:\n",
    "                af_df[v] = np.nan\n",
    "        \n",
    "        # Extract Evolutionary Frustration from EvolFrust column (if available)\n",
    "        if 'EvolFrust' in df.columns:\n",
    "            evol_frust = pd.to_numeric(df['EvolFrust'], errors='coerce')\n",
    "        else:\n",
    "            evol_frust = pd.Series([np.nan] * len(df))\n",
    "        \n",
    "        # Ensure numeric columns are numeric\n",
    "        numeric_cols = ['B_Factor', 'ExpFrust']\n",
    "        for col in numeric_cols:\n",
    "            exp_df[col] = pd.to_numeric(exp_df[col], errors='coerce')\n",
    "            af_df[col] = pd.to_numeric(af_df[col], errors='coerce')\n",
    "        \n",
    "        return exp_df, af_df, evol_frust\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only 'summary' is supported.\")\n",
    "\n",
    "def lowess_smoothing(x, y, frac=0.1, it=3):\n",
    "    \"\"\"\n",
    "    Apply LOWESS smoothing to the data.\n",
    "    \"\"\"\n",
    "    mask = ~(pd.isna(x) | pd.isna(y))\n",
    "    x_clean = x[mask]\n",
    "    y_clean = y[mask]\n",
    "    if len(x_clean) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    lowess = sm.nonparametric.lowess\n",
    "    z = lowess(y_clean, x_clean, frac=frac, it=it, return_sorted=False)\n",
    "    return x_clean, z\n",
    "\n",
    "def create_gradient_line(x, y, values, cmap, linestyle='-', linewidth=3):\n",
    "    \"\"\"\n",
    "    Create a gradient line as a collection of segments.\n",
    "    \"\"\"\n",
    "    if len(x) < 2:\n",
    "        return None\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, linestyle=linestyle, linewidth=linewidth)\n",
    "    lc.set_array(values[:-1])\n",
    "    return lc\n",
    "\n",
    "def create_dashed_gradient_line(x, y, values, cmap, linewidth=3, dash_on=10, dash_off=5):\n",
    "    \"\"\"\n",
    "    Create a single dashed gradient line.\n",
    "    \"\"\"\n",
    "    if len(x) < 2:\n",
    "        return None\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    v = np.asarray(values)\n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    seg_lengths = np.sqrt(dx*dx + dy*dy)\n",
    "    dist = np.concatenate(([0], np.cumsum(seg_lengths)))\n",
    "    def color_at_distance(d_val):\n",
    "        return np.interp(d_val, dist, v)\n",
    "    pattern_length = dash_on + dash_off\n",
    "    def get_on_subsegments(s1, s2):\n",
    "        segments_on = []\n",
    "        current = s1\n",
    "        while current < s2:\n",
    "            cycle_pos = (current % pattern_length)\n",
    "            cycle_on_end = current - cycle_pos + dash_on\n",
    "            if cycle_on_end <= current:\n",
    "                next_cycle_start = current - cycle_pos + pattern_length\n",
    "                current = next_cycle_start\n",
    "                continue\n",
    "            seg_start = current\n",
    "            seg_end = min(cycle_on_end, s2)\n",
    "            if seg_end > seg_start:\n",
    "                segments_on.append((seg_start, seg_end))\n",
    "            current = seg_end\n",
    "            cycle_off_end = current - (current % pattern_length) + pattern_length\n",
    "            if cycle_off_end < current:\n",
    "                cycle_off_end += pattern_length\n",
    "            current = max(current, min(cycle_off_end, s2))\n",
    "        return segments_on\n",
    "    all_on_segments = []\n",
    "    color_values = []\n",
    "    for i in range(len(x) - 1):\n",
    "        s1 = dist[i]\n",
    "        s2 = dist[i+1]\n",
    "        if s2 == s1:\n",
    "            continue\n",
    "        on_subs = get_on_subsegments(s1, s2)\n",
    "        if not on_subs:\n",
    "            continue\n",
    "        for (s_on_start, s_on_end) in on_subs:\n",
    "            t1 = (s_on_start - s1) / (s2 - s1)\n",
    "            x1 = x[i] + t1 * (x[i+1] - x[i])\n",
    "            y1 = y[i] + t1 * (y[i+1] - y[i])\n",
    "            t2 = (s_on_end - s1) / (s2 - s1)\n",
    "            x2 = x[i] + t2 * (x[i+1] - x[i])\n",
    "            y2 = y[i] + t2 * (y[i+1] - y[i])\n",
    "            mid = 0.5*(s_on_start + s_on_end)\n",
    "            c_mid = color_at_distance(mid)\n",
    "            all_on_segments.append([[x1, y1], [x2, y2]])\n",
    "            color_values.append(c_mid)\n",
    "    if not all_on_segments:\n",
    "        return None\n",
    "    lc = LineCollection(\n",
    "        all_on_segments,\n",
    "        cmap=cmap,\n",
    "        norm=plt.Normalize(v.min(), v.max()),\n",
    "        linewidth=linewidth,\n",
    "        linestyles='solid'\n",
    "    )\n",
    "    lc.set_array(np.array(color_values))\n",
    "    return lc\n",
    "\n",
    "def create_custom_cmap(vmin, vmax):\n",
    "    \"\"\"\n",
    "    Create a custom colormap that transitions through gray at zero.\n",
    "    \"\"\"\n",
    "    total = abs(vmin) + abs(vmax)\n",
    "    zero_pos = abs(vmin) / total if total != 0 else 0.5\n",
    "    colors = [(0, '#0c1359'), (zero_pos, '#D0D0D0'), (1, '#f05b05')]\n",
    "    return LinearSegmentedColormap.from_list(\"custom\", colors, N=100)\n",
    "\n",
    "def create_helix(x_start, width, height=0.5, frequency=2):\n",
    "    \"\"\"\n",
    "    Create a helix representation for alpha helices.\n",
    "    \"\"\"\n",
    "    num_points = int(width * 20)\n",
    "    x = np.linspace(x_start, x_start + width, num_points)\n",
    "    y = height * np.sin(2 * np.pi * frequency * (x - x_start) / width)\n",
    "    return x, y\n",
    "\n",
    "def create_arrow(x_start, width, height=0.5):\n",
    "    \"\"\"\n",
    "    Create an arrow representation for beta sheets.\n",
    "    \"\"\"\n",
    "    x = [x_start, x_start,\n",
    "         x_start, x_start + 0.7*width,\n",
    "         x_start + 0.7*width, x_start + width,\n",
    "         x_start + 0.7*width,\n",
    "         x_start + 0.7*width, x_start,\n",
    "         x_start]\n",
    "    y = [-height/2, height/2,\n",
    "         -height/2, -height/2,\n",
    "         -height/2, 0,\n",
    "         height/2,\n",
    "         height/2, height/2,\n",
    "         -height/2]\n",
    "    return x, y\n",
    "\n",
    "def create_scatter_subplot(ax, x_data, y_data, color, title, xlabel, ylabel, marker='o'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot with rank correlation.\n",
    "    \"\"\"\n",
    "    mask = ~(pd.isna(x_data) | pd.isna(y_data))\n",
    "    x_clean = x_data[mask]\n",
    "    y_clean = y_data[mask]\n",
    "    if len(x_clean) < 2:\n",
    "        ax.text(0.5, 0.5, \"Insufficient data\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title, fontsize=16, pad=20)\n",
    "        return\n",
    "    try:\n",
    "        x_rank = x_clean.rank()\n",
    "        y_rank = y_clean.rank()\n",
    "        rho, pval = spearmanr(x_clean, y_clean)\n",
    "        sns.scatterplot(x=x_rank, y=y_rank, ax=ax, color=color, alpha=0.6, marker=marker, linewidth=2, s=100)\n",
    "        if len(x_rank.unique()) > 1 and len(y_rank.unique()) > 1:\n",
    "            sns.regplot(x=x_rank, y=y_rank, ax=ax, scatter=False, color='gray', \n",
    "                        line_kws={'linestyle': '--', 'alpha': 0.8})\n",
    "        corr_text = f\" = {rho:.3f}\\np = {pval:.2e}\"\n",
    "        ax.text(0.05, 0.95, corr_text, transform=ax.transAxes, verticalalignment='top', fontsize=12,\n",
    "                color='black', bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        ax.set_title(title, fontsize=16, pad=20)\n",
    "        ax.set_xlabel(xlabel, fontsize=14)\n",
    "        ax.set_ylabel(ylabel, fontsize=14)\n",
    "        ax.tick_params(labelsize=12)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in scatter plot creation: {e}\")\n",
    "        ax.text(0.5, 0.5, \"Error in plot creation\", ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "########################################\n",
    "# 2) MAIN PLOTTING FUNCTION           #\n",
    "########################################\n",
    "\n",
    "def plot_frustration_comparison(summary_filepath, \n",
    "                                box_height_ratio=0.05, \n",
    "                                spacing_ratio=0.075, \n",
    "                                additional_space_ratio=0.30, \n",
    "                                box_padding_ratio=0.02, \n",
    "                                legend_separation_ratio=-0.05):\n",
    "    \"\"\"\n",
    "    Create a comprehensive plot comparing protein frustration data.\n",
    "    \n",
    "    The main plot (row 0) shows the LOWESS-smoothed frustration curves:\n",
    "      - Solid line: Experimental Frustration (from ExpFrust)\n",
    "      - Dashed line: AlphaFold Frustration (from AFFrust)\n",
    "      - Dotted line: Evolutionary Frustration (from EvolFrust)\n",
    "    \n",
    "    Rows 13 show scatter plots of each frustration metric (ranked) vs. the B-Factor (ranked).\n",
    "    \n",
    "    Row 4 shows the summary Spearman correlation for each metric.\n",
    "    \n",
    "    Row 5 shows the normalized smoothed B-Factor.\n",
    "    \n",
    "    Row 6 displays ROC and PrecisionRecall analyses (using quartilebased binary classifications).\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (20, 60),\n",
    "        'font.size': 14,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16\n",
    "    })\n",
    "    \n",
    "    # Read frustration data (Experimental and AlphaFold)\n",
    "    exp_data, af_data, evol_frust = read_frustration_file(summary_filepath)\n",
    "    \n",
    "    # Merge Experimental and AlphaFold data on AlnIndex.\n",
    "    merged_data = exp_data.merge(af_data, on='AlnIndex', suffixes=('_Experimental', '_AlphaFold'))\n",
    "    merged_data['EvolFrust'] = evol_frust\n",
    "    # Use Experimental B_Factor.\n",
    "    if \"B_Factor\" not in merged_data.columns and \"B_Factor_Experimental\" in merged_data.columns:\n",
    "        merged_data[\"B_Factor\"] = merged_data[\"B_Factor_Experimental\"]\n",
    "    \n",
    "    # Create a complete-data mask.\n",
    "    complete_data_mask = (\n",
    "        ~merged_data['ExpFrust_Experimental'].isna() &\n",
    "        ~merged_data['ExpFrust_AlphaFold'].isna() &\n",
    "        ~merged_data['EvolFrust'].isna() &\n",
    "        ~merged_data['B_Factor'].isna()\n",
    "    )\n",
    "    merged_data_filtered = merged_data[complete_data_mask]\n",
    "    if merged_data_filtered.empty or len(merged_data_filtered) < 5:\n",
    "        raise ValueError(\"Insufficient complete data to generate plot.\")\n",
    "    \n",
    "    # Set up grid: 7 rows x 2 columns.\n",
    "    nrows = 7\n",
    "    grid_cols = 2\n",
    "    height_ratios = [3, 2, 2, 2, 3, 2, 3]  # row6 will hold ROC/PR analysis\n",
    "    gs = gridspec.GridSpec(nrows, grid_cols, height_ratios=height_ratios, wspace=0.3, hspace=0.4)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 60))\n",
    "    # Row 0: Main frustration curves.\n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    # Row 4: Summary correlation plot.\n",
    "    ax_corr_summary = fig.add_subplot(gs[4, :])\n",
    "    \n",
    "    # LOWESS smoothing for frustration metrics and B_Factor.\n",
    "    exp_x, exp_smooth = lowess_smoothing(merged_data_filtered['AlnIndex'], merged_data_filtered['ExpFrust_Experimental'])\n",
    "    af_x, af_smooth = lowess_smoothing(merged_data_filtered['AlnIndex'], merged_data_filtered['ExpFrust_AlphaFold'])\n",
    "    evol_x, evol_smooth = lowess_smoothing(merged_data_filtered['AlnIndex'], merged_data_filtered['EvolFrust'])\n",
    "    bf_x, bf_smooth = lowess_smoothing(merged_data_filtered['AlnIndex'], merged_data_filtered['B_Factor'])\n",
    "    \n",
    "    default_y_min, default_y_max = -2, 2\n",
    "    all_y = np.concatenate([exp_smooth, af_smooth, evol_smooth])\n",
    "    finite_mask = np.isfinite(all_y)\n",
    "    try:\n",
    "        if np.any(finite_mask):\n",
    "            y_min = float(np.nanmin(all_y[finite_mask]))\n",
    "            y_max = float(np.nanmax(all_y[finite_mask]))\n",
    "            if not (np.isfinite(y_min) and np.isfinite(y_max)):\n",
    "                y_min, y_max = default_y_min, default_y_max\n",
    "        else:\n",
    "            y_min, y_max = default_y_min, default_y_max\n",
    "        y_range = y_max - y_min\n",
    "        y_padding = y_range * 0.05\n",
    "        plot_y_min = y_min - y_padding\n",
    "        plot_y_max = y_max + y_padding + additional_space_ratio * y_range\n",
    "        if legend_separation_ratio < 0:\n",
    "            plot_y_min += y_range * legend_separation_ratio\n",
    "        elif legend_separation_ratio > 0:\n",
    "            plot_y_max += y_range * legend_separation_ratio\n",
    "        if not (np.isfinite(plot_y_min) and np.isfinite(plot_y_max)):\n",
    "            plot_y_min, plot_y_max = default_y_min, default_y_max\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating plot limits: {e}\")\n",
    "        plot_y_min, plot_y_max = default_y_min, default_y_max\n",
    "\n",
    "    ax_main.set_ylim(plot_y_min, plot_y_max)\n",
    "    \n",
    "    # Create custom colormaps.\n",
    "    cmap_exp = create_custom_cmap(exp_smooth.min(), exp_smooth.max())\n",
    "    cmap_af = create_custom_cmap(af_smooth.min(), af_smooth.max())\n",
    "    cmap_evol = create_custom_cmap(evol_smooth.min(), evol_smooth.max())\n",
    "    \n",
    "    exp_line = create_gradient_line(exp_x, exp_smooth, exp_smooth, cmap_exp, linestyle='-', linewidth=4)\n",
    "    if exp_line:\n",
    "        ax_main.add_collection(exp_line)\n",
    "    af_line = create_dashed_gradient_line(af_x, af_smooth, af_smooth, cmap_af, linewidth=4, dash_on=2, dash_off=2)\n",
    "    if af_line:\n",
    "        ax_main.add_collection(af_line)\n",
    "    evol_line = create_gradient_line(evol_x, evol_smooth, evol_smooth, cmap_evol, linestyle=':', linewidth=2)\n",
    "    if evol_line:\n",
    "        ax_main.add_collection(evol_line)\n",
    "    \n",
    "    x_min = merged_data_filtered['AlnIndex'].min()\n",
    "    x_max = merged_data_filtered['AlnIndex'].max()\n",
    "    ax_main.set_xlim(x_min, x_max)\n",
    "    \n",
    "    ax_main.set_title('Protein Frustration Comparison', fontsize=24, fontweight='bold', pad=20)\n",
    "    ax_main.set_xlabel('Residue Number', fontsize=20, fontweight='bold')\n",
    "    ax_main.set_ylabel('Frustration', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Add legends for frustration types and levels.\n",
    "    legends = []\n",
    "    line_style_legend = [\n",
    "        Line2D([0], [0], color='black', linestyle='-', linewidth=4, label='Experimental Frustration'),\n",
    "        Line2D([0], [0], color='black', linestyle='--', linewidth=4, label='AlphaFold Frustration'),\n",
    "        Line2D([0], [0], color='black', linestyle=':', linewidth=2, label='Evolutionary Frustration')\n",
    "    ]\n",
    "    legends.append(('Frustration Types', line_style_legend))\n",
    "    frustration_level_legend = [\n",
    "        Line2D([0], [0], color='#0c1359', label='Minimally Frustrated', linewidth=3),\n",
    "        Line2D([0], [0], color='#D0D0D0', label='Neutral', linewidth=3),\n",
    "        Line2D([0], [0], color='#f05b05', label='Highly Frustrated', linewidth=3)\n",
    "    ]\n",
    "    legends.append(('Frustration Level', frustration_level_legend))\n",
    "    num_legends = len(legends)\n",
    "    spacing = 1.0/(num_legends+1)\n",
    "    legend_y = 0.02\n",
    "    for i, (title, handles) in enumerate(legends):\n",
    "        x_pos = spacing*(i+1)\n",
    "        legend = ax_main.legend(handles=handles, title=title, fontsize=14, title_fontsize=16,\n",
    "                                loc='lower center', bbox_to_anchor=(x_pos, legend_y),\n",
    "                                frameon=True, ncol=1)\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "        legend.get_frame().set_edgecolor('black')\n",
    "        ax_main.add_artist(legend)\n",
    "    \n",
    "    # Define colors for scatter plots  using the established Spearman colors.\n",
    "    category_colors = {\n",
    "        'Experimental Frustration': '#e41a1c',  # dark red\n",
    "        'AlphaFold Frustration': '#377eb8',      # red\n",
    "        'Evolutionary Frustration': '#4DAF4A'      # green\n",
    "    }\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # Scatter Plots for Each Frustration Metric (Rows 13)\n",
    "    # ---------------------------\n",
    "    bf_rank = merged_data_filtered['B_Factor'].rank()\n",
    "    def plot_scatter_row(row_index, metric_series, metric_label, color):\n",
    "        ax_scatter = fig.add_subplot(gs[row_index, :])\n",
    "        create_scatter_subplot(ax_scatter, bf_rank, metric_series.rank(), color,\n",
    "                               f'{metric_label} vs B-Factor',\n",
    "                               'B-Factor Rank', metric_label)\n",
    "    plot_scatter_row(1, merged_data_filtered['ExpFrust_Experimental'], 'Experimental Frustration', category_colors['Experimental Frustration'])\n",
    "    plot_scatter_row(2, merged_data_filtered['ExpFrust_AlphaFold'], 'AlphaFold Frustration', category_colors['AlphaFold Frustration'])\n",
    "    plot_scatter_row(3, merged_data_filtered['EvolFrust'], 'Evolutionary Frustration', category_colors['Evolutionary Frustration'])\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Summary Correlation Plot (Row 4)\n",
    "    # ---------------------------\n",
    "    summary_correlations = []\n",
    "    metrics_dict = {\n",
    "        'Experimental Frustration': merged_data_filtered['ExpFrust_Experimental'],\n",
    "        'AlphaFold Frustration': merged_data_filtered['ExpFrust_AlphaFold'],\n",
    "        'Evolutionary Frustration': merged_data_filtered['EvolFrust']\n",
    "    }\n",
    "    for metric_name, metric_series in metrics_dict.items():\n",
    "        if not metric_series.isna().all() and not merged_data_filtered['B_Factor'].isna().all():\n",
    "            rho, pval = spearmanr(metric_series, merged_data_filtered['B_Factor'])\n",
    "            summary_correlations.append({'Metric': metric_name, 'Spearman_rho': rho, 'pval': pval})\n",
    "    x_positions = {'Experimental Frustration': 1, 'AlphaFold Frustration': 2, 'Evolutionary Frustration': 3}\n",
    "    for corr in summary_correlations:\n",
    "        x_val = x_positions[corr['Metric']]\n",
    "        y_val = corr['Spearman_rho']\n",
    "        pval = corr['pval']\n",
    "        ax_corr_summary.scatter(x_val, y_val, c=[category_colors[corr['Metric']]], marker='o', s=200, linewidth=2)\n",
    "        if pval < 0.05:\n",
    "            ax_corr_summary.scatter(x_val, y_val, facecolors='none', edgecolors='black', linewidth=2, s=500, marker='s', zorder=6)\n",
    "    ax_corr_summary.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_corr_summary.grid(True, alpha=0.3)\n",
    "    ax_corr_summary.set_xlim(0.5, 3.5)\n",
    "    if summary_correlations:\n",
    "        y_min_corr = min(corr['Spearman_rho'] for corr in summary_correlations)\n",
    "        y_max_corr = max(corr['Spearman_rho'] for corr in summary_correlations)\n",
    "        y_padding_corr = (y_max_corr - y_min_corr) * 0.1 if (y_max_corr - y_min_corr) != 0 else 1\n",
    "    else:\n",
    "        y_min_corr, y_max_corr = -1, 1\n",
    "        y_padding_corr = 0.1\n",
    "    ax_corr_summary.set_ylim(y_min_corr - y_padding_corr, y_max_corr + y_padding_corr)\n",
    "    ax_corr_summary.set_xticks([1, 2, 3])\n",
    "    ax_corr_summary.set_xticklabels(['Experimental Frustration', 'AlphaFold Frustration', 'Evolutionary Frustration'],\n",
    "                                    fontsize=12, ha='center')\n",
    "    ax_corr_summary.set_ylabel(\"Spearman's \", fontsize=16)\n",
    "    ax_corr_summary.spines['top'].set_visible(False)\n",
    "    ax_corr_summary.spines['right'].set_visible(False)\n",
    "    ax_corr_summary.yaxis.set_ticks_position('left')\n",
    "    ax_corr_summary.set_title('Summary of B-Factor Correlations', fontsize=18, pad=20)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Normalized Smoothed B-Factor Plot (Row 5)\n",
    "    # ---------------------------\n",
    "    ax_bf_norm = fig.add_subplot(gs[5, :])\n",
    "    def normalize_series(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val == 0:\n",
    "            return pd.Series([0.5] * len(series), index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    bf_normalized = normalize_series(pd.Series(bf_smooth, index=bf_x.index))\n",
    "    ax_bf_norm.plot(bf_x, bf_normalized, label='Normalized B-Factor', color='blue', linewidth=2)\n",
    "    ax_bf_norm.set_title('Normalized Smoothed B-Factor', fontsize=18, pad=20)\n",
    "    ax_bf_norm.set_xlabel('Residue Number', fontsize=14)\n",
    "    ax_bf_norm.set_ylabel('Normalized B-Factor', fontsize=14)\n",
    "    ax_bf_norm.legend(loc='upper right', fontsize=12)\n",
    "    ax_bf_norm.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # ROC and PrecisionRecall Analysis (Row 6)\n",
    "    # ---------------------------\n",
    "    ax_roc = fig.add_subplot(gs[6, 0])\n",
    "    ax_pr = fig.add_subplot(gs[6, 1])\n",
    "    \n",
    "    # Use quartiles to define extremes. For each frustration metric, binary classification on B_Factor:\n",
    "    # Label 1 if B_Factor >= top quartile; 0 if B_Factor <= bottom quartile.\n",
    "    # Use the frustration metrics continuous value as the score.\n",
    "    roc_summary = []\n",
    "    pr_summary = []\n",
    "    roc_metrics = {\n",
    "        'ExpFrust_Experimental': merged_data_filtered['ExpFrust_Experimental'],\n",
    "        'ExpFrust_AlphaFold':    merged_data_filtered['ExpFrust_AlphaFold'],\n",
    "        'EvolFrust':             merged_data_filtered['EvolFrust']\n",
    "    }\n",
    "    b_series = merged_data_filtered['B_Factor']\n",
    "    for fkey, f_series in roc_metrics.items():\n",
    "        b_low = b_series.quantile(0.25)\n",
    "        b_high = b_series.quantile(0.75)\n",
    "        b_mask = (b_series <= b_low) | (b_series >= b_high)\n",
    "        valid_mask = b_mask\n",
    "        if valid_mask.sum() < 5:\n",
    "            continue\n",
    "        score = f_series[valid_mask]\n",
    "        truth = np.where(b_series[valid_mask] >= b_high, 1, 0)\n",
    "        fpr, tpr, _ = roc_curve(truth, score, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        color = {'ExpFrust_Experimental': '#e41a1c',\n",
    "                 'ExpFrust_AlphaFold':    '#377eb8',\n",
    "                 'EvolFrust':             '#4DAF4A'}[fkey]\n",
    "        ax_roc.plot(fpr, tpr, color=color, linestyle='-', linewidth=2,\n",
    "                    label=f\"{fkey} (AUC={roc_auc:.2f})\")\n",
    "        roc_summary.append({'frustration_metric': fkey, 'roc_auc': roc_auc})\n",
    "        \n",
    "        #  HIGH-B-FACTOR as positive class \n",
    "        truth_high = np.where(b_series[valid_mask] >= b_high, 1, 0)\n",
    "        score_high = score\n",
    "        prec_high, rec_high, _ = precision_recall_curve(truth_high, score_high, pos_label=1)\n",
    "        ap_high = average_precision_score(truth_high, score_high)\n",
    "        ax_pr.plot(rec_high, prec_high,\n",
    "                   color=color, linestyle='-', linewidth=2,\n",
    "                   label=f\"{fkey} high-B AP={ap_high:.2f}\")\n",
    "        pr_summary.append({'frustration_metric': fkey, 'pr_ap': ap_high})\n",
    "\n",
    "        #  LOW-B-FACTOR as positive class \n",
    "        truth_low = np.where(b_series[valid_mask] <= b_low, 1, 0)\n",
    "        score_low = -score_high   # invert so higher score  more likely low-B\n",
    "        prec_low, rec_low, _ = precision_recall_curve(truth_low, score_low, pos_label=1)\n",
    "        ap_low = average_precision_score(truth_low, score_low)\n",
    "        ax_pr.plot(rec_low, prec_low,\n",
    "                   color=color, linestyle='--', linewidth=2,\n",
    "                   label=f\"{fkey} low-B AP={ap_low:.2f}\")\n",
    "        pr_summary.append({'frustration_metric': fkey + '_lowB', 'pr_ap': ap_low})\n",
    "    \n",
    "    # Add fixed baseline at 0.5 in the PR plot.\n",
    "    ax_pr.axhline(y=0.5, color='grey', linestyle='--', linewidth=2, label='PR Baseline')\n",
    "    \n",
    "    # Finalize ROC plot\n",
    "    ax_roc.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    ax_roc.set_xlabel('False Positive Rate', fontsize=14)\n",
    "    ax_roc.set_ylabel('True Positive Rate', fontsize=14)\n",
    "    ax_roc.set_title('ROC Curves: Frustration predicting B-Factor extremes', fontsize=16)\n",
    "    ax_roc.legend(fontsize=10)\n",
    "    remap_legend(ax_roc, DISPLAY_MAP, fontsize=10)\n",
    "    ax_roc.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Finalize PR plot\n",
    "    ax_pr.set_xlabel('Recall', fontsize=14)\n",
    "    ax_pr.set_ylabel('Precision', fontsize=14)\n",
    "    ax_pr.set_title('PrecisionRecall Curves: Frustration predicting B-Factor extremes', fontsize=16)\n",
    "    ax_pr.legend(fontsize=10)\n",
    "    remap_legend(ax_pr, DISPLAY_MAP, fontsize=10)\n",
    "    ax_pr.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Return ROC/PR summaries for final summary figure.\n",
    "    return fig, (roc_summary, pr_summary)\n",
    "\n",
    "########################################\n",
    "# 3) PROCESSING ALL SUBDIRECTORIES     #\n",
    "########################################\n",
    "\n",
    "def process_all_subdirectories(root_dir,\n",
    "                               summary_filename=\"summary.txt\",\n",
    "                               box_height_ratio=0.05,\n",
    "                               spacing_ratio=0.15,\n",
    "                               additional_space_ratio=0.295,\n",
    "                               box_padding_ratio=0.05,\n",
    "                               legend_separation_ratio=-0.75):\n",
    "    \"\"\"\n",
    "    Iterate through all immediate subdirectories of 'root_dir'. For each subdirectory\n",
    "    containing 'summary_filename', generate a frustration-comparison plot and save it\n",
    "    as '{PDBID}_frustration_comparison.pdf' inside that subdirectory, with a suptitle\n",
    "    'Figure S#. {PDBID}'. Then aggregate all metrics into a single 'all_plots.pdf'.\n",
    "    \"\"\"\n",
    "    big_figures = []\n",
    "    all_roc_summary = []\n",
    "    all_pr_summary = []\n",
    "    counter = 1\n",
    "\n",
    "    for entry in os.listdir(root_dir):\n",
    "        subdir_path = os.path.join(root_dir, entry)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "\n",
    "        # Remove any old 'frustration' PDFs\n",
    "        for fn in os.listdir(subdir_path):\n",
    "            if fn.lower().endswith('.pdf') and \"frustration\" in fn.lower():\n",
    "                try:\n",
    "                    os.remove(os.path.join(subdir_path, fn))\n",
    "                except Exception as err:\n",
    "                    print(f\"Error removing file {fn}: {err}\")\n",
    "\n",
    "        summary_filepath = os.path.join(subdir_path, summary_filename)\n",
    "        if not os.path.exists(summary_filepath):\n",
    "            print(f\"Skipping '{entry}': summary file not found.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing '{entry}'...\")\n",
    "            fig, (roc_summary, pr_summary) = plot_frustration_comparison(\n",
    "                summary_filepath,\n",
    "                box_height_ratio=box_height_ratio,\n",
    "                spacing_ratio=spacing_ratio,\n",
    "                additional_space_ratio=additional_space_ratio,\n",
    "                box_padding_ratio=box_padding_ratio,\n",
    "                legend_separation_ratio=legend_separation_ratio\n",
    "            )\n",
    "\n",
    "            # Updated suptitle with numbering\n",
    "            fig.suptitle(f\"Figure S{counter}. {entry}\",\n",
    "                         fontsize=20, fontweight='bold', y=0.9)\n",
    "            counter += 1\n",
    "\n",
    "            output_filename = f\"{entry}_frustration_comparison.pdf\"\n",
    "            output_path = os.path.join(subdir_path, output_filename)\n",
    "            fig.savefig(output_path, dpi=600, bbox_inches='tight')\n",
    "            print(f\"Plot saved successfully at: {output_path}\")\n",
    "\n",
    "            big_figures.append(fig)\n",
    "            # tag cluster for summary tables\n",
    "            for d in roc_summary:\n",
    "                d['cluster'] = entry\n",
    "            for d in pr_summary:\n",
    "                d['cluster'] = entry\n",
    "            all_roc_summary.extend(roc_summary)\n",
    "            all_pr_summary.extend(pr_summary)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping '{entry}' due to error: {e}\")\n",
    "\n",
    "    # Build the final summary PDF if we have any figures\n",
    "    if big_figures:\n",
    "        roc_df = pd.DataFrame(all_roc_summary)\n",
    "        pr_df  = pd.DataFrame(all_pr_summary)\n",
    "\n",
    "        # Split PR into high-B and low-B\n",
    "        pr_high_df = pr_df[~pr_df['frustration_metric'].str.endswith('_lowB')].copy()\n",
    "        pr_low_df  = pr_df[ pr_df['frustration_metric'].str.endswith('_lowB')].copy()\n",
    "        pr_low_df['frustration_metric'] = pr_low_df['frustration_metric'].str.replace('_lowB', '', regex=False)\n",
    "\n",
    "        # Rename codes for plotting\n",
    "        rename_map = {\n",
    "            'ExpFrust_Experimental': 'ExpFrust',\n",
    "            'ExpFrust_AlphaFold':    'AFFrust',\n",
    "            'EvolFrust':             'EvolFrust'\n",
    "        }\n",
    "        roc_df['frustration_metric']      = roc_df['frustration_metric'].replace(rename_map)\n",
    "        pr_high_df['frustration_metric']  = pr_high_df['frustration_metric'].replace(rename_map)\n",
    "        pr_low_df['frustration_metric']   = pr_low_df['frustration_metric'].replace(rename_map)\n",
    "\n",
    "        color_map = {\n",
    "            'ExpFrust':  '#8B0000',\n",
    "            'AFFrust':   '#FF4444',\n",
    "            'EvolFrust': '#4DAF4A'\n",
    "        }\n",
    "\n",
    "        # Create 3-row summary figure\n",
    "        fig_summary = plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # 1) ROC AUC\n",
    "        ax_roc = fig_summary.add_subplot(311)\n",
    "        sns.scatterplot(data=roc_df, x='cluster', y='roc_auc',\n",
    "                        hue='frustration_metric', palette=color_map,\n",
    "                        s=100, ax=ax_roc)\n",
    "        ax_roc.set_title('ROC AUC by Protein and Frustration Metric', fontsize=16)\n",
    "        ax_roc.set_xlabel('Protein PDB_ID')\n",
    "        ax_roc.set_ylabel('ROC AUC')\n",
    "        for metric in roc_df['frustration_metric'].unique():\n",
    "            avg = roc_df.loc[roc_df['frustration_metric']==metric, 'roc_auc'].mean()\n",
    "            ax_roc.axhline(avg, color=color_map[metric], linestyle='--', linewidth=2,\n",
    "                           label=f\"{metric} mean AUC\")\n",
    "        ax_roc.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "        # 2) PR high-B\n",
    "        ax_pr_high = fig_summary.add_subplot(312)\n",
    "        sns.scatterplot(data=pr_high_df, x='cluster', y='pr_ap',\n",
    "                        hue='frustration_metric', palette=color_map,\n",
    "                        s=100, ax=ax_pr_high, legend='brief')\n",
    "        ax_pr_high.set_title('PR Average Precision (High B-factor) by Protein and Frustration Metric', fontsize=16)\n",
    "        ax_pr_high.set_xlabel('Protein PDB_ID')\n",
    "        ax_pr_high.set_ylabel('PR Average Precision')\n",
    "        ax_pr_high.axhline(0.5, color='gray', linestyle='--', linewidth=2, label='Baseline 0.5')\n",
    "        for metric in pr_high_df['frustration_metric'].unique():\n",
    "            avg = pr_high_df.loc[pr_high_df['frustration_metric']==metric, 'pr_ap'].mean()\n",
    "            ax_pr_high.axhline(avg, color=color_map[metric], linestyle=':', linewidth=2,\n",
    "                               label=f\"{metric} mean PR\")\n",
    "        # Wilcoxon\n",
    "        wilcox_texts = [\"Wilcoxon p-values (AP vs 0.5):\"]\n",
    "        for metric in pr_high_df['frustration_metric'].unique():\n",
    "            vals = pr_high_df.loc[pr_high_df['frustration_metric']==metric, 'pr_ap']\n",
    "            stat, p = wilcoxon(vals - 0.5)\n",
    "            wilcox_texts.append(f\"{metric}: p={p:.3f}\")\n",
    "        ax_pr_high.text(1.05, 0.6, \"\\n\".join(wilcox_texts),\n",
    "                        transform=ax_pr_high.transAxes, fontsize=10,\n",
    "                        va='top', ha='left')\n",
    "        ax_pr_high.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "        # 3) PR low-B\n",
    "        ax_pr_low = fig_summary.add_subplot(313)\n",
    "        sns.scatterplot(data=pr_low_df, x='cluster', y='pr_ap',\n",
    "                        hue='frustration_metric', palette=color_map,\n",
    "                        s=100, ax=ax_pr_low, legend='brief')\n",
    "        ax_pr_low.set_title('PR Average Precision (Low B-factor) by Protein and Frustration Metric', fontsize=16)\n",
    "        ax_pr_low.set_xlabel('Protein PDB_ID')\n",
    "        ax_pr_low.set_ylabel('PR Average Precision')\n",
    "        ax_pr_low.axhline(0.5, color='gray', linestyle='--', linewidth=2, label='Baseline 0.5')\n",
    "        for metric in pr_low_df['frustration_metric'].unique():\n",
    "            avg = pr_low_df.loc[pr_low_df['frustration_metric']==metric, 'pr_ap'].mean()\n",
    "            ax_pr_low.axhline(avg, color=color_map[metric], linestyle=':', linewidth=2,\n",
    "                              label=f\"{metric} mean PR\")\n",
    "        wilcox_texts_low = [\"Wilcoxon p-values (AP vs 0.5):\"]\n",
    "        for metric in pr_low_df['frustration_metric'].unique():\n",
    "            vals = pr_low_df.loc[pr_low_df['frustration_metric']==metric, 'pr_ap']\n",
    "            stat, p = wilcoxon(vals - 0.5)\n",
    "            wilcox_texts_low.append(f\"{metric}: p={p:.3f}\")\n",
    "        ax_pr_low.text(1.05, 0.6, \"\\n\".join(wilcox_texts_low),\n",
    "                       transform=ax_pr_low.transAxes, fontsize=10,\n",
    "                       va='top', ha='left')\n",
    "        ax_pr_low.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        # 4) Histogram of Spearman correlations between EvolFrust and ExpFrust\n",
    "        spearman_vals = []\n",
    "        for fig, entry in zip(big_figures, [d['cluster'] for d in all_roc_summary if d['frustration_metric'] == 'EvolFrust']):\n",
    "            subdir_path = os.path.join(root_dir, entry)\n",
    "            summary_path = os.path.join(subdir_path, summary_filename)\n",
    "            if not os.path.exists(summary_path):\n",
    "                continue\n",
    "            try:\n",
    "                exp_df, af_df, evol = read_frustration_file(summary_path)\n",
    "                merged = exp_df.copy()\n",
    "                merged['EvolFrust'] = evol\n",
    "                mask = ~merged['ExpFrust'].isna() & ~merged['EvolFrust'].isna()\n",
    "                if mask.sum() > 2:\n",
    "                    rho, _ = spearmanr(merged['ExpFrust'][mask], merged['EvolFrust'][mask])\n",
    "                    if np.isfinite(rho):\n",
    "                        spearman_vals.append(rho)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping Spearman histogram for {entry}: {e}\")\n",
    "\n",
    "        # Save histogram to separate file\n",
    "        if spearman_vals:\n",
    "            fig_hist = plt.figure(figsize=(12, 5))\n",
    "            ax_hist = fig_hist.add_subplot(111)\n",
    "            sns.histplot(spearman_vals, bins=20, kde=False, color=\"#4DAF4A\", edgecolor='black', ax=ax_hist)\n",
    "            ax_hist.set_title(\"Spearman Correlation: Evolutionary vs Experimental Frustration\", fontsize=16)\n",
    "            ax_hist.set_xlabel(\"Spearman's \", fontsize=14)\n",
    "            ax_hist.set_ylabel(\"Frequency\", fontsize=14)\n",
    "            mean_rho = np.mean(spearman_vals)\n",
    "            ax_hist.axvline(mean_rho, color='black', linestyle='--', linewidth=2)\n",
    "            ax_hist.text(0.95, 0.95,\n",
    "                         f\"Mean = {mean_rho:.2f}\",\n",
    "                         transform=ax_hist.transAxes,\n",
    "                         ha='right', va='top',\n",
    "                         fontsize=12,\n",
    "                         bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "            hist_path = os.path.join(root_dir, \"evol_vs_exp_spearman_histogram.pdf\")\n",
    "            fig_hist.savefig(hist_path, dpi=600, bbox_inches='tight')\n",
    "            plt.close(fig_hist)\n",
    "            print(f\"Spearman histogram saved to: {hist_path}\")\n",
    "        plt.tight_layout()\n",
    "        big_figures.append(fig_summary)\n",
    "\n",
    "        # Save all into one PDF\n",
    "        big_pdf_path = os.path.join(root_dir, \"all_plots.pdf\")\n",
    "        with PdfPages(big_pdf_path) as pdf:\n",
    "            for fig in big_figures:\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "        print(f\"All plots saved in one PDF at: {big_pdf_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No figures to save in the big PDF.\")\n",
    "\n",
    "########################################\n",
    "# 4) MAIN EXECUTION BLOCK              #\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = \"\"  # Change this path as needed\n",
    "    process_all_subdirectories(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306721b2",
   "metadata": {},
   "source": [
    "# The following section contains the scripts used to analyze the 20 highly flexible proteins (20F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139989d",
   "metadata": {},
   "source": [
    "Script to calculate average B-factor for the two separate pdb files (one for each conformation), labeled by PDB ID and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import Bio.PDB\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_b_factors(pdb_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Calculate the average B-factors for each residue in the PDB structure and save them to a file,\n",
    "    including the one-letter residue name, indexed starting from 1.\n",
    "\n",
    "    Parameters:\n",
    "    - pdb_path: str, path to the input PDB file.\n",
    "    - output_txt_path: str, path to the output text file.\n",
    "    \"\"\"\n",
    "    # Mapping three-letter residue names to one-letter codes\n",
    "    one_letter_code = {\n",
    "        \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "        \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "        \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "        \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\",\n",
    "        # Handle uncommon residues with a placeholder\n",
    "        \"UNK\": \"X\"\n",
    "    }\n",
    "\n",
    "    parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure(\"protein\", pdb_path)\n",
    "        b_factors = []\n",
    "        residue_names = []\n",
    "\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    res_id = residue.get_id()[1]\n",
    "                    res_name = residue.get_resname()\n",
    "                    b_factor_list = [atom.get_bfactor() for atom in residue]\n",
    "                    average_b_factor = np.mean(b_factor_list)\n",
    "                    b_factors.append(average_b_factor)\n",
    "                    residue_names.append(one_letter_code.get(res_name, \"X\"))  # Default to 'X' for unknown residues\n",
    "\n",
    "        # Write the output file with re-indexed residue numbers starting from 1\n",
    "        with open(output_txt_path, \"w\") as file:\n",
    "            file.write(\"Residue\\tResidueAA\\tAverage_B_Factor\\n\")\n",
    "            for idx, (aa, b_factor) in enumerate(zip(residue_names, b_factors), start=1):\n",
    "                file.write(f\"{idx}\\t{aa}\\t{b_factor:.3f}\\n\")\n",
    "\n",
    "        print(f\"Average B-factors for {os.path.basename(pdb_path)} saved to {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDB file {pdb_path}: {e}\")\n",
    "\n",
    "def has_chain_letter(filename):\n",
    "    \"\"\"\n",
    "    Determines if the given filename has a chain letter before the .pdb extension.\n",
    "    Assumes that a chain letter is a single uppercase letter appended to the PDB ID.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: str, name of the file.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if a chain letter is present, False otherwise.\n",
    "    \"\"\"\n",
    "    # Regex to match filenames ending with a single uppercase letter before .pdb\n",
    "    return bool(re.match(r'^.+[A-Z]\\.pdb$', filename))\n",
    "\n",
    "def main(base_directory):\n",
    "    \"\"\"\n",
    "    Iterate through each protein directory in the base_directory, find the two PDB files with\n",
    "    chain letters, calculate their average B-factors, and save the results.\n",
    "\n",
    "    Parameters:\n",
    "    - base_directory: str, path to the base directory containing protein subdirectories.\n",
    "    \"\"\"\n",
    "    for protein_dir in os.listdir(base_directory):\n",
    "        protein_path = os.path.join(base_directory, protein_dir)\n",
    "        if os.path.isdir(protein_path):  # Ensure it's a directory\n",
    "            experimental_data_dir = os.path.join(protein_path, \"experimental_data\")\n",
    "            if not os.path.isdir(experimental_data_dir):\n",
    "                print(f\"'experimental_data' directory not found in {protein_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # List all PDB files in the experimental_data directory\n",
    "            all_pdb_files = [f for f in os.listdir(experimental_data_dir) if f.lower().endswith('.pdb')]\n",
    "\n",
    "            # Filter PDB files that have a chain letter in their filename\n",
    "            chain_pdb_files = [f for f in all_pdb_files if has_chain_letter(f)]\n",
    "\n",
    "            if len(chain_pdb_files) != 2:\n",
    "                print(f\"Expected 2 PDB files with chain letters in {experimental_data_dir}, found {len(chain_pdb_files)}. Skipping {protein_dir}.\")\n",
    "                continue\n",
    "\n",
    "            for pdb_file in chain_pdb_files:\n",
    "                pdb_path = os.path.join(experimental_data_dir, pdb_file)\n",
    "                # Define output filename, e.g., 'average_b_factors_1bgxT.txt'\n",
    "                pdb_base = os.path.splitext(pdb_file)[0]\n",
    "                output_txt_filename = f\"average_b_factors_{pdb_base}.txt\"\n",
    "                output_txt_path = os.path.join(experimental_data_dir, output_txt_filename)\n",
    "\n",
    "                # Calculate average B-factors\n",
    "                calculate_average_b_factors(pdb_path, output_txt_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # **Specify your base directory here**\n",
    "    base_directory = \"\"\n",
    "    main(base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7769262",
   "metadata": {},
   "source": [
    "Summarize (compress) the frustratometer outputs for two structures for each sequence. The directory containing the frustratometer output .rar files are expected to be named frustratometer_1 and frustratometer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import patoolib\n",
    "import pandas as pd\n",
    "\n",
    "# Function to clean directories by deleting all files except .rar and removing subdirectories\n",
    "def clean_directory(directory):\n",
    "    \"\"\"\n",
    "    Deletes all files in the directory except .rar files.\n",
    "    Deletes all subdirectories and their contents.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory to clean.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning directory: {directory}\")\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        \n",
    "        # If it's a file\n",
    "        if os.path.isfile(item_path):\n",
    "            if not item.lower().endswith('.rar'):\n",
    "                print(f\"Deleting file: {item_path}\")\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                print(f\"Keeping .rar file: {item_path}\")\n",
    "        \n",
    "        # If it's a directory\n",
    "        elif os.path.isdir(item_path):\n",
    "            print(f\"Deleting directory and its contents: {item_path}\")\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "# Function to extract .rar files from directories\n",
    "def extract_rar_from_directory(input_dir):\n",
    "    \"\"\"\n",
    "    Extracts the first .rar file found in the specified directory.\n",
    "    \"\"\"\n",
    "    print(f\"Scanning directory for .rar files: {input_dir}\")\n",
    "    rar_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.rar')]\n",
    "    if not rar_files:\n",
    "        print(f\"No .rar files found in {input_dir}.\")\n",
    "        return\n",
    "    \n",
    "    rar_file = os.path.join(input_dir, rar_files[0])\n",
    "    extracted_folder = os.path.join(input_dir, \"extracted\")\n",
    "    \n",
    "    try:\n",
    "        patoolib.extract_archive(rar_file, outdir=extracted_folder)\n",
    "        print(f\"Extracted {rar_file} to {extracted_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {rar_file}: {e}\")\n",
    "\n",
    "# Function to locate the .pdb_mutational file\n",
    "def locate_pdb_mutational_file(extracted_folder):\n",
    "    print(f\"Searching for FrustrationData directory in: {extracted_folder}\")\n",
    "    frustration_dirs = [\n",
    "        os.path.join(root, dir)\n",
    "        for root, dirs, files in os.walk(extracted_folder)\n",
    "        for dir in dirs if \"FrustrationData\" in dir\n",
    "    ]\n",
    "    if not frustration_dirs:\n",
    "        raise FileNotFoundError(f\"No FrustrationData directory found in {extracted_folder}\")\n",
    "    \n",
    "    pdb_files = [\n",
    "        os.path.join(frustration_dirs[0], f)\n",
    "        for f in os.listdir(frustration_dirs[0])\n",
    "        if f.endswith('.pdb_mutational')\n",
    "    ]\n",
    "    if not pdb_files:\n",
    "        raise FileNotFoundError(f\"No .pdb_mutational files found in {frustration_dirs[0]}\")\n",
    "    \n",
    "    return pdb_files[0]\n",
    "\n",
    "# Function to process the .pdb_mutational file\n",
    "def process_frustration_file(input_file, output_file):\n",
    "    print(f\"Processing .pdb_mutational file: {input_file}\")\n",
    "    col_names = [\n",
    "        \"Res1\", \"Res2\", \"ChainRes1\", \"ChainRes2\",\n",
    "        \"DensityRes1\", \"DensityRes2\", \"AA1\", \"AA2\",\n",
    "        \"NativeEnergy\", \"DecoyEnergy\", \"SDEnergy\",\n",
    "        \"FrstIndex\", \"Welltype\", \"FrstState\"\n",
    "    ]\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, sep=r'\\s+', comment=\"#\", names=col_names)\n",
    "        residue_data = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            energy_diff = row[\"NativeEnergy\"] - row[\"DecoyEnergy\"]\n",
    "            residues = [(row[\"Res1\"], row[\"AA1\"]), (row[\"Res2\"], row[\"AA2\"])]\n",
    "            for res, aa in residues:\n",
    "                if res not in residue_data:\n",
    "                    residue_data[res] = {\"aa\": aa, \"sum_diff\": 0.0, \"count\": 0}\n",
    "                residue_data[res][\"sum_diff\"] += energy_diff\n",
    "                residue_data[res][\"count\"] += 1\n",
    "\n",
    "        output_data = []\n",
    "        for res, data in sorted(residue_data.items()):\n",
    "            average_diff = data[\"sum_diff\"] / data[\"count\"] if data[\"count\"] > 0 else 0.0\n",
    "            output_data.append([res, data[\"aa\"], average_diff])\n",
    "\n",
    "        reindexed_data = []\n",
    "        for new_residue_index, row in enumerate(output_data, start=1):\n",
    "            reindexed_data.append([new_residue_index, row[1], row[2]])\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"Residue# ResidueAA Difference\\n\")\n",
    "            for row in reindexed_data:\n",
    "                f.write(f\"{row[0]} {row[1]} {row[2]:.4f}\\n\")\n",
    "        print(f\"Output written to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {input_file}: {e}\")\n",
    "\n",
    "# Function to process extracted directories\n",
    "def process_subdirectory(test_dir, sub_dir, output_filename):\n",
    "    subdirectory_path = os.path.join(test_dir, sub_dir)\n",
    "    extracted_folder = os.path.join(subdirectory_path, \"extracted\")\n",
    "    output_file = os.path.join(subdirectory_path, output_filename)\n",
    "\n",
    "    if not os.path.isdir(extracted_folder):\n",
    "        print(f\"Extracted folder not found: {extracted_folder}. Skipping {sub_dir} in {test_dir}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pdb_mutational_file = locate_pdb_mutational_file(extracted_folder)\n",
    "        process_frustration_file(pdb_mutational_file, output_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error processing {sub_dir} in {test_dir}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {sub_dir} in {test_dir}: {e}\")\n",
    "\n",
    "# Main function\n",
    "def main(root_directory):\n",
    "    for dirpath, dirnames, _ in os.walk(root_directory):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.lower() in ['frustratometer_1', 'frustratometer_2']:\n",
    "                target_dir = os.path.join(dirpath, dirname)\n",
    "                print(f\"\\nFound target directory: {target_dir}\")\n",
    "                \n",
    "                # **Clean the target directory before extraction**\n",
    "                clean_directory(target_dir)\n",
    "                \n",
    "                # Proceed with extraction\n",
    "                extract_rar_from_directory(target_dir)\n",
    "                \n",
    "                # Determine output filename based on directory type\n",
    "                if 'frustratometer_2' in dirname.lower():\n",
    "                    output_filename = \"frustration_summary.txt\"\n",
    "                elif 'frustratometer_1' in dirname.lower():\n",
    "                    output_filename = \"frustration_summary.txt\"\n",
    "                else:\n",
    "                    print(f\"Unknown directory type: {dirname}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Process the subdirectory\n",
    "                process_subdirectory(dirpath, dirname, output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # **Specify your root directory here**\n",
    "    root_directory = \"\" \n",
    "    main(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4d4a7",
   "metadata": {},
   "source": [
    "Collect data in summary file for 20F set of proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from itertools import chain\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "from collections import defaultdict\n",
    "from Bio.PDB.Polypeptide import is_aa\n",
    "from Bio.PDB import PDBParser, DSSP\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Logging Setup\n",
    "# -------------------------------------------------------------------------\n",
    "def setup_logging(debug=False):\n",
    "    level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging(debug=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) Directory Validation\n",
    "# -------------------------------------------------------------------------\n",
    "def validate_directory_structure(dir_path):\n",
    "    \"\"\"\n",
    "    Checks which data sources are available in the directory.\n",
    "    Returns a tuple (has_any_data, available_sources).\n",
    "    \"\"\"\n",
    "    available_sources = []\n",
    "    dir_path = Path(dir_path)\n",
    "    \n",
    "    # Detect frustratometer_1 and frustratometer_2 directories\n",
    "    frustratometer_dirs = sorted([d for d in dir_path.iterdir() if d.is_dir() and d.name.startswith('frustratometer_')])\n",
    "    if len(frustratometer_dirs) > 2:\n",
    "        logger.warning(f\"More than two frustratometer directories found in {dir_path}. Only the first two will be considered.\")\n",
    "        frustratometer_dirs = frustratometer_dirs[:2]\n",
    "    for i, subdir in enumerate(frustratometer_dirs, start=1):\n",
    "        required_files = ['frustration_summary.txt']\n",
    "        if all((subdir / file).is_file() for file in required_files):\n",
    "            source_name = f'EXP_FRUST_{i}'\n",
    "            available_sources.append({'type': 'frustratometer', 'name': source_name, 'path': subdir})\n",
    "            logger.debug(f\"Detected frustration source: {source_name} at {subdir}\")\n",
    "        else:\n",
    "            logger.warning(f\"Frustratometer directory {subdir} is missing required files.\")\n",
    "    \n",
    "    # Detect frustratometer_af\n",
    "    frustratometer_af = dir_path / 'frustratometer_af'\n",
    "    if frustratometer_af.is_dir() and (frustratometer_af / 'frustration_af_summary.txt').is_file():\n",
    "        available_sources.append({'type': 'frustratometer_af', 'name': 'AF_FRUST_1', 'path': frustratometer_af})\n",
    "        logger.debug(f\"Detected frustration AF source: AF_FRUST_1 at {frustratometer_af}\")\n",
    "    \n",
    "    # Detect average B-factor files in experimental_data\n",
    "    experimental_data = dir_path / 'experimental_data'\n",
    "    if experimental_data.is_dir():\n",
    "        # Use itertools.chain to combine glob results\n",
    "        b_factor_files = sorted(\n",
    "            chain(\n",
    "                experimental_data.glob('average_b_factors*.csv'), \n",
    "                experimental_data.glob('average_b_factors*.txt')\n",
    "            )\n",
    "        )\n",
    "        b_factor_counter = 1  # Initialize counter for B-factor sources\n",
    "        for bf_file in b_factor_files:\n",
    "            # Extract pdbID and chainID from filename\n",
    "            match = re.search(r'average_b_factors_(\\w+)([A-Z])\\.(csv|txt)$', bf_file.name)\n",
    "            if match:\n",
    "                source_name = f'B_FACTOR_{b_factor_counter}'\n",
    "                available_sources.append({'type': 'b_factor', 'name': source_name, 'path': bf_file})\n",
    "                logger.debug(f\"Detected B-factor source: {source_name} at {bf_file}\")\n",
    "                b_factor_counter += 1\n",
    "            else:\n",
    "                logger.warning(f\"B-factor file {bf_file} does not match the expected naming convention.\")\n",
    "        \n",
    "        # Detect RMSF files if present\n",
    "        rmsf_files = sorted(\n",
    "            chain(\n",
    "                experimental_data.glob('rmsf*.csv'), \n",
    "                experimental_data.glob('rmsf*.txt')\n",
    "            )\n",
    "        )\n",
    "        rmsf_counter = 1  # Initialize counter for RMSF sources\n",
    "        for rmsf_file in rmsf_files:\n",
    "            # Extract pdbID and chainID from filename\n",
    "            match = re.search(r'rmsf_(\\w+)([A-Z])\\.(csv|txt)$', rmsf_file.name)\n",
    "            if match:\n",
    "                source_name = f'RMSF_{rmsf_counter}'\n",
    "                available_sources.append({'type': 'rmsf', 'name': source_name, 'path': rmsf_file})\n",
    "                logger.debug(f\"Detected RMSF source: {source_name} at {rmsf_file}\")\n",
    "                rmsf_counter += 1\n",
    "            else:\n",
    "                logger.warning(f\"RMSF file {rmsf_file} does not match the expected naming convention.\")\n",
    "    \n",
    "    # Detect mj_analysis\n",
    "    mj_analysis = dir_path / 'mj_analysis'\n",
    "    if mj_analysis.is_dir() and (mj_analysis / 'stability_scores.txt').is_file():\n",
    "        available_sources.append({'type': 'mj_analysis', 'name': 'EVOL_FRUST', 'path': mj_analysis / 'stability_scores.txt'})\n",
    "        logger.debug(f\"Detected MJ analysis source: EVOL_FRUST at {mj_analysis / 'stability_scores.txt'}\")\n",
    "    \n",
    "    # Detect PDB files with chain letters\n",
    "    pdb_files = sorted([f for f in experimental_data.glob(\"*.pdb\") if re.search(r'\\w+[A-Z]\\.pdb$', f.name)])\n",
    "    pdb_counter = 1  # Initialize counter for PDB sources\n",
    "    if len(pdb_files) < 2:\n",
    "        logger.warning(f\"Less than two PDB files with chain IDs found in {experimental_data}. Expected two.\")\n",
    "    elif len(pdb_files) > 2:\n",
    "        logger.warning(f\"More than two PDB files with chain IDs found in {experimental_data}. Only the first two will be considered.\")\n",
    "        pdb_files = pdb_files[:2]\n",
    "    \n",
    "    for pdb_file in pdb_files:\n",
    "        if pdb_counter <= len(frustratometer_dirs):\n",
    "            src_name = f'PDB_{pdb_counter}'\n",
    "            available_sources.append({'type': 'pdb', 'name': src_name, 'path': pdb_file, 'frustratometer': f'EXP_FRUST_{pdb_counter}'})\n",
    "            logger.debug(f\"Mapped PDB file {pdb_file.name} to frustration source EXP_FRUST_{pdb_counter}\")\n",
    "            pdb_counter += 1\n",
    "        else:\n",
    "            logger.warning(f\"No corresponding frustratometer directory for PDB file {pdb_file.name}\")\n",
    "    \n",
    "    has_any_data = len(available_sources) > 0\n",
    "    return has_any_data, available_sources\n",
    "\n",
    "def get_valid_directories(root_path):\n",
    "    \"\"\"\n",
    "    Finds all subdirectories that have at least one valid data source.\n",
    "    Returns a list of tuples (directory_path, available_sources).\n",
    "    \"\"\"\n",
    "    valid_dirs = []\n",
    "    root_path = Path(root_path)\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        logger.error(f\"Root directory does not exist: {root_path}\")\n",
    "        return []\n",
    "        \n",
    "    for entry in root_path.iterdir():\n",
    "        if entry.is_dir():\n",
    "            has_data, available_sources = validate_directory_structure(entry)\n",
    "            if has_data:\n",
    "                valid_dirs.append((entry, available_sources))\n",
    "                source_names = ', '.join([src['name'] for src in available_sources])\n",
    "                logger.info(f\"Found directory {entry} with data sources: {source_names}\")\n",
    "            else:\n",
    "                logger.warning(f\"Directory {entry} has no valid data sources, skipping\")\n",
    "    \n",
    "    return sorted(valid_dirs, key=lambda x: str(x[0]))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Parsing Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def parse_frustration_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses frustration_summary.txt or frustration_af_summary.txt.\n",
    "    \n",
    "    Format:\n",
    "        Residue# ResidueAA Difference\n",
    "        1 A -0.1234\n",
    "        2 G 0.5678\n",
    "        3 S -1.2345\n",
    "        ...\n",
    "        \n",
    "    Returns:\n",
    "        sequence_str (str): Amino acid sequence in 1-letter codes.\n",
    "        pos_values (dict): Dictionary mapping 1-based residue positions to frustration differences.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_frustration_file: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"Residue#\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    res_num = int(parts[0])\n",
    "                    aa = parts[1].upper()\n",
    "                    difference = float(parts[2])\n",
    "                    \n",
    "                    if aa not in \"ACDEFGHIKLMNPQRSTVWY\":\n",
    "                        logger.warning(f\"Line {line_number}: Unknown amino acid code '{aa}'. Assigned as 'X'.\")\n",
    "                        aa = 'X'\n",
    "\n",
    "                    lines.append((res_num, aa, difference))\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"parse_frustration_file: Skipping line {line_number} due to ValueError: {line}\")\n",
    "                    continue\n",
    "            else:\n",
    "                logger.warning(f\"parse_frustration_file: Line {line_number} does not have enough parts: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(f\"parse_frustration_file: No valid data found in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines_sorted = sorted(lines, key=lambda x: x[0])\n",
    "    sequence = ''.join([aa for (_, aa, _) in lines_sorted])\n",
    "    pos_values = {res_num: difference for (res_num, _, difference) in lines_sorted}\n",
    "\n",
    "    return sequence, pos_values\n",
    "\n",
    "def parse_b_factor(file_path):\n",
    "    \"\"\"\n",
    "    Parses average_b_factors*.csv or average_b_factors*.txt\n",
    "    Format:\n",
    "      Residue,ResidueAA,Average_B_Factor (for .csv)\n",
    "      or\n",
    "      Residue\\tResidueAA\\tAverage_B_Factor (for .txt)\n",
    "    Returns:\n",
    "        sequence_str (str): Amino acid sequence from file.\n",
    "        pos_values (dict): Dictionary {1-based_pos: B-factor}.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_b_factor: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    # Determine delimiter based on file extension\n",
    "    if file_path.suffix.lower() == '.txt':\n",
    "        delimiter = '\\t'\n",
    "    else:\n",
    "        delimiter = ','\n",
    "    \n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the header to confirm columns\n",
    "        header = f.readline().strip()\n",
    "        expected_headers = ['Residue', 'ResidueAA', 'Average_B_Factor']\n",
    "        actual_headers = header.split(delimiter)\n",
    "        if actual_headers != expected_headers:\n",
    "            logger.warning(f\"parse_b_factor: Unexpected headers in {file_path}: {actual_headers}\")\n",
    "            # Optionally, handle different header names or order here\n",
    "        for line_number, line in enumerate(f, start=2):  # Start at 2 to account for header\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(delimiter)\n",
    "            if len(parts) < 3:\n",
    "                logger.warning(f\"parse_b_factor: Skipping malformed line {line_number}: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                idx = int(parts[0])\n",
    "                aa = parts[1].upper()\n",
    "                bfact = float(parts[2])\n",
    "                lines.append((idx, aa, bfact))\n",
    "            except ValueError as ve:\n",
    "                logger.warning(f\"parse_b_factor: Skipping line {line_number} due to ValueError: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(f\"parse_b_factor: No lines parsed in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    data_dict = {r[0]: (r[1], r[2]) for r in lines}\n",
    "    sorted_indices = sorted(data_dict.keys())\n",
    "\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for pos, idx in enumerate(sorted_indices, start=1):\n",
    "        aa, bfact = data_dict[idx]\n",
    "        seq_builder.append(aa)\n",
    "        pos_values[pos] = bfact\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    logger.debug(f\"parse_b_factor: Parsed sequence length={len(sequence_str)}, B-factors extracted={len(pos_values)}\")\n",
    "    logger.debug(f\"parse_b_factor: B-factor values: {list(pos_values.items())[:5]}...\")  # Show first 5 for brevity\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_evolutionary(file_path):\n",
    "    \"\"\"\n",
    "    Parses stability_scores.txt\n",
    "    Format:\n",
    "      Label Score Difference\n",
    "      M1C -456.89159 -0.0\n",
    "      ...\n",
    "    Returns (sequence_str, pos_values)\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_evolutionary: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    data_map = defaultdict(list)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"Label\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            label = parts[0]\n",
    "            try:\n",
    "                diff = float(parts[2])\n",
    "            except ValueError:\n",
    "                logger.warning(f\"parse_evolutionary: Invalid difference value in line: {line}\")\n",
    "                continue\n",
    "\n",
    "            if label.lower() == \"wt\":\n",
    "                continue\n",
    "\n",
    "            m = re.match(r'([A-Z])(\\d+)([A-Z])', label, re.IGNORECASE)\n",
    "            if m:\n",
    "                native_aa = m.group(1).upper()\n",
    "                idx = int(m.group(2))\n",
    "                data_map[(native_aa, idx)].append(diff)\n",
    "\n",
    "    if not data_map:\n",
    "        logger.debug(f\"parse_evolutionary: No valid lines in {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    index_to_aa = {}\n",
    "    index_to_diff = {}\n",
    "\n",
    "    used_positions = set()\n",
    "    sorted_keys = sorted(data_map.keys(), key=lambda x: x[1])\n",
    "    for (aa, i) in sorted_keys:\n",
    "        if i in used_positions:\n",
    "            continue\n",
    "        used_positions.add(i)\n",
    "        diffs = data_map[(aa, i)]\n",
    "        avg_diff = sum(diffs)/len(diffs) if diffs else 0.0\n",
    "        index_to_aa[i] = aa\n",
    "        index_to_diff[i] = avg_diff\n",
    "\n",
    "    sorted_indices = sorted(index_to_aa.keys())\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for pos, idx in enumerate(sorted_indices, start=1):\n",
    "        seq_builder.append(index_to_aa[idx])\n",
    "        pos_values[pos] = index_to_diff[idx]\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_rmsf(file_path):\n",
    "    \"\"\"\n",
    "    Parses experimental_data/rmsf*.csv or rmsf*.txt of the form:\n",
    "      26,A,2.472\n",
    "      27,A,2.308\n",
    "      28,A,2.657\n",
    "      ...\n",
    "    We reindex so that the first residue (e.g. 26) -> 1, second (27) -> 2, etc.\n",
    "    Returns (sequence_str, pos_values) => {1-based_pos: RMSF}.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_rmsf: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(',')\n",
    "            if len(parts) < 3:\n",
    "                logger.warning(f\"parse_rmsf: Skipping malformed line {line_number}: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                old_idx = int(parts[0])\n",
    "                rmsf_val = float(parts[2])\n",
    "                lines.append((old_idx, rmsf_val))\n",
    "            except ValueError:\n",
    "                logger.warning(f\"parse_rmsf: Skipping invalid line {line_number}: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not lines:\n",
    "        logger.debug(\"parse_rmsf: No valid lines in RMSF file.\")\n",
    "        return \"\", {}\n",
    "\n",
    "    # Sort by the original residue index just in case\n",
    "    lines_sorted = sorted(lines, key=lambda x: x[0])\n",
    "\n",
    "    # Determine offset\n",
    "    offset = lines_sorted[0][0] - 1  # e.g., if first index is 26, offset is 25\n",
    "    pos_values = {}\n",
    "    seq_builder = []\n",
    "    for (old_idx, val) in lines_sorted:\n",
    "        new_idx = old_idx - offset\n",
    "        pos_values[new_idx] = val\n",
    "        # Use a dummy 'A' (or any single-letter code) for alignment\n",
    "        seq_builder.append(\"A\")\n",
    "\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "def parse_mutation_scores(file_path):\n",
    "    \"\"\"\n",
    "    Parses mutation_scores.txt for Mutability.\n",
    "    The file has the columns:\n",
    "       segment  mutant  pos wt subs    frequency  column_conservation  effect_prediction_epistatic\n",
    "    For each unique residue position (given by the 'pos' column) the function computes\n",
    "    the negative average of the effect_prediction_epistatic score.\n",
    "    \n",
    "    Returns:\n",
    "        sequence_str (str): Concatenated wt residues for each position (ordered by pos).\n",
    "        pos_values (dict): Mapping (1-based sequential) of the negative average effect per position.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        logger.debug(f\"parse_mutation_scores: File not found {file_path}\")\n",
    "        return \"\", {}\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    wt_map = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "        for line_number, line in enumerate(f, start=2):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Use regex to split on one or more whitespace characters.\n",
    "            parts = re.split(r'\\s+', line)\n",
    "            if len(parts) < 8:\n",
    "                logger.warning(f\"parse_mutation_scores: Skipping malformed line {line_number}: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                pos = int(parts[2])\n",
    "                wt_res = parts[3].upper()\n",
    "                effect = float(parts[7])\n",
    "                data[pos].append(effect)\n",
    "                wt_map[pos] = wt_res\n",
    "            except ValueError as ve:\n",
    "                logger.warning(f\"parse_mutation_scores: Error parsing line {line_number}: {line}\")\n",
    "                continue\n",
    "\n",
    "    if not data:\n",
    "        return \"\", {}\n",
    "\n",
    "    sorted_positions = sorted(data.keys())\n",
    "    seq_builder = []\n",
    "    pos_values = {}\n",
    "    for count, pos in enumerate(sorted_positions, start=1):\n",
    "        avg_effect = sum(data[pos]) / len(data[pos])\n",
    "        neg_avg = -avg_effect\n",
    "        seq_builder.append(wt_map[pos])\n",
    "        pos_values[count] = neg_avg  # using sequential index in the new sequence\n",
    "    sequence_str = \"\".join(seq_builder)\n",
    "    logger.debug(f\"parse_mutation_scores: Parsed mutation score sequence length={len(sequence_str)}\")\n",
    "    return sequence_str, pos_values\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Alignment Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def align_two(seqA, seqB, gap_open=-2, gap_extend=-0.5):\n",
    "    \"\"\"\n",
    "    Attempt a global alignment with Biopython pairwise2.\n",
    "    Returns (alignedA, alignedB).\n",
    "    \"\"\"\n",
    "    seqA = seqA.upper()\n",
    "    seqB = seqB.upper()\n",
    "\n",
    "    if len(seqA) == 0 and len(seqB) == 0:\n",
    "        logger.debug(\"align_two: Both sequences empty => '' \")\n",
    "        return \"\", \"\"\n",
    "    if len(seqA) == 0:\n",
    "        logger.debug(f\"align_two: SeqA empty, SeqB length={len(seqB)} => trivial alignment\")\n",
    "        return \"-\" * len(seqB), seqB\n",
    "    if len(seqB) == 0:\n",
    "        logger.debug(f\"align_two: SeqB empty, SeqA length={len(seqA)} => trivial alignment\")\n",
    "        return seqA, \"-\" * len(seqA)\n",
    "\n",
    "    logger.debug(f\"align_two: Attempting global alignment: len(seqA)={len(seqA)}, len(seqB)={len(seqB)}\")\n",
    "    alignments = pairwise2.align.globalms(seqA, seqB, 2, -1, gap_open, gap_extend)\n",
    "    if not alignments:\n",
    "        logger.warning(\"align_two: No alignment from Biopython => trivial fallback.\")\n",
    "        max_len = max(len(seqA), len(seqB))\n",
    "        if len(seqA) == max_len:\n",
    "            return seqA, seqB + \"-\"*(len(seqA)-len(seqB))\n",
    "        else:\n",
    "            return seqA + \"-\"*(len(seqB)-len(seqA)), seqB\n",
    "\n",
    "    best = alignments[0]\n",
    "    return best[0], best[1]\n",
    "\n",
    "def merge_val_alignment(alnA, alnB, valA, valB):\n",
    "    \"\"\"\n",
    "    Merge aligned sequences with their values.\n",
    "    Returns (aligned_valsA, aligned_valsB) as lists.\n",
    "    \"\"\"\n",
    "    aligned_valsA = []\n",
    "    aligned_valsB = []\n",
    "    origA_pos = 1\n",
    "    origB_pos = 1\n",
    "    \n",
    "    for i in range(len(alnA)):\n",
    "        cA = alnA[i]\n",
    "        cB = alnB[i]\n",
    "\n",
    "        if cA == '-':\n",
    "            aligned_valsA.append('n/a')\n",
    "        else:\n",
    "            aligned_valsA.append(valA.get(origA_pos, 'n/a'))\n",
    "            origA_pos += 1\n",
    "\n",
    "        if cB == '-':\n",
    "            aligned_valsB.append('n/a')\n",
    "        else:\n",
    "            aligned_valsB.append(valB.get(origB_pos, 'n/a'))\n",
    "            origB_pos += 1\n",
    "            \n",
    "    return aligned_valsA, aligned_valsB\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) Multiple Sequence Alignment\n",
    "# -------------------------------------------------------------------------\n",
    "def multi_align_sequences(seq_list):\n",
    "    \"\"\"\n",
    "    Progressive multiple sequence alignment.\n",
    "    seq_list: [(name, seq_str, val_dict), ...]\n",
    "    Returns a list of (name, final_aln_seq, final_aln_vals).\n",
    "    \"\"\"\n",
    "    if not seq_list:\n",
    "        logger.debug(\"multi_align_sequences: No sequences to align.\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize with the first sequence as the master\n",
    "    master_name, master_seq, master_vals = seq_list[0]\n",
    "    master_aln_seq = master_seq\n",
    "    master_aln_vals = [master_vals.get(i, 'n/a') for i in range(1, len(master_seq)+1)]\n",
    "    \n",
    "    aligned_sequences = [{'name': master_name, 'aln_seq': master_aln_seq, 'aln_vals': master_aln_vals}]\n",
    "    \n",
    "    for name, seq, vals in seq_list[1:]:\n",
    "        logger.debug(f\"multi_align: Aligning MASTER({master_name}) with {name}\")\n",
    "        # Align master sequence with the new sequence\n",
    "        alnA, alnB = align_two(master_aln_seq, seq)\n",
    "        # Merge values based on alignment\n",
    "        new_master_vals, new_seq_vals = merge_val_alignment(alnA, alnB, \n",
    "                                                             {i+1: v for i, v in enumerate(master_aln_vals)}, \n",
    "                                                             vals)\n",
    "        # Update master alignment\n",
    "        master_aln_seq = alnA\n",
    "        master_aln_vals = new_master_vals\n",
    "        aligned_sequences[0]['aln_seq'] = master_aln_seq\n",
    "        aligned_sequences[0]['aln_vals'] = master_aln_vals\n",
    "        # Add the new sequence alignment\n",
    "        aligned_sequences.append({'name': name, 'aln_seq': alnB, 'aln_vals': new_seq_vals})\n",
    "    \n",
    "    # After progressive alignment, ensure all sequences have the same length\n",
    "    final_len = len(master_aln_seq)\n",
    "    for seq in aligned_sequences:\n",
    "        aln_length = len(seq['aln_seq'])\n",
    "        if aln_length < final_len:\n",
    "            padding = '-' * (final_len - aln_length)\n",
    "            seq['aln_seq'] += padding\n",
    "            seq['aln_vals'] += ['n/a'] * (final_len - aln_length)\n",
    "        elif aln_length > final_len:\n",
    "            logger.warning(f\"{seq['name']} alignment is longer than master!? Truncating.\")\n",
    "            seq['aln_seq'] = seq['aln_seq'][:final_len]\n",
    "            seq['aln_vals'] = seq['aln_vals'][:final_len]\n",
    "    \n",
    "    # Prepare final data\n",
    "    final_data = []\n",
    "    for seq in aligned_sequences:\n",
    "        final_data.append((seq['name'], seq['aln_seq'], seq['aln_vals']))\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def map_ss_to_alignment(ss_maps, residue_seq, aligned_dict):\n",
    "    \"\"\"\n",
    "    Maps multiple secondary structure assignments to aligned sequence positions.\n",
    "    \n",
    "    ss_maps: dict of {frustratometer_name: ss_map}\n",
    "    residue_seq: aligned master sequence\n",
    "    aligned_dict: {source_name: (aln_seq, aln_vals)}\n",
    "    \n",
    "    Returns:\n",
    "        ss_result: dict of {SecondaryStructure_<frustratometer_name>: [ss_assignments]}\n",
    "    \"\"\"\n",
    "    ss_result = {}\n",
    "    \n",
    "    for frustratometer_name, ss_map in ss_maps.items():\n",
    "        if not ss_map:\n",
    "            ss_result[f'SecondaryStructure_{frustratometer_name}'] = ['n/a'] * len(residue_seq)\n",
    "            continue\n",
    "        \n",
    "        # Check if frustration source exists in aligned_dict\n",
    "        if frustratometer_name not in aligned_dict:\n",
    "            logger.warning(f\"Frustratometer source {frustratometer_name} not found in alignment for SS mapping.\")\n",
    "            ss_result[f'SecondaryStructure_{frustratometer_name}'] = ['n/a'] * len(residue_seq)\n",
    "            continue\n",
    "        \n",
    "        aln_seq, aln_vals = aligned_dict[frustratometer_name]\n",
    "        \n",
    "        # Initialize SS list\n",
    "        ss_list = ['n/a'] * len(residue_seq)\n",
    "        \n",
    "        ss_pos = 1\n",
    "        for i, (master_res, src_res) in enumerate(zip(residue_seq, aln_seq)):\n",
    "            if src_res == '-' or aln_vals[i] == 'n/a':\n",
    "                ss_list[i] = 'n/a'\n",
    "            else:\n",
    "                ss_list[i] = ss_map.get(ss_pos, 'o')\n",
    "                ss_pos += 1\n",
    "                \n",
    "        ss_result[f'SecondaryStructure_{frustratometer_name}'] = ss_list\n",
    "    \n",
    "    return ss_result\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) PDB Parsing Function\n",
    "# -------------------------------------------------------------------------\n",
    "def parse_secondary_structure_from_pdb(pdb_file_path):\n",
    "    \"\"\"\n",
    "    Parses a PDB file to extract per-residue secondary structure assignments using DSSP.\n",
    "    Returns:\n",
    "        ss_map (dict): Mapping from sequential position (1-based) to 'A', 'B', or 'o'.\n",
    "    \"\"\"\n",
    "    ss_map = {}\n",
    "    pdb_file = Path(pdb_file_path)\n",
    "    \n",
    "    if not pdb_file.is_file():\n",
    "        logger.error(f\"PDB file not found: {pdb_file_path}\")\n",
    "        return ss_map\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    try:\n",
    "        structure = parser.get_structure('protein', pdb_file)\n",
    "        model = structure[0]\n",
    "        chains = list(model.get_chains())\n",
    "        logger.debug(f\"Found chains: {[chain.id for chain in chains]}\")\n",
    "        \n",
    "        # Initialize DSSP\n",
    "        dssp = DSSP(model, str(pdb_file), dssp='/opt/homebrew/bin/mkdssp')\n",
    "        logger.debug(f\"DSSP successful, found {len(dssp.keys())} residues\")\n",
    "        \n",
    "        # Sort residues by chain ID and residue number\n",
    "        sorted_keys = sorted(dssp.keys(), key=lambda x: (x[1][0], x[1][1]))  # (chain_id, res_num)\n",
    "        \n",
    "        # Assign sequential positions\n",
    "        for seq_pos, key in enumerate(sorted_keys, start=1):\n",
    "            ss = dssp[key][2]\n",
    "            if ss in ('H', 'G', 'I'):\n",
    "                ss_code = 'A'  # Alpha-helix\n",
    "            elif ss in ('E', 'B'):\n",
    "                ss_code = 'B'  # Beta-sheet\n",
    "            else:\n",
    "                ss_code = 'o'  # Other/loop\n",
    "            ss_map[seq_pos] = ss_code\n",
    "            logger.debug(f\"Assigned SS for sequential position {seq_pos}: {ss} -> {ss_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing PDB: {str(e)}\")\n",
    "        return ss_map\n",
    "\n",
    "    logger.info(f\"Successfully parsed {len(ss_map)} residues with SS assignments from {pdb_file_path}\")\n",
    "    return ss_map\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7) Main Processing Function\n",
    "# -------------------------------------------------------------------------\n",
    "def process_directory(root_directory, output_dir=None):\n",
    "    \"\"\"\n",
    "    Process a directory containing subdirectories with data.\n",
    "    \"\"\"\n",
    "    root_directory = Path(root_directory)\n",
    "    \n",
    "    # Set up summary data directory\n",
    "    if output_dir:\n",
    "        summary_data_dir = Path(output_dir)\n",
    "    else:\n",
    "        summary_data_dir = root_directory / \"summary_data\"\n",
    "        \n",
    "    try:\n",
    "        summary_data_dir.mkdir(exist_ok=True)\n",
    "        logger.info(f\"Using summary directory: {summary_data_dir}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create summary directory: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get valid directories and their available sources\n",
    "    dir_info = get_valid_directories(root_directory)\n",
    "\n",
    "    if not dir_info:\n",
    "        logger.error(\"No directories with valid data sources found.\")\n",
    "        return\n",
    "\n",
    "    # Process each directory\n",
    "    for dir_path, available_sources in dir_info:\n",
    "        protein_id = dir_path.name\n",
    "        logger.info(f\"Processing: {dir_path}\")\n",
    "        \n",
    "        # Initialize data containers\n",
    "        data_sources = []\n",
    "        ss_maps = {}  # Secondary Structure mappings per frustratometer\n",
    "    \n",
    "        try:\n",
    "            # Extract PDB to frustratometer mapping\n",
    "            pdb_to_frustratometer = {}\n",
    "            for src in available_sources:\n",
    "                if src['type'] == 'pdb':\n",
    "                    pdb_name = src['path'].name\n",
    "                    frustratometer_name = src.get('frustratometer')\n",
    "                    if frustratometer_name:\n",
    "                        pdb_to_frustratometer[frustratometer_name] = src['name']\n",
    "                        logger.debug(f\"Mapping frustratometer {frustratometer_name} to PDB file {pdb_name}\")\n",
    "\n",
    "            # Parse frustration sources\n",
    "            for src in available_sources:\n",
    "                src_type = src['type']\n",
    "                src_name = src['name']\n",
    "                src_path = src['path']\n",
    "                \n",
    "                if src_type == 'frustratometer':\n",
    "                    # Parse frustration summary\n",
    "                    frustration_summary = src_path / \"frustration_summary.txt\"\n",
    "                    seq, vals = parse_frustration_file(str(frustration_summary))\n",
    "                    if seq:\n",
    "                        data_sources.append((src_name, seq, vals))\n",
    "                        logger.debug(f\"Parsed frustration source {src_name}: length={len(seq)}\")\n",
    "                \n",
    "                elif src_type == 'frustratometer_af':\n",
    "                    # Parse AlphaFold frustration summary\n",
    "                    frustration_af_summary = src_path / \"frustration_af_summary.txt\"\n",
    "                    seq, vals = parse_frustration_file(str(frustration_af_summary))\n",
    "                    if seq:\n",
    "                        data_sources.append((src_name, seq, vals))\n",
    "                        logger.debug(f\"Parsed AlphaFold frustration source {src_name}: length={len(seq)}\")\n",
    "                \n",
    "                elif src_type == 'b_factor':\n",
    "                    # Parse B-factor file\n",
    "                    seq, vals = parse_b_factor(str(src_path))\n",
    "                    if seq:\n",
    "                        data_sources.append((src_name, seq, vals))\n",
    "                        logger.debug(f\"Parsed B-factor source {src_name}: length={len(seq)}, B-factors count={len(vals)}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"B-factor source {src_name} was parsed but returned empty data.\")\n",
    "                \n",
    "                elif src_type == 'rmsf':\n",
    "                    # Parse RMSF file\n",
    "                    seq, vals = parse_rmsf(str(src_path))\n",
    "                    if seq:\n",
    "                        data_sources.append((src_name, seq, vals))\n",
    "                        logger.debug(f\"Parsed RMSF source {src_name}: length={len(seq)}\")\n",
    "                \n",
    "                elif src_type == 'mj_analysis':\n",
    "                    # Parse Evolutionary data\n",
    "                    seq, vals = parse_evolutionary(str(src_path))\n",
    "                    if seq:\n",
    "                        data_sources.append((src_name, seq, vals))\n",
    "                        logger.debug(f\"Parsed Evolutionary source {src_name}: length={len(seq)}\")\n",
    "    \n",
    "            # Parse Secondary Structure for each PDB and map to frustratometer\n",
    "            for frustratometer_name, pdb_source_name in pdb_to_frustratometer.items():\n",
    "                # Find the corresponding PDB source in available_sources\n",
    "                pdb_src = next((src for src in available_sources if src['name'] == pdb_source_name), None)\n",
    "                if pdb_src:\n",
    "                    ss_map = parse_secondary_structure_from_pdb(str(pdb_src['path']))\n",
    "                    ss_maps[frustratometer_name] = ss_map\n",
    "                    if ss_map:\n",
    "                        logger.info(f\"Secondary structure information extracted from {pdb_src['path'].name} for {frustratometer_name}.\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Secondary structure information could not be extracted from {pdb_src['path'].name} for {frustratometer_name}. Assigning 'n/a' to all residues.\")\n",
    "                else:\n",
    "                    logger.warning(f\"No PDB source found for frustratometer {frustratometer_name}\")\n",
    "    \n",
    "            # Collect available data sources for alignment\n",
    "            if not data_sources:\n",
    "                logger.warning(f\"No valid sequence data found in {dir_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Found {len(data_sources)} valid data sources for alignment\")\n",
    "\n",
    "            # Perform multiple alignment\n",
    "            aligned = multi_align_sequences(data_sources)\n",
    "            if not aligned:\n",
    "                logger.warning(f\"Alignment failed for {dir_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Successfully aligned {len(aligned)} sequences\")\n",
    "\n",
    "            # The reference alignment is the first entry\n",
    "            residue_seq = aligned[0][1]\n",
    "            final_len = len(residue_seq)\n",
    "            logger.debug(f\"Final alignment length: {final_len}\")\n",
    "\n",
    "            # Build a lookup for name -> (aln_seq, aln_vals)\n",
    "            aligned_dict = {x[0]: (x[1], x[2]) for x in aligned}\n",
    "            logger.debug(f\"Available data types in alignment: {list(aligned_dict.keys())}\")\n",
    "\n",
    "            # Process mutation scores for Mutability\n",
    "            mut_scores_file = dir_path / \"evc_output\" / \"couplings\" / \"mutation_scores.txt\"\n",
    "            if mut_scores_file.is_file():\n",
    "                mut_seq, mut_vals = parse_mutation_scores(str(mut_scores_file))\n",
    "                if mut_seq:\n",
    "                    logger.debug(f\"Parsed mutation scores: sequence length {len(mut_seq)}\")\n",
    "                    # Align the mutation score sequence to the master residue sequence.\n",
    "                    aln_master, aln_mut = align_two(residue_seq, mut_seq)\n",
    "                    # Create a dummy dictionary for the master alignment (not used) with length equal to mut_seq length.\n",
    "                    dummy_master = {i+1: 'n/a' for i in range(len(mut_seq))}\n",
    "                    _, aligned_mut_vals = merge_val_alignment(aln_master, aln_mut, dummy_master, mut_vals)\n",
    "                else:\n",
    "                    logger.warning(\"Mutation scores file parsed but returned empty sequence.\")\n",
    "                    aligned_mut_vals = ['n/a'] * final_len\n",
    "            else:\n",
    "                logger.warning(\"Mutation scores file not found.\")\n",
    "                aligned_mut_vals = ['n/a'] * final_len\n",
    "\n",
    "            # Prepare columns\n",
    "            raw_index = list(range(1, final_len+1))\n",
    "            raw_res = list(residue_seq)\n",
    "            \n",
    "            # Assign Secondary Structure based on alignment\n",
    "            ss_assignments = map_ss_to_alignment(ss_maps, residue_seq, aligned_dict)\n",
    "            if not ss_assignments:\n",
    "                ss_assignments = {}\n",
    "            \n",
    "            # Initialize dictionary to hold all values\n",
    "            summary_dict = {\n",
    "                'AlnIndex': raw_index,\n",
    "                'Residue': raw_res\n",
    "            }\n",
    "\n",
    "            # Add SS columns\n",
    "            for ss_col, ss_vals in ss_assignments.items():\n",
    "                summary_dict[ss_col] = ss_vals\n",
    "\n",
    "            # Dynamically add all available data sources to the summary\n",
    "            for source_name in aligned_dict:\n",
    "                seq_aln, vals_aln = aligned_dict[source_name]\n",
    "                summary_dict[source_name] = vals_aln\n",
    "\n",
    "            # Add the new column for Mutability\n",
    "            summary_dict[\"Mutability\"] = aligned_mut_vals\n",
    "\n",
    "            # Convert the summary dictionary to a DataFrame\n",
    "            df_summary = pd.DataFrame(summary_dict)\n",
    "\n",
    "            # Define the order of columns\n",
    "            columns_order = ['AlnIndex', 'Residue']\n",
    "            # Add SS columns first\n",
    "            ss_columns = sorted([col for col in df_summary.columns if col.startswith('SecondaryStructure_')])\n",
    "            columns_order.extend(ss_columns)\n",
    "            # Then add the new mutation susceptibility column\n",
    "            columns_order.append(\"Mutability\")\n",
    "            # Finally, add the remaining data source columns\n",
    "            data_columns = sorted([col for col in df_summary.columns if col not in columns_order])\n",
    "            columns_order.extend(data_columns)\n",
    "\n",
    "            df_summary = df_summary[columns_order]\n",
    "\n",
    "            # -------------------------\n",
    "            # Modified Output: Write CSV instead of TXT\n",
    "            # -------------------------\n",
    "            # Write to summary.csv inside each directory\n",
    "            out_path = dir_path / \"summary.csv\"\n",
    "            df_summary.to_csv(out_path, index=False, na_rep='n/a')\n",
    "            logger.info(f\"Wrote summary to {out_path}\")\n",
    "\n",
    "            # Also write to the central summary_data directory\n",
    "            summary_filename = f\"summary_{dir_path.name}.csv\"\n",
    "            summary_path = summary_data_dir / summary_filename\n",
    "            df_summary.to_csv(summary_path, index=False, na_rep='n/a')\n",
    "            logger.info(f\"Wrote summary to {summary_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dir_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return summary_data_dir\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 9) Example Usage\n",
    "# -------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"\"\n",
    "    summary_dir = process_directory(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7462d84",
   "metadata": {},
   "source": [
    "Generate Violin plots of distributions of spearman correlation coefficients between frustration and B-factor for 20F (Figure 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, levene, bartlett, kruskal, mannwhitneyu\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def compute_spearman(x, y):\n",
    "    \"\"\"\n",
    "    Compute Spearman correlation with improved handling of constant arrays.\n",
    "    Returns None if correlation cannot be computed.\n",
    "    \"\"\"\n",
    "    mask = x.notna() & y.notna()\n",
    "    if mask.sum() > 1:\n",
    "        x_valid = x[mask]\n",
    "        y_valid = y[mask]\n",
    "        if x_valid.std() == 0 or y_valid.std() == 0:\n",
    "            return None\n",
    "        try:\n",
    "            corr, _ = spearmanr(x_valid, y_valid)\n",
    "            return corr if not np.isnan(corr) else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_data(data_dir):\n",
    "    \"\"\"\n",
    "    Process all data files in the specified directory and compute Spearman correlations.\n",
    "    \n",
    "    Expects files with (at least) the following columns:\n",
    "      - B_FACTOR_1\n",
    "      - B_FACTOR_2\n",
    "      - EXP_FRUST_1\n",
    "      - EXP_FRUST_2\n",
    "      - EVOL_FRUST\n",
    "      \n",
    "    Computes Spearman correlations for the following pairs:\n",
    "      1. B_FACTOR_1 vs EXP_FRUST_1\n",
    "      2. B_FACTOR_1 vs EXP_FRUST_2\n",
    "      3. B_FACTOR_2 vs EXP_FRUST_1\n",
    "      4. B_FACTOR_2 vs EXP_FRUST_2\n",
    "      5. B_FACTOR_1 vs EVOL_FRUST\n",
    "      6. B_FACTOR_2 vs EVOL_FRUST\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    required_cols = ['B_FACTOR_1', 'B_FACTOR_2', 'EXP_FRUST_1', 'EXP_FRUST_2', 'EVOL_FRUST']\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(('.txt', '.csv')):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                sep = '\\t' if filename.endswith('.txt') else ','\n",
    "                df = pd.read_csv(filepath, sep=sep, na_values=['n/a', 'N/A'])\n",
    "                missing = [col for col in required_cols if col not in df.columns]\n",
    "                if missing:\n",
    "                    print(f\"Skipping {filename}: Missing columns {missing}\")\n",
    "                    continue\n",
    "                for col in required_cols:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                b_factors = ['B_FACTOR_1', 'B_FACTOR_2']\n",
    "                exp_frust = ['EXP_FRUST_1', 'EXP_FRUST_2']\n",
    "                evol_frust = ['EVOL_FRUST']\n",
    "                \n",
    "                # Experimental frustration comparisons\n",
    "                for b in b_factors:\n",
    "                    for f in exp_frust:\n",
    "                        corr = compute_spearman(df[b], df[f])\n",
    "                        if corr is not None:\n",
    "                            results.append({\n",
    "                                'Protein': filename,\n",
    "                                'Pair': f\"{b} vs {f}\",\n",
    "                                'Spearman_Correlation': corr\n",
    "                            })\n",
    "                # Evolutionary frustration comparisons\n",
    "                for b in b_factors:\n",
    "                    for f in evol_frust:\n",
    "                        corr = compute_spearman(df[b], df[f])\n",
    "                        if corr is not None:\n",
    "                            results.append({\n",
    "                                'Protein': filename,\n",
    "                                'Pair': f\"{b} vs {f}\",\n",
    "                                'Spearman_Correlation': corr\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def pairwise_mannwhitney_corrected(df, group_col, value_col, alpha=0.05, correction='bonferroni'):\n",
    "    \"\"\"\n",
    "    Perform pairwise MannWhitney U tests between all group combinations with Bonferroni correction.\n",
    "    \"\"\"\n",
    "    groups = df[group_col].unique()\n",
    "    pairwise = list(combinations(groups, 2))\n",
    "    results = []\n",
    "    p_values = []\n",
    "    for (group1, group2) in pairwise:\n",
    "        data1 = df[df[group_col] == group1][value_col].dropna()\n",
    "        data2 = df[df[group_col] == group2][value_col].dropna()\n",
    "        stat, p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "        results.append({'Group1': group1, 'Group2': group2, 'Statistic': stat, 'p-value': p})\n",
    "        p_values.append(p)\n",
    "    adjusted = multipletests(p_values, alpha=alpha, method=correction)\n",
    "    adjusted_pvals = adjusted[1]\n",
    "    reject = adjusted[0]\n",
    "    for i, res in enumerate(results):\n",
    "        res['Adjusted p-value'] = adjusted_pvals[i]\n",
    "        res['Reject H0'] = reject[i]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def add_all_mw_annotations(ax, mw_df, order, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Add bracket annotations for ALL pairwise MannWhitney U tests that are significant.\n",
    "    \n",
    "    For each pair (of groups given by 'order'), this function draws a bracket to the right \n",
    "    of the plot and annotates the adjusted p-valuebut only if the p-value is below the significance_level.\n",
    "    \n",
    "    A greedy algorithm assigns a \"level\" (vertical offset) so that overlapping brackets are staggered.\n",
    "    \"\"\"\n",
    "    # Map each group to its y-axis position \n",
    "    y_positions = {group: idx for idx, group in enumerate(order)}\n",
    "    xlim = ax.get_xlim()\n",
    "    max_x = xlim[1]\n",
    "    base_offset = max_x * 0.10  # base horizontal offset from the plot\n",
    "    hline_length = max_x * 0.02  # horizontal length of bracket lines\n",
    "    text_offset = max_x * 0.015  # extra horizontal space before text\n",
    "    \n",
    "    # List to store already used annotation levels\n",
    "    annotations = []\n",
    "    \n",
    "    # Create all pairwise combinations from the order list\n",
    "    pairs = []\n",
    "    for i in range(len(order)):\n",
    "        for j in range(i+1, len(order)):\n",
    "            pairs.append((order[i], order[j]))\n",
    "    # Sort pairs by vertical span (shorter spans first)\n",
    "    pairs = sorted(pairs, key=lambda pair: (y_positions[pair[1]] - y_positions[pair[0]], y_positions[pair[0]]))\n",
    "    \n",
    "    for group1, group2 in pairs:\n",
    "        mask = ((mw_df['Group1'] == group1) & (mw_df['Group2'] == group2)) | \\\n",
    "               ((mw_df['Group1'] == group2) & (mw_df['Group2'] == group1))\n",
    "        if mw_df[mask].empty:\n",
    "            continue\n",
    "        p_val = mw_df[mask]['Adjusted p-value'].values[0]\n",
    "        # Only annotate if significant\n",
    "        if p_val >= significance_level:\n",
    "            continue\n",
    "        y1 = y_positions[group1]\n",
    "        y2 = y_positions[group2]\n",
    "        level = 0\n",
    "        # Increase level until no overlapping bracket is found\n",
    "        while any(level == lev and not (y2 <= y_low or y1 >= y_high) for (y_low, y_high, lev) in annotations):\n",
    "            level += 1\n",
    "        annotations.append((y1, y2, level))\n",
    "        x0 = max_x + level * (hline_length + base_offset)\n",
    "        # Draw bracket lines\n",
    "        ax.plot([x0, x0 + hline_length], [y1, y1], lw=1.5, c='darkred')\n",
    "        ax.plot([x0, x0 + hline_length], [y2, y2], lw=1.5, c='darkred')\n",
    "        ax.plot([x0 + hline_length, x0 + hline_length], [y1, y2], lw=1.5, c='darkred')\n",
    "        asterisk = '*' if p_val < significance_level else ''\n",
    "        ax.text(x0 + hline_length + text_offset, (y1 + y2) / 2,\n",
    "                f'p = {p_val:.3f}{asterisk}', \n",
    "                ha='left', va='center', fontsize=10, color='darkred', rotation=270)\n",
    "    \n",
    "    plt.subplots_adjust(right=0.85)\n",
    "\n",
    "def create_violin_plot(df, mw_results, kruskal_stat, kruskal_p,\n",
    "                       group_col='Pair', value_col='Spearman_Correlation'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=600)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_context(\"paper\", font_scale=1.5)\n",
    "\n",
    "    order = [\n",
    "        \"B_FACTOR_1 vs EXP_FRUST_1\",\n",
    "        \"B_FACTOR_2 vs EXP_FRUST_1\",\n",
    "        \"B_FACTOR_1 vs EXP_FRUST_2\",\n",
    "        \"B_FACTOR_2 vs EXP_FRUST_2\",\n",
    "        \"B_FACTOR_1 vs EVOL_FRUST\",\n",
    "        \"B_FACTOR_2 vs EVOL_FRUST\"\n",
    "    ]\n",
    "\n",
    "    palette_colors = sns.color_palette(\"Blues\", n_colors=len(order))\n",
    "    palette = dict(zip(order, palette_colors))\n",
    "\n",
    "\n",
    "    ax.grid(True, axis='x', linestyle='--', alpha=0.7, zorder=0)\n",
    "\n",
    "    # vertical dashed lines at each group's mean\n",
    "    for pair in order:\n",
    "        mean_val = df[df[group_col] == pair][value_col].mean()\n",
    "        ax.axvline(x=mean_val,\n",
    "                   color=palette[pair],\n",
    "                   linestyle=':',\n",
    "                   linewidth=2,\n",
    "                   alpha=0.8,\n",
    "                   zorder=1)\n",
    "\n",
    "    # draw the violin plot with our new blue gradient palette\n",
    "    sns.violinplot(\n",
    "        data=df,\n",
    "        x=value_col,\n",
    "        y=group_col,\n",
    "        order=order,\n",
    "        inner='box',\n",
    "        palette=palette,\n",
    "        linewidth=1.5,\n",
    "        zorder=2,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title('Distribution of Spearman Correlations\\n(B-factor vs. Frustration Metrics)', \n",
    "                 pad=20, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Spearman Correlation', labelpad=15, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('', labelpad=15, fontsize=14, fontweight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15, left=0.15, right=0.85, top=0.85)\n",
    "\n",
    "    # annotate significant pairwise MW tests\n",
    "    add_all_mw_annotations(ax, mw_results, order, significance_level=0.05)\n",
    "\n",
    "    fig.text(0.01, 0.01, \n",
    "             'Annotations: Only significant pairwise MannWhitney U tests (Bonferroni corrected) are shown as brackets to the right.',\n",
    "             ha='left', va='bottom', fontsize=8, style='italic')\n",
    "\n",
    "    # KruskalWallis box\n",
    "    if kruskal_p < 0.05:\n",
    "        significance_comment = \"Distributions are significantly different.\"\n",
    "    else:\n",
    "        significance_comment = \"No significant differences in distributions.\"\n",
    "    kruskal_text = f'KruskalWallis Test:\\nH = {kruskal_stat:.2f}, p = {kruskal_p:.3e}\\n{significance_comment}'\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.0, linewidth=1.5)\n",
    "    fig.text(0.00, 0.95, kruskal_text, fontsize=12,\n",
    "             verticalalignment='top', bbox=props, ha='left')\n",
    "\n",
    "    return plt\n",
    "    \n",
    "def test_distribution_equality(df):\n",
    "    \"\"\"\n",
    "    Perform KruskalWallis test to assess equality of distributions across pairs.\n",
    "    Returns the test statistic and p-value.\n",
    "    \"\"\"\n",
    "    groups = [group['Spearman_Correlation'].dropna() for name, group in df.groupby('Pair')]\n",
    "    stat, p_value = kruskal(*groups)\n",
    "    print(\"KruskalWallis Test for Equality of Distributions\")\n",
    "    print(f\"Statistic: {stat:.4f}, p-value: {p_value:.4e}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: Significant differences (reject H0)\")\n",
    "    else:\n",
    "        print(\"Result: No significant differences (fail to reject H0)\")\n",
    "    return stat, p_value\n",
    "\n",
    "def main():\n",
    "    # Set your data directory \n",
    "    DATA_DIR = ''\n",
    "    results_df = process_data(DATA_DIR)\n",
    "    if results_df.empty:\n",
    "        print(\"No valid data found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    summary_stats = results_df.groupby('Pair')['Spearman_Correlation'].agg(['mean', 'std', 'count'])\n",
    "    print(summary_stats)\n",
    "    \n",
    "    print(\"\\n--- Distribution Equality Test (KruskalWallis Test) ---\")\n",
    "    kruskal_stat, kruskal_p = test_distribution_equality(results_df)\n",
    "    \n",
    "    print(\"\\n--- Pairwise Distribution Comparisons (MannWhitney U Tests) with Bonferroni Correction ---\")\n",
    "    mw_results = pairwise_mannwhitney_corrected(\n",
    "        df=results_df,\n",
    "        group_col='Pair',\n",
    "        value_col='Spearman_Correlation',\n",
    "        alpha=0.05,\n",
    "        correction='bonferroni'\n",
    "    )\n",
    "    print(mw_results)\n",
    "    \n",
    "    plt_obj = create_violin_plot(results_df, mw_results, kruskal_stat, kruskal_p)\n",
    "    plt_obj.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35d99f",
   "metadata": {},
   "source": [
    "Generate pairwise spearman difference violin plots (figure 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a62123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, wilcoxon\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1. Functions to compute Spearman correlations per file\n",
    "# -----------------------------\n",
    "\n",
    "def compute_spearman(x, y):\n",
    "    \"\"\"\n",
    "    Compute Spearman correlation with improved handling of constant arrays.\n",
    "    Returns None if correlation cannot be computed.\n",
    "    \"\"\"\n",
    "    mask = x.notna() & y.notna()\n",
    "    if mask.sum() > 1:\n",
    "        x_valid = x[mask]\n",
    "        y_valid = y[mask]\n",
    "        if x_valid.std() == 0 or y_valid.std() == 0:\n",
    "            return None\n",
    "        try:\n",
    "            corr, _ = spearmanr(x_valid, y_valid)\n",
    "            return corr if not np.isnan(corr) else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_data(data_dir):\n",
    "    \"\"\"\n",
    "    Process all data files in the specified directory and compute six Spearman correlations per protein.\n",
    "    \n",
    "    Expects files with at least the following columns:\n",
    "      - B_FACTOR_1\n",
    "      - B_FACTOR_2\n",
    "      - EXP_FRUST_1\n",
    "      - EXP_FRUST_2\n",
    "      - EVOL_FRUST\n",
    "      \n",
    "    For each protein (file), the following correlations are computed:\n",
    "      1. B_FACTOR_1 vs EXP_FRUST_1\n",
    "      2. B_FACTOR_1 vs EXP_FRUST_2\n",
    "      3. B_FACTOR_2 vs EXP_FRUST_1\n",
    "      4. B_FACTOR_2 vs EXP_FRUST_2\n",
    "      5. B_FACTOR_1 vs EVOL_FRUST\n",
    "      6. B_FACTOR_2 vs EVOL_FRUST\n",
    "      \n",
    "    Returns a DataFrame with columns: Protein, Pair, Spearman_Correlation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    required_cols = ['B_FACTOR_1', 'B_FACTOR_2', 'EXP_FRUST_1', 'EXP_FRUST_2', 'EVOL_FRUST']\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(('.txt', '.csv')):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                sep = '\\t' if filename.endswith('.txt') else ','\n",
    "                df = pd.read_csv(filepath, sep=sep, na_values=['n/a', 'N/A'])\n",
    "                missing = [col for col in required_cols if col not in df.columns]\n",
    "                if missing:\n",
    "                    print(f\"Skipping {filename}: Missing columns {missing}\")\n",
    "                    continue\n",
    "                for col in required_cols:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                protein_id = filename  \n",
    "                \n",
    "                b_factors = ['B_FACTOR_1', 'B_FACTOR_2']\n",
    "                exp_frust = ['EXP_FRUST_1', 'EXP_FRUST_2']\n",
    "                evol_frust = ['EVOL_FRUST']\n",
    "                \n",
    "                # Compute experimental correlations (for each B-factor with each experimental frustration)\n",
    "                for b in b_factors:\n",
    "                    for f in exp_frust:\n",
    "                        corr = compute_spearman(df[b], df[f])\n",
    "                        if corr is not None:\n",
    "                            results.append({\n",
    "                                'Protein': protein_id,\n",
    "                                'Pair': f\"{b} vs {f}\",\n",
    "                                'Spearman_Correlation': corr\n",
    "                            })\n",
    "                # Compute evolutionary correlations (for each B-factor with EVOL_FRUST)\n",
    "                for b in b_factors:\n",
    "                    for f in evol_frust:\n",
    "                        corr = compute_spearman(df[b], df[f])\n",
    "                        if corr is not None:\n",
    "                            results.append({\n",
    "                                'Protein': protein_id,\n",
    "                                'Pair': f\"{b} vs {f}\",\n",
    "                                'Spearman_Correlation': corr\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2. Reorganize the data and compute differences per protein\n",
    "# -----------------------------\n",
    "\n",
    "def compute_difference_df(corr_df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with columns [Protein, Pair, Spearman_Correlation] (6 rows per protein),\n",
    "    pivot it so that each protein becomes a row with columns for each Pair. Then compute\n",
    "    the differences:\n",
    "      For B_FACTOR_1:\n",
    "          diff_B1_EXP1 = (B_FACTOR_1 vs EVOL_FRUST) - (B_FACTOR_1 vs EXP_FRUST_1)\n",
    "          diff_B1_EXP2 = (B_FACTOR_1 vs EVOL_FRUST) - (B_FACTOR_1 vs EXP_FRUST_2)\n",
    "      For B_FACTOR_2:\n",
    "          diff_B2_EXP1 = (B_FACTOR_2 vs EVOL_FRUST) - (B_FACTOR_2 vs EXP_FRUST_1)\n",
    "          diff_B2_EXP2 = (B_FACTOR_2 vs EVOL_FRUST) - (B_FACTOR_2 vs EXP_FRUST_2)\n",
    "    Returns a long-format DataFrame with columns: Protein, Diff_Type, Difference.\n",
    "    \"\"\"\n",
    "    pivot_df = corr_df.pivot(index='Protein', columns='Pair', values='Spearman_Correlation')\n",
    "    needed = [\"B_FACTOR_1 vs EXP_FRUST_1\", \"B_FACTOR_1 vs EXP_FRUST_2\", \"B_FACTOR_1 vs EVOL_FRUST\",\n",
    "              \"B_FACTOR_2 vs EXP_FRUST_1\", \"B_FACTOR_2 vs EXP_FRUST_2\", \"B_FACTOR_2 vs EVOL_FRUST\"]\n",
    "    for col in needed:\n",
    "        if col not in pivot_df.columns:\n",
    "            pivot_df[col] = np.nan\n",
    "    \n",
    "    pivot_df['diff_B1_EXP1'] = pivot_df[\"B_FACTOR_1 vs EVOL_FRUST\"] - pivot_df[\"B_FACTOR_1 vs EXP_FRUST_1\"]\n",
    "    pivot_df['diff_B1_EXP2'] = pivot_df[\"B_FACTOR_1 vs EVOL_FRUST\"] - pivot_df[\"B_FACTOR_1 vs EXP_FRUST_2\"]\n",
    "    pivot_df['diff_B2_EXP1'] = pivot_df[\"B_FACTOR_2 vs EVOL_FRUST\"] - pivot_df[\"B_FACTOR_2 vs EXP_FRUST_1\"]\n",
    "    pivot_df['diff_B2_EXP2'] = pivot_df[\"B_FACTOR_2 vs EVOL_FRUST\"] - pivot_df[\"B_FACTOR_2 vs EXP_FRUST_2\"]\n",
    "    \n",
    "    diff_cols = ['diff_B1_EXP1', 'diff_B1_EXP2', 'diff_B2_EXP1', 'diff_B2_EXP2']\n",
    "    diff_long = pivot_df[diff_cols].reset_index().melt(id_vars=\"Protein\", \n",
    "                                                       value_vars=diff_cols,\n",
    "                                                       var_name=\"Diff_Type\",\n",
    "                                                       value_name=\"Difference\")\n",
    "    diff_long = diff_long.dropna(subset=[\"Difference\"])\n",
    "    return diff_long\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3. Plotting and Statistical Testing (Horizontal Violins)\n",
    "# -----------------------------\n",
    "\n",
    "def plot_difference_violins(diff_long):\n",
    "    \"\"\"\n",
    "    Plot horizontal violin plots for the difference distributions (one violin per Diff_Type).\n",
    "    Draw mean lines, then the zero reference line *behind* the violins, and finally the violins.\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 7), dpi=300)\n",
    "\n",
    "    # 1) Define the exact order of the four categories\n",
    "    order = [\"diff_B1_EXP1\", \"diff_B1_EXP2\", \"diff_B2_EXP1\", \"diff_B2_EXP2\"]\n",
    "    \n",
    "    # 2) Generate a blue gradient palette of length 4\n",
    "    palette_colors = sns.color_palette(\"Blues\", n_colors=len(order))\n",
    "    palette = dict(zip(order, palette_colors))\n",
    "\n",
    "    # 3) Draw each group's mean as a dashed line (zorder=1)\n",
    "    for diff_type in order:\n",
    "        mean_val = diff_long.loc[\n",
    "            diff_long[\"Diff_Type\"] == diff_type, \"Difference\"\n",
    "        ].mean()\n",
    "        ax.axvline(\n",
    "            x=mean_val,\n",
    "            color=palette[diff_type],\n",
    "            linestyle=\":\",\n",
    "            linewidth=2,\n",
    "            alpha=0.8,\n",
    "            zorder=1\n",
    "        )\n",
    "\n",
    "    # 4) Draw the zero reference line BEFORE the violins, at the lowest zorder\n",
    "    ax.axvline(0, ls=\"--\", color=\"gray\", lw=1, zorder=0)\n",
    "\n",
    "    # 5) Now draw violins on top (zorder=2)\n",
    "    sns.violinplot(\n",
    "        data=diff_long,\n",
    "        x=\"Difference\",\n",
    "        y=\"Diff_Type\",\n",
    "        order=order,\n",
    "        inner=\"box\",\n",
    "        palette=palette,\n",
    "        zorder=2,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # 6) Title, labels, ticks\n",
    "    ax.set_title(\"Paired Spearman Correlation Differences\\n(Evolutionary - Experimental)\", \n",
    "                 fontsize=16, fontweight=\"bold\", pad=15)\n",
    "    ax.set_xlabel(\"Difference in Spearman Correlation\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.set_yticklabels(order, fontsize=12)\n",
    "\n",
    "    # 7) Wilcoxon tests and pvalue annotations\n",
    "    stat_results = {}\n",
    "    for diff_type in order:\n",
    "        group = diff_long.loc[diff_long[\"Diff_Type\"] == diff_type, \"Difference\"]\n",
    "        try:\n",
    "            stat, p = wilcoxon(group - 0)\n",
    "        except ValueError:\n",
    "            stat, p = np.nan, np.nan\n",
    "        stat_results[diff_type] = (stat, p)\n",
    "\n",
    "        median_val = group.median()\n",
    "        y_pos = order.index(diff_type)\n",
    "        ax.text(\n",
    "            median_val,\n",
    "            y_pos + 0.25,\n",
    "            f\"p = {p:.3f}\" if not np.isnan(p) else \"n.s.\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=12\n",
    "        )\n",
    "\n",
    "    # 8) Caption\n",
    "    fig.text(\n",
    "        0.5, 0.02, \n",
    "        \"P-values (from Wilcoxon signed-rank tests) indicate whether the median difference\\nis significantly different from 0.\",\n",
    "        ha=\"center\", fontsize=10, style=\"italic\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    return fig, stat_results\n",
    "# -----------------------------\n",
    "# Main driver function\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Set your data directory \n",
    "    DATA_DIR = ''  \n",
    "    \n",
    "    corr_df = process_data(DATA_DIR)\n",
    "    if corr_df.empty:\n",
    "        print(\"No valid data found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nPer-protein Spearman correlations:\")\n",
    "    print(corr_df.groupby(\"Protein\").size())\n",
    "    \n",
    "    # Compute differences per protein (4 differences per protein)\n",
    "    diff_long = compute_difference_df(corr_df)\n",
    "    print(\"\\nDifference data (first few rows):\")\n",
    "    print(diff_long.head())\n",
    "    \n",
    "    # Plot horizontal violin plots of the difference distributions and perform significance tests\n",
    "    fig, stat_results = plot_difference_violins(diff_long)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print test results to the console\n",
    "    print(\"\\nWilcoxon test results for each difference type:\")\n",
    "    for diff_type, (stat, p) in stat_results.items():\n",
    "        print(f\"{diff_type}: statistic = {stat:.3f}, p-value = {p:.3e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ee53a",
   "metadata": {},
   "source": [
    "Plotting for 20F set of proteins supplimental figures 21-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages \n",
    "\n",
    "########################################\n",
    "# 1) BASIC SETUP AND HELPER FUNCTIONS  #\n",
    "########################################\n",
    "\n",
    "def read_frustration_file(filepath, file_type='summary'):\n",
    "    \"\"\"\n",
    "    Read and process frustration data from a summary file containing both REP1 and REP2.\n",
    "    Now modified to work with CSV input.\n",
    "    \"\"\"\n",
    "    if file_type == 'summary':\n",
    "        \n",
    "        df = pd.read_csv(filepath, na_values=['n/a'])\n",
    "        \n",
    "        # Extract REP1 data\n",
    "        rep1_columns = {\n",
    "            'AlnIndex': 'AlnIndex',\n",
    "            'Residue': 'Residue',\n",
    "            'SecondaryStructure_EXP_FRUST_1': 'SecondaryStructure',\n",
    "            'B_FACTOR_1': 'B_Factor',\n",
    "            'EXP_FRUST_1': 'ExpFrust'\n",
    "        }\n",
    "        rep1_present_cols = [col for col in rep1_columns.keys() if col in df.columns]\n",
    "        rep1_df = df[rep1_present_cols].rename(columns={k: v for k, v in rep1_columns.items() if k in rep1_present_cols})\n",
    "        for v in rep1_columns.values():\n",
    "            if v not in rep1_df.columns:\n",
    "                rep1_df[v] = np.nan\n",
    "        \n",
    "        # Extract REP2 data\n",
    "        rep2_columns = {\n",
    "            'AlnIndex': 'AlnIndex',\n",
    "            'Residue': 'Residue',\n",
    "            'SecondaryStructure_EXP_FRUST_2': 'SecondaryStructure',\n",
    "            'B_FACTOR_2': 'B_Factor',\n",
    "            'EXP_FRUST_2': 'ExpFrust'\n",
    "        }\n",
    "        rep2_present_cols = [col for col in rep2_columns.keys() if col in df.columns]\n",
    "        rep2_df = df[rep2_present_cols].rename(columns={k: v for k, v in rep2_columns.items() if k in rep2_present_cols})\n",
    "        for v in rep2_columns.values():\n",
    "            if v not in rep2_df.columns:\n",
    "                rep2_df[v] = np.nan\n",
    "        \n",
    "        # Extract evolutionary frustration\n",
    "        if 'EVOL_FRUST' in df.columns:\n",
    "            evol_frust = pd.to_numeric(df['EVOL_FRUST'], errors='coerce')\n",
    "        else:\n",
    "            evol_frust = pd.Series([np.nan]*len(df))\n",
    "        \n",
    "        numeric_cols = ['B_Factor', 'ExpFrust']\n",
    "        for col in numeric_cols:\n",
    "            rep1_df[col] = pd.to_numeric(rep1_df[col], errors='coerce')\n",
    "            rep2_df[col] = pd.to_numeric(rep2_df[col], errors='coerce')\n",
    "        \n",
    "        evol_frust = pd.to_numeric(evol_frust, errors='coerce')\n",
    "        \n",
    "        return rep1_df, rep2_df, evol_frust\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only 'summary' is supported.\")\n",
    "\n",
    "def lowess_smoothing(x, y, frac=0.1, it=3):\n",
    "    \"\"\"\n",
    "    Apply LOWESS smoothing to the data.\n",
    "    \"\"\"\n",
    "    mask = ~(pd.isna(x) | pd.isna(y))\n",
    "    x_clean = x[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    if len(x_clean) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    lowess = sm.nonparametric.lowess\n",
    "    z = lowess(y_clean, x_clean, frac=frac, it=it, return_sorted=False)\n",
    "    return x_clean, z\n",
    "\n",
    "def create_gradient_line(x, y, values, cmap, linestyle='-', linewidth=3):\n",
    "    \"\"\"\n",
    "    Create a gradient line as a collection of segments.\n",
    "    \"\"\"\n",
    "    if len(x) < 2:\n",
    "        return None\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    lc = LineCollection(segments, cmap=cmap, linestyle=linestyle, linewidth=linewidth)\n",
    "    lc.set_array(values[:-1])\n",
    "    return lc\n",
    "\n",
    "def create_dashed_gradient_line(x, y, values, cmap, linewidth=3, dash_on=10, dash_off=5):\n",
    "    \"\"\"\n",
    "    Create a single dashed gradient line.\n",
    "    \"\"\"\n",
    "    if len(x) < 2:\n",
    "        return None\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    v = np.asarray(values)\n",
    "    \n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    seg_lengths = np.sqrt(dx*dx + dy*dy)\n",
    "    dist = np.concatenate(([0], np.cumsum(seg_lengths)))\n",
    "    \n",
    "    def color_at_distance(d_val):\n",
    "        return np.interp(d_val, dist, v)\n",
    "    \n",
    "    pattern_length = dash_on + dash_off\n",
    "    \n",
    "    def get_on_subsegments(s1, s2):\n",
    "        segments_on = []\n",
    "        current = s1\n",
    "        while current < s2:\n",
    "            cycle_pos = (current % pattern_length)\n",
    "            cycle_on_end = current - cycle_pos + dash_on\n",
    "            \n",
    "            if cycle_on_end <= current:\n",
    "                next_cycle_start = current - cycle_pos + pattern_length\n",
    "                current = next_cycle_start\n",
    "                continue\n",
    "            \n",
    "            seg_start = current\n",
    "            seg_end = min(cycle_on_end, s2)\n",
    "            if seg_end > seg_start:\n",
    "                segments_on.append((seg_start, seg_end))\n",
    "            \n",
    "            current = seg_end\n",
    "            cycle_off_end = current - (current % pattern_length) + pattern_length\n",
    "            if cycle_off_end < current:\n",
    "                cycle_off_end += pattern_length\n",
    "            \n",
    "            current = max(current, min(cycle_off_end, s2))\n",
    "        \n",
    "        return segments_on\n",
    "\n",
    "    all_on_segments = []\n",
    "    color_values = []\n",
    "    \n",
    "    for i in range(len(x) - 1):\n",
    "        s1 = dist[i]\n",
    "        s2 = dist[i+1]\n",
    "        if s2 == s1:\n",
    "            continue\n",
    "        \n",
    "        on_subs = get_on_subsegments(s1, s2)\n",
    "        if not on_subs:\n",
    "            continue\n",
    "        \n",
    "        for (s_on_start, s_on_end) in on_subs:\n",
    "            t1 = (s_on_start - s1) / (s2 - s1)\n",
    "            x1 = x[i] + t1 * (x[i+1] - x[i])\n",
    "            y1 = y[i] + t1 * (y[i+1] - y[i])\n",
    "            \n",
    "            t2 = (s_on_end - s1) / (s2 - s1)\n",
    "            x2 = x[i] + t2 * (x[i+1] - x[i])\n",
    "            y2 = y[i] + t2 * (y[i+1] - y[i])\n",
    "            \n",
    "            mid = 0.5*(s_on_start + s_on_end)\n",
    "            c_mid = color_at_distance(mid)\n",
    "            \n",
    "            all_on_segments.append([[x1, y1], [x2, y2]])\n",
    "            color_values.append(c_mid)\n",
    "    \n",
    "    if not all_on_segments:\n",
    "        return None\n",
    "    \n",
    "    lc = LineCollection(\n",
    "        all_on_segments,\n",
    "        cmap=cmap,\n",
    "        norm=plt.Normalize(v.min(), v.max()),\n",
    "        linewidth=linewidth,\n",
    "        linestyles='solid'\n",
    "    )\n",
    "    lc.set_array(np.array(color_values))\n",
    "    return lc\n",
    "\n",
    "def create_custom_cmap(vmin, vmax):\n",
    "    \"\"\"\n",
    "    Create a custom colormap that transitions through gray at zero.\n",
    "    \"\"\"\n",
    "    total = abs(vmin) + abs(vmax)\n",
    "    zero_pos = abs(vmin) / total if total != 0 else 0.5\n",
    "    colors = [\n",
    "        (0, '#0c1359'),\n",
    "        (zero_pos, '#D0D0D0'),\n",
    "        (1, '#f05b05')\n",
    "    ]\n",
    "    return LinearSegmentedColormap.from_list(\"custom\", colors, N=100)\n",
    "\n",
    "def read_binding_sites(exp_data_path):\n",
    "    \"\"\"\n",
    "    Read binding sites from binding_sites.txt in the experimental_data directory.\n",
    "    Returns a dictionary of residue numbers involved in binding.\n",
    "    \"\"\"\n",
    "    binding_sites_file = os.path.join(exp_data_path, \"binding_sites.txt\")\n",
    "    binding_residues = set()\n",
    "    \n",
    "    if not os.path.exists(binding_sites_file):\n",
    "        return binding_residues\n",
    "        \n",
    "    try:\n",
    "        with open(binding_sites_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.strip().startswith(('ALA', 'CYS', 'ASP', 'GLU', 'PHE', 'GLY', 'HIS', 'ILE', 'LYS', 'LEU', 'MET', 'ASN', 'PRO', 'GLN', 'ARG', 'SER', 'THR', 'VAL', 'TRP', 'TYR')):\n",
    "                    # Extract residue number - assuming format like \"ALA 123\"\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        try:\n",
    "                            residue_num = int(parts[1])\n",
    "                            binding_residues.add(residue_num)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading binding sites file: {e}\")\n",
    "    \n",
    "    return binding_residues\n",
    "    \n",
    "def create_helix(x_start, width, height=0.5, frequency=2):\n",
    "    \"\"\"\n",
    "    Create a helix representation for alpha helices.\n",
    "    \"\"\"\n",
    "    num_points = int(width * 20)\n",
    "    x = np.linspace(x_start, x_start + width, num_points)\n",
    "    y = height * np.sin(2 * np.pi * frequency * (x - x_start) / width)\n",
    "    return x, y\n",
    "\n",
    "def create_arrow(x_start, width, height=0.5):\n",
    "    \"\"\"\n",
    "    Create an arrow representation for beta sheets.\n",
    "    \"\"\"\n",
    "    x = [x_start, x_start,\n",
    "         x_start, x_start + 0.7*width,\n",
    "         x_start + 0.7*width, x_start + width,\n",
    "         x_start + 0.7*width,\n",
    "         x_start + 0.7*width, x_start,\n",
    "         x_start]\n",
    "    y = [-height/2, height/2,\n",
    "         -height/2, -height/2,\n",
    "         -height/2, 0,\n",
    "         height/2,\n",
    "         height/2, height/2,\n",
    "         -height/2]\n",
    "    return x, y\n",
    "\n",
    "def create_scatter_subplot(ax, x_data, y_data, color, title, xlabel, ylabel, marker='o'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot with rank correlation.\n",
    "    \"\"\"\n",
    "    # Ensure x_data and y_data are Series with a default integer index.\n",
    "    x_data = pd.Series(x_data).reset_index(drop=True)\n",
    "    y_data = pd.Series(y_data).reset_index(drop=True)\n",
    "    \n",
    "    # For evolutionary frustration data, filter out zeros.\n",
    "    if 'Evol' in title or 'Evolutionary' in title or 'Evolutionary' in ylabel:\n",
    "        mask = y_data != 0\n",
    "        x_data = x_data[mask].reset_index(drop=True)\n",
    "        y_data = y_data[mask].reset_index(drop=True)\n",
    "    \n",
    "    mask = ~(pd.isna(x_data) | pd.isna(y_data))\n",
    "    x_clean = x_data[mask]\n",
    "    y_clean = y_data[mask]\n",
    "    \n",
    "    if len(x_clean) < 2:\n",
    "        ax.text(0.5, 0.5, \"Insufficient data\", \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title, fontsize=16, pad=20)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        x_rank = x_clean.rank()\n",
    "        y_rank = y_clean.rank()\n",
    "        rho, pval = spearmanr(x_clean, y_clean)\n",
    "        sns.scatterplot(x=x_rank, y=y_rank, ax=ax, color=color, alpha=0.6, marker=marker, linewidth=2, s=100)\n",
    "        if len(x_rank.unique()) > 1 and len(y_rank.unique()) > 1:\n",
    "            sns.regplot(x=x_rank, y=y_rank, ax=ax, scatter=False, \n",
    "                       color='gray', line_kws={'linestyle': '--', 'alpha': 0.8})\n",
    "        corr_text = f\" = {rho:.3f}\\np = {pval:.2e}\"\n",
    "        ax.text(0.05, 0.95, corr_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', fontsize=12, color='black',\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        ax.set_title(title, fontsize=16, pad=20)\n",
    "        ax.set_xlabel(xlabel, fontsize=14)\n",
    "        ax.set_ylabel(ylabel, fontsize=14)\n",
    "        ax.tick_params(labelsize=12)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in scatter plot creation: {e}\")\n",
    "        ax.text(0.5, 0.5, \"Error in plot creation\", \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "########################################\n",
    "# 2) MAIN PLOTTING FUNCTION           #\n",
    "########################################\n",
    "\n",
    "def plot_frustration_comparison(summary_filepath, \n",
    "                                box_height_ratio=0.05, \n",
    "                                spacing_ratio=0.075, \n",
    "                                additional_space_ratio=0.30, \n",
    "                                box_padding_ratio=0.02, \n",
    "                                legend_separation_ratio=-0.05):\n",
    "    \"\"\"\n",
    "    Create a comprehensive plot comparing protein frustration data for REP1 and REP2.\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (20, 50),\n",
    "        'font.size': 14,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16\n",
    "    })\n",
    "    \n",
    "    rep1_data, rep2_data, evol_frust = read_frustration_file(summary_filepath)\n",
    "    fig = plt.figure(figsize=(20, 50))\n",
    "    # Update the grid to 9 rows (adding two new full-width rows for the Spearman scatter plots)\n",
    "    gs = gridspec.GridSpec(9, 2, \n",
    "                          height_ratios=[3, 2, 2, 2, 3, 2, 2, 2, 2],\n",
    "                          width_ratios=[1, 1], \n",
    "                          hspace=0.4, \n",
    "                          wspace=0.3)\n",
    "                          \n",
    "    ax_summary = fig.add_subplot(gs[4, :])\n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    has_ss = 'SecondaryStructure' in rep1_data.columns and 'SecondaryStructure' in rep2_data.columns\n",
    "\n",
    "    merged_data = rep1_data.merge(rep2_data, on='AlnIndex', suffixes=('_REP1', '_REP2'))\n",
    "    merged_data['EvolFrust'] = evol_frust\n",
    "    # Reset the index so that later boolean masks align properly.\n",
    "    merged_data = merged_data.reset_index(drop=True)\n",
    "    \n",
    "    complete_data_mask = (\n",
    "        ~merged_data['ExpFrust_REP1'].isna() &\n",
    "        ~merged_data['ExpFrust_REP2'].isna() &\n",
    "        ~merged_data['EvolFrust'].isna() &\n",
    "        ~merged_data['B_Factor_REP1'].isna() &\n",
    "        ~merged_data['B_Factor_REP2'].isna()\n",
    "    )\n",
    "    \n",
    "    merged_data_filtered = merged_data[complete_data_mask].reset_index(drop=True)\n",
    "    if merged_data_filtered.empty or len(merged_data_filtered) < 5:\n",
    "        raise ValueError(\"Insufficient complete data to generate plot.\")\n",
    "    \n",
    "    rep1_x_exp, rep1_smooth_exp = lowess_smoothing(merged_data_filtered['AlnIndex'], \n",
    "                                                merged_data_filtered['ExpFrust_REP1'])\n",
    "    rep2_x_exp, rep2_smooth_exp = lowess_smoothing(merged_data_filtered['AlnIndex'], \n",
    "                                                merged_data_filtered['ExpFrust_REP2'])\n",
    "    evol_x, evol_smooth = lowess_smoothing(merged_data_filtered['AlnIndex'], \n",
    "                                          merged_data_filtered['EvolFrust'])\n",
    "    \n",
    "    rep1_x_bf, rep1_smooth_bf = lowess_smoothing(merged_data_filtered['AlnIndex'], \n",
    "                                                merged_data_filtered['B_Factor_REP1'])\n",
    "    rep2_x_bf, rep2_smooth_bf = lowess_smoothing(merged_data_filtered['AlnIndex'], \n",
    "                                                merged_data_filtered['B_Factor_REP2'])\n",
    "    \n",
    "    default_y_min, default_y_max = -2, 2\n",
    "    all_y = np.concatenate([rep1_smooth_exp, rep2_smooth_exp, evol_smooth])\n",
    "    finite_mask = np.isfinite(all_y)\n",
    "    \n",
    "    try:\n",
    "        if np.any(finite_mask):\n",
    "            y_min = float(np.nanmin(all_y[finite_mask]))\n",
    "            y_max = float(np.nanmax(all_y[finite_mask]))\n",
    "            if not (np.isfinite(y_min) and np.isfinite(y_max)):\n",
    "                y_min, y_max = default_y_min, default_y_max\n",
    "        else:\n",
    "            y_min, y_max = default_y_min, default_y_max\n",
    "            \n",
    "        y_range = y_max - y_min\n",
    "        y_padding = y_range * 0.05\n",
    "        plot_y_min = y_min - y_padding\n",
    "        plot_y_max = y_max + y_padding + additional_space_ratio * y_range\n",
    "        \n",
    "        if legend_separation_ratio < 0:\n",
    "            plot_y_min += y_range * legend_separation_ratio\n",
    "        elif legend_separation_ratio > 0:\n",
    "            plot_y_max += y_range * legend_separation_ratio\n",
    "        \n",
    "        if not (np.isfinite(plot_y_min) and np.isfinite(plot_y_max)):\n",
    "            plot_y_min, plot_y_max = default_y_min, default_y_max\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating plot limits: {e}\")\n",
    "        plot_y_min, plot_y_max = default_y_min, default_y_max\n",
    "\n",
    "    if has_ss:\n",
    "        y_max_extended = plot_y_max + (y_max - y_min) * additional_space_ratio\n",
    "        ax_main.set_ylim(plot_y_min, y_max_extended)\n",
    "        ss_colors = {\n",
    "            'A': '#800080',\n",
    "            'B': '#008080',\n",
    "            'O': '#808080'\n",
    "        }\n",
    "        box_height = y_range * box_height_ratio\n",
    "        spacing = y_range * spacing_ratio\n",
    "        box_padding = y_range * box_padding_ratio\n",
    "        \n",
    "        rep1_box_bottom = y_max_extended - spacing - box_height  \n",
    "        rep2_box_bottom = rep1_box_bottom - spacing - box_height  \n",
    "        \n",
    "        ax_main.add_patch(Rectangle((merged_data_filtered['AlnIndex'].min(), rep1_box_bottom),\n",
    "                                     merged_data_filtered['AlnIndex'].max() - merged_data_filtered['AlnIndex'].min(),\n",
    "                                     box_height + box_padding,\n",
    "                                     facecolor='#f5f5f5',\n",
    "                                     edgecolor='#d3d3d3',\n",
    "                                     alpha=1.0,\n",
    "                                     zorder=2))\n",
    "        \n",
    "        ax_main.add_patch(Rectangle((merged_data_filtered['AlnIndex'].min(), rep2_box_bottom),\n",
    "                                     merged_data_filtered['AlnIndex'].max() - merged_data_filtered['AlnIndex'].min(),\n",
    "                                     box_height + box_padding,\n",
    "                                     facecolor='#f5f5f5',\n",
    "                                     edgecolor='#d3d3d3',\n",
    "                                     alpha=1.0,\n",
    "                                     zorder=2))\n",
    "        \n",
    "        ss_symbol_height = box_height / 2  \n",
    "        prev_ss_rep1 = None\n",
    "        start_idx_rep1 = None\n",
    "        \n",
    "        for idx, ss in zip(merged_data_filtered['AlnIndex'], merged_data_filtered['SecondaryStructure_REP1']):\n",
    "            if pd.isna(ss):\n",
    "                continue\n",
    "            if ss != prev_ss_rep1:\n",
    "                if prev_ss_rep1 is not None:\n",
    "                    width = idx - start_idx_rep1\n",
    "                    if width <= 0:\n",
    "                        width = 1\n",
    "                    if prev_ss_rep1 == 'A':\n",
    "                        x_helix, y_helix = create_helix(start_idx_rep1, width, height=ss_symbol_height)\n",
    "                        ax_main.plot(x_helix, y_helix + rep1_box_bottom + (box_height + box_padding)/2, \n",
    "                                    color=ss_colors.get('A', '#800080'), linewidth=2, zorder=4)\n",
    "                    elif prev_ss_rep1 == 'B':\n",
    "                        x_arrow, y_arrow = create_arrow(start_idx_rep1, width, height=ss_symbol_height)\n",
    "                        ax_main.plot(x_arrow, np.array(y_arrow) + rep1_box_bottom + (box_height + box_padding)/2, \n",
    "                                    color=ss_colors.get('B', '#008080'), linewidth=2, zorder=4)\n",
    "                    else:\n",
    "                        ax_main.plot([start_idx_rep1, idx], \n",
    "                                    [rep1_box_bottom + (box_height + box_padding)/2, rep1_box_bottom + (box_height + box_padding)/2], \n",
    "                                    color=ss_colors.get('O', '#808080'), linewidth=1, zorder=4)\n",
    "                start_idx_rep1 = idx\n",
    "                prev_ss_rep1 = ss\n",
    "        \n",
    "        if prev_ss_rep1 is not None:\n",
    "            width = merged_data_filtered['AlnIndex'].iloc[-1] - start_idx_rep1 + 1\n",
    "            if width <= 0:\n",
    "                width = 1\n",
    "            if prev_ss_rep1 == 'A':\n",
    "                x_helix, y_helix = create_helix(start_idx_rep1, width, height=ss_symbol_height)\n",
    "                ax_main.plot(x_helix, y_helix + rep1_box_bottom + (box_height + box_padding)/2, \n",
    "                            color=ss_colors.get('A', '#800080'), linewidth=2, zorder=4)\n",
    "            elif prev_ss_rep1 == 'B':\n",
    "                x_arrow, y_arrow = create_arrow(start_idx_rep1, width, height=ss_symbol_height)\n",
    "                ax_main.plot(x_arrow, np.array(y_arrow) + rep1_box_bottom + (box_height + box_padding)/2, \n",
    "                            color=ss_colors.get('B', '#008080'), linewidth=2, zorder=4)\n",
    "            else:\n",
    "                ax_main.plot([start_idx_rep1, merged_data_filtered['AlnIndex'].iloc[-1]], \n",
    "                            [rep1_box_bottom + (box_height + box_padding)/2, rep1_box_bottom + (box_height + box_padding)/2], \n",
    "                            color=ss_colors.get('O', '#808080'), linewidth=1, zorder=4)\n",
    "        \n",
    "        prev_ss_rep2 = None\n",
    "        start_idx_rep2 = None\n",
    "        \n",
    "        for idx, ss in zip(merged_data_filtered['AlnIndex'], merged_data_filtered['SecondaryStructure_REP2']):\n",
    "            if pd.isna(ss):\n",
    "                continue\n",
    "            if ss != prev_ss_rep2:\n",
    "                if prev_ss_rep2 is not None:\n",
    "                    width = idx - start_idx_rep2\n",
    "                    if width <= 0:\n",
    "                        width = 1\n",
    "                    if prev_ss_rep2 == 'A':\n",
    "                        x_helix, y_helix = create_helix(start_idx_rep2, width, height=ss_symbol_height)\n",
    "                        ax_main.plot(x_helix, y_helix + rep2_box_bottom + (box_height + box_padding)/2, \n",
    "                                    color=ss_colors.get('A', '#800080'), linewidth=2, zorder=3)\n",
    "                    elif prev_ss_rep2 == 'B':\n",
    "                        x_arrow, y_arrow = create_arrow(start_idx_rep2, width, height=ss_symbol_height)\n",
    "                        ax_main.plot(x_arrow, np.array(y_arrow) + rep2_box_bottom + (box_height + box_padding)/2, \n",
    "                                    color=ss_colors.get('B', '#008080'), linewidth=2, zorder=3)\n",
    "                    else:\n",
    "                        ax_main.plot([start_idx_rep2, idx], \n",
    "                                    [rep2_box_bottom + (box_height + box_padding)/2, rep2_box_bottom + (box_height + box_padding)/2], \n",
    "                                    color=ss_colors.get('O', '#808080'), linewidth=1, zorder=3)\n",
    "                start_idx_rep2 = idx\n",
    "                prev_ss_rep2 = ss\n",
    "        \n",
    "        if prev_ss_rep2 is not None:\n",
    "            width = merged_data_filtered['AlnIndex'].iloc[-1] - start_idx_rep2 + 1\n",
    "            if width <= 0:\n",
    "                width = 1\n",
    "            if prev_ss_rep2 == 'A':\n",
    "                x_helix, y_helix = create_helix(start_idx_rep2, width, height=ss_symbol_height)\n",
    "                ax_main.plot(x_helix, y_helix + rep2_box_bottom + (box_height + box_padding)/2, \n",
    "                            color=ss_colors.get('A', '#800080'), linewidth=2, zorder=3)\n",
    "            elif prev_ss_rep2 == 'B':\n",
    "                x_arrow, y_arrow = create_arrow(start_idx_rep2, width, height=ss_symbol_height)\n",
    "                ax_main.plot(x_arrow, np.array(y_arrow) + rep2_box_bottom + (box_height + box_padding)/2, \n",
    "                            color=ss_colors.get('B', '#008080'), linewidth=2, zorder=3)\n",
    "            else:\n",
    "                ax_main.plot([start_idx_rep2, merged_data_filtered['AlnIndex'].iloc[-1]], \n",
    "                            [rep2_box_bottom + (box_height + box_padding)/2, rep2_box_bottom + (box_height + box_padding)/2], \n",
    "                            color=ss_colors.get('O', '#808080'), linewidth=1, zorder=3)\n",
    "        \n",
    "        x_min = merged_data_filtered['AlnIndex'].min()\n",
    "        x_max = merged_data_filtered['AlnIndex'].max()\n",
    "        x_label = x_min + 0.02 * (x_max - x_min)\n",
    "        legend_y = 0.02  \n",
    "        \n",
    "        ax_main.text(x_label, rep1_box_bottom + box_height + box_padding, 'REP1', \n",
    "                     horizontalalignment='left', verticalalignment='bottom', fontsize=14, color='black', rotation=0, zorder=5)\n",
    "        ax_main.text(x_label, rep2_box_bottom + box_height + box_padding, 'REP2', \n",
    "                     horizontalalignment='left', verticalalignment='bottom', fontsize=14, color='black', rotation=0, zorder=5)\n",
    "    else:\n",
    "        ax_main.set_ylim(plot_y_min, plot_y_max)\n",
    "\n",
    "    cmap_rep1_exp = create_custom_cmap(rep1_smooth_exp.min(), rep1_smooth_exp.max())\n",
    "    cmap_rep2_exp = create_custom_cmap(rep2_smooth_exp.min(), rep2_smooth_exp.max())\n",
    "    cmap_evol = create_custom_cmap(evol_smooth.min(), evol_smooth.max())\n",
    "\n",
    "    rep1_line_exp = create_gradient_line(\n",
    "        rep1_x_exp, rep1_smooth_exp, rep1_smooth_exp, cmap_rep1_exp,\n",
    "        linestyle='-', linewidth=4\n",
    "    )\n",
    "    if rep1_line_exp:\n",
    "        ax_main.add_collection(rep1_line_exp)\n",
    "\n",
    "    rep2_line_exp = create_dashed_gradient_line(\n",
    "        rep2_x_exp, rep2_smooth_exp, rep2_smooth_exp, cmap_rep2_exp,\n",
    "        linewidth=4, dash_on=2, dash_off=2\n",
    "    )\n",
    "    if rep2_line_exp:\n",
    "        ax_main.add_collection(rep2_line_exp)\n",
    "\n",
    "    evol_line = create_gradient_line(\n",
    "        evol_x, evol_smooth, evol_smooth, cmap_evol,\n",
    "        linestyle=':', linewidth=2\n",
    "    )\n",
    "    if evol_line:\n",
    "        ax_main.add_collection(evol_line)\n",
    "\n",
    "    x_min = merged_data_filtered['AlnIndex'].min()\n",
    "    x_max = merged_data_filtered['AlnIndex'].max()\n",
    "    ax_main.set_xlim(x_min, x_max)\n",
    "\n",
    "    ax_main.set_title('Protein Frustration Comparison: REP1 vs REP2', \n",
    "                      fontsize=24, fontweight='bold', pad=20)\n",
    "    ax_main.set_xlabel('Residue Number', fontsize=20, fontweight='bold')\n",
    "    ax_main.set_ylabel('Frustration', fontsize=20, fontweight='bold')\n",
    "\n",
    "    legends = []\n",
    "    line_style_legend = [\n",
    "        Line2D([0], [0], color='black', linestyle='-', linewidth=4, label='REP1 Experimental'),\n",
    "        Line2D([0], [0], color='black', linestyle='--', linewidth=4, label='REP2 Experimental'),\n",
    "        Line2D([0], [0], color='black', linestyle=':', linewidth=2, label='Evolutionary Frustration')\n",
    "    ]\n",
    "    legends.append(('Frustration Types', line_style_legend))\n",
    "\n",
    "    frustration_legend = [\n",
    "        Line2D([0], [0], color='#0c1359', label='Minimally Frustrated', linewidth=3),\n",
    "        Line2D([0], [0], color='#D0D0D0', label='Neutral', linewidth=3),\n",
    "        Line2D([0], [0], color='#f05b05', label='Highly Frustrated', linewidth=3)\n",
    "    ]\n",
    "    legends.append(('Frustration Level', frustration_legend))\n",
    "    \n",
    "    num_legends = len(legends)\n",
    "    spacing = 1.0 / (num_legends + 1)\n",
    "    \n",
    "    for i, (title, handles) in enumerate(legends):\n",
    "        x_pos = spacing * (i + 1)\n",
    "        legend = ax_main.legend(\n",
    "            handles=handles,\n",
    "            title=title,\n",
    "            fontsize=14,\n",
    "            title_fontsize=16,\n",
    "            loc='lower center',\n",
    "            bbox_to_anchor=(x_pos, 0.02),\n",
    "            frameon=True,\n",
    "            ncol=1\n",
    "        )\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "        legend.get_frame().set_edgecolor('black')\n",
    "        ax_main.add_artist(legend)\n",
    "\n",
    "    category_colors = {\n",
    "        'Experimental Frustration Rep1': '#8B0000',\n",
    "        'Experimental Frustration Rep2': '#FF4444',\n",
    "        'Evolutionary Frustration': '#4DAF4A'\n",
    "    }\n",
    "    marker_styles = {\n",
    "        'REP1 B-Factor': 'o',\n",
    "        'REP2 B-Factor': 'x'\n",
    "    }\n",
    "\n",
    "    #  scatter plots \n",
    "    # left column (REP1 B-Factor)\n",
    "    mask_evol = merged_data['EvolFrust'] != 0\n",
    "    temp_evol = merged_data.loc[mask_evol].reset_index(drop=True)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[1, 0])\n",
    "    create_scatter_subplot(\n",
    "        ax1,\n",
    "        merged_data['B_Factor_REP1'],\n",
    "        merged_data['ExpFrust_REP1'],\n",
    "        category_colors['Experimental Frustration Rep1'],\n",
    "        'REP1 Experimental vs REP1 B-Factor',\n",
    "        'REP1 B-Factor Rank',\n",
    "        'REP1 Exp. Frustration Rank',\n",
    "        marker=marker_styles['REP1 B-Factor']\n",
    "    )\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[2, 0])\n",
    "    create_scatter_subplot(\n",
    "        ax2,\n",
    "        merged_data['B_Factor_REP1'],\n",
    "        merged_data['ExpFrust_REP2'],\n",
    "        category_colors['Experimental Frustration Rep2'],\n",
    "        'REP2 Experimental vs REP1 B-Factor',\n",
    "        'REP1 B-Factor Rank',\n",
    "        'REP2 Exp. Frustration Rank',\n",
    "        marker=marker_styles['REP2 B-Factor']\n",
    "    )\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[3, 0])\n",
    "    create_scatter_subplot(\n",
    "        ax3,\n",
    "        temp_evol['B_Factor_REP1'],\n",
    "        temp_evol['EvolFrust'],\n",
    "        category_colors['Evolutionary Frustration'],\n",
    "        'Evolutionary Frustration vs REP1 B-Factor',\n",
    "        'REP1 B-Factor Rank',\n",
    "        'Evolutionary Frustration Rank',\n",
    "        marker='^'\n",
    "    )\n",
    "\n",
    "    # right column (REP2 B-Factor)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    create_scatter_subplot(\n",
    "        ax4,\n",
    "        merged_data['B_Factor_REP2'],\n",
    "        merged_data['ExpFrust_REP1'],\n",
    "        category_colors['Experimental Frustration Rep1'],\n",
    "        'REP1 Experimental vs REP2 B-Factor',\n",
    "        'REP2 B-Factor Rank',\n",
    "        'REP1 Exp. Frustration Rank',\n",
    "        marker=marker_styles['REP1 B-Factor']\n",
    "    )\n",
    "\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    create_scatter_subplot(\n",
    "        ax5,\n",
    "        merged_data['B_Factor_REP2'],\n",
    "        merged_data['ExpFrust_REP2'],\n",
    "        category_colors['Experimental Frustration Rep2'],\n",
    "        'REP2 Experimental vs REP2 B-Factor',\n",
    "        'REP2 B-Factor Rank',\n",
    "        'REP2 Exp. Frustration Rank',\n",
    "        marker=marker_styles['REP2 B-Factor']\n",
    "    )\n",
    "\n",
    "    ax6 = fig.add_subplot(gs[3, 1])\n",
    "    create_scatter_subplot(\n",
    "        ax6,\n",
    "        temp_evol['B_Factor_REP2'],\n",
    "        temp_evol['EvolFrust'],\n",
    "        category_colors['Evolutionary Frustration'],\n",
    "        'Evolutionary Frustration vs REP2 B-Factor',\n",
    "        'REP2 B-Factor Rank',\n",
    "        'Evolutionary Frustration Rank',\n",
    "        marker='^'\n",
    "    )\n",
    "    # 4. Normalized Smoothed B-Factor Plot (Full Width)\n",
    "    ax_bfactor_normalized = fig.add_subplot(gs[5, :])\n",
    "    def normalize_series(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val == 0:\n",
    "            return pd.Series([0.5] * len(series), index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "    rep1_normalized = normalize_series(pd.Series(rep1_smooth_bf, index=rep1_x_bf.index))\n",
    "    rep2_normalized = normalize_series(pd.Series(rep2_smooth_bf, index=rep2_x_bf.index))\n",
    "\n",
    "    ax_bfactor_normalized.plot(rep1_x_bf, rep1_normalized, label='REP1 Normalized B-Factor', color='blue', linewidth=2)\n",
    "    ax_bfactor_normalized.plot(rep2_x_bf, rep2_normalized, label='REP2 Normalized B-Factor', color='orange', linewidth=2)\n",
    "    ax_bfactor_normalized.set_title('Normalized Smoothed B-Factor Comparison: REP1 vs REP2', fontsize=18, pad=20)\n",
    "    ax_bfactor_normalized.set_xlabel('Residue Number', fontsize=14)\n",
    "    ax_bfactor_normalized.set_ylabel('Normalized B-Factor', fontsize=14)\n",
    "    ax_bfactor_normalized.legend(loc='upper right', fontsize=12)\n",
    "    ax_bfactor_normalized.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. B-Factor Ranks Scatter Plot (Full Width)\n",
    "    ax_bfactor_rank_scatter = fig.add_subplot(gs[6, :])\n",
    "    rep1_bfactor_rank = merged_data_filtered['B_Factor_REP1'].rank()\n",
    "    rep2_bfactor_rank = merged_data_filtered['B_Factor_REP2'].rank()\n",
    "    rho_bfactor, pval_bfactor = spearmanr(rep1_bfactor_rank, rep2_bfactor_rank)\n",
    "    sns.scatterplot(x=rep1_bfactor_rank, y=rep2_bfactor_rank, ax=ax_bfactor_rank_scatter,\n",
    "                    color='purple', alpha=0.6, marker='D', edgecolor='black', linewidth=0.5, s=100)\n",
    "    if len(rep1_bfactor_rank.unique()) > 1 and len(rep2_bfactor_rank.unique()) > 1:\n",
    "        sns.regplot(x=rep1_bfactor_rank, y=rep2_bfactor_rank, ax=ax_bfactor_rank_scatter, scatter=False, \n",
    "                   color='gray', line_kws={'linestyle': '--', 'alpha': 0.8})\n",
    "    corr_text_bfactor = f\" = {rho_bfactor:.3f}\\np = {pval_bfactor:.2e}\"\n",
    "    ax_bfactor_rank_scatter.text(0.05, 0.95, corr_text_bfactor, transform=ax_bfactor_rank_scatter.transAxes,\n",
    "                                  verticalalignment='top', fontsize=12, color='black',\n",
    "                                  bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    ax_bfactor_rank_scatter.set_title('B-Factor Rank Comparison: REP1 vs REP2', fontsize=18, pad=20)\n",
    "    ax_bfactor_rank_scatter.set_xlabel('REP1 B-Factor Rank', fontsize=14)\n",
    "    ax_bfactor_rank_scatter.set_ylabel('REP2 B-Factor Rank', fontsize=14)\n",
    "    legend_elements_bfactor = [\n",
    "        Line2D([0], [0], marker='D', color='w', markerfacecolor='purple', markersize=10, label='B-Factor Ranks')\n",
    "    ]\n",
    "    ax_bfactor_rank_scatter.legend(handles=legend_elements_bfactor, loc='upper right', fontsize=12)\n",
    "    ax_bfactor_rank_scatter.grid(True, alpha=0.3)\n",
    "\n",
    "    # NEW: Two Spearman Scatter Plots comparing Experimental Frustration to Evolutionary Frustration\n",
    "    mask_evol_filtered = merged_data_filtered['EvolFrust'] != 0\n",
    "    temp_evol3 = merged_data_filtered.loc[mask_evol_filtered].reset_index(drop=True)\n",
    "    ax_spearman_rep1 = fig.add_subplot(gs[7, :])\n",
    "    create_scatter_subplot(ax_spearman_rep1,\n",
    "                           temp_evol3['ExpFrust_REP1'],\n",
    "                           temp_evol3['EvolFrust'],\n",
    "                           category_colors['Experimental Frustration Rep1'],\n",
    "                           'Evolutionary Frustration vs REP1 Experimental',\n",
    "                           'REP1 Experimental Frustration Rank',\n",
    "                           'Evolutionary Frustration Rank',\n",
    "                           marker='s')  # square marker\n",
    "\n",
    "    ax_spearman_rep2 = fig.add_subplot(gs[8, :])\n",
    "    create_scatter_subplot(ax_spearman_rep2,\n",
    "                           temp_evol3['ExpFrust_REP2'],\n",
    "                           temp_evol3['EvolFrust'],\n",
    "                           category_colors['Experimental Frustration Rep2'],\n",
    "                           'Evolutionary Frustration vs REP2 Experimental',\n",
    "                           'REP2 Experimental Frustration Rank',\n",
    "                           'Evolutionary Frustration Rank',\n",
    "                           marker='^')  # triangle marker\n",
    "\n",
    "    summary_correlations = []\n",
    "    metrics = {\n",
    "        'Experimental Frustration Rep1': merged_data_filtered['ExpFrust_REP1'],\n",
    "        'Experimental Frustration Rep2': merged_data_filtered['ExpFrust_REP2'],\n",
    "        'Evolutionary Frustration': merged_data_filtered['EvolFrust']\n",
    "    }\n",
    "    \n",
    "    b_factors = {\n",
    "        'REP1 B-Factor': merged_data_filtered['B_Factor_REP1'],\n",
    "        'REP2 B-Factor': merged_data_filtered['B_Factor_REP2']\n",
    "    }\n",
    "    \n",
    "    for metric_name, metric_series in metrics.items():\n",
    "        for b_factor_name, b_factor_series in b_factors.items():\n",
    "            # If metric is evolutionary frustration, use less strict filtering (AND INCLUDE ZEROS)\n",
    "            if metric_name == \"Evolutionary Frustration\":\n",
    "                # --- Start Modification ---\n",
    "                # Use original data, not the pre-filtered series\n",
    "                evol_series_orig = merged_data['EvolFrust'] # Use the full EvolFrust series including zeros\n",
    "\n",
    "                # Get the corresponding original B-factor series based on the loop variable 'b_factor_name'\n",
    "                if b_factor_name == 'REP1 B-Factor':\n",
    "                    b_factor_series_orig = merged_data['B_Factor_REP1']\n",
    "                else: # Assumes 'REP2 B-Factor'\n",
    "                    b_factor_series_orig = merged_data['B_Factor_REP2']\n",
    "\n",
    "                # ----> NO ZERO FILTERING <----\n",
    "                # We directly use evol_series_orig and b_factor_series_orig\n",
    "\n",
    "                # Let spearmanr handle the NaNs within the pair using nan_policy='omit'\n",
    "                try:\n",
    "                        # Pass the original series; nan_policy='omit' handles pairs with NaNs\n",
    "                        rho, pval = spearmanr(evol_series_orig, b_factor_series_orig, nan_policy='omit')\n",
    "\n",
    "                        # Check if rho is NaN (can happen if < 2 valid pairs after 'omit')\n",
    "                        if not np.isnan(rho):\n",
    "                            summary_correlations.append({\n",
    "                                'Metric': metric_name,\n",
    "                                'Spearman_rho': rho,\n",
    "                                'B_Factor': b_factor_name,\n",
    "                                'pval': pval\n",
    "                            })\n",
    "                        # else: spearmanr resulted in NaN (e.g., < 2 valid pairs), so don't append\n",
    "                except ValueError:\n",
    "                    # Should ideally be caught by nan_policy, but just in case\n",
    "                    pass # Do not append if spearmanr fails\n",
    "                # --- End Modification ---\n",
    "\n",
    "            else: # Original block for Experimental Frustration \n",
    "                # Reset index just in case\n",
    "                metric_series = metric_series.reset_index(drop=True)\n",
    "                b_factor_series = b_factor_series.reset_index(drop=True)\n",
    "                # Check for sufficient non-NaN pairs\n",
    "                valid_mask = ~metric_series.isna() & ~b_factor_series.isna()\n",
    "                if valid_mask.sum() >= 2:\n",
    "                    rho, pval = spearmanr(metric_series[valid_mask], b_factor_series[valid_mask])\n",
    "                    summary_correlations.append({\n",
    "                        'Metric': metric_name,\n",
    "                        'Spearman_rho': rho,\n",
    "                        'B_Factor': b_factor_name,\n",
    "                        'pval': pval\n",
    "                    })\n",
    "\n",
    "    x_positions = {\n",
    "        'Experimental Frustration Rep1': 1,\n",
    "        'Experimental Frustration Rep2': 2,\n",
    "        'Evolutionary Frustration': 3\n",
    "    }\n",
    "    \n",
    "    for corr in summary_correlations:\n",
    "        x = x_positions[corr['Metric']]\n",
    "        y = corr['Spearman_rho']\n",
    "        pval = corr['pval']\n",
    "        if corr['B_Factor'] == 'REP1 B-Factor':\n",
    "            marker = 'o'\n",
    "            color = category_colors[corr['Metric']]\n",
    "        else:\n",
    "            marker = 'x'\n",
    "            color = category_colors[corr['Metric']]\n",
    "        ax_summary.scatter(x, y, c=[color], marker=marker, s=200, linewidth=2)\n",
    "        if pval < 0.05:\n",
    "            ax_summary.scatter(x, y, facecolors='none', edgecolors='black', linewidth=2, s=500, marker='s', zorder=6)\n",
    "\n",
    "    ax_summary.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_summary.grid(True, alpha=0.3)\n",
    "    ax_summary.set_xlim(0.5, 3.5)\n",
    "    if summary_correlations:\n",
    "        y_min_corr = min(corr['Spearman_rho'] for corr in summary_correlations)\n",
    "        y_max_corr = max(corr['Spearman_rho'] for corr in summary_correlations)\n",
    "        y_padding_corr = (y_max_corr - y_min_corr) if (y_max_corr - y_min_corr) != 0 else 1\n",
    "    else:\n",
    "        y_min_corr, y_max_corr = -1, 1\n",
    "        y_padding_corr = 0.1\n",
    "    ax_summary.set_ylim(y_min_corr - y_padding_corr, y_max_corr + y_padding_corr)\n",
    "    ax_summary.set_xticks([1, 2, 3])\n",
    "    ax_summary.set_xticklabels(['Experimental Frustration Rep1', \n",
    "                                'Experimental Frustration Rep2', \n",
    "                                'Evolutionary Frustration'], \n",
    "                               fontsize=12, rotation=0, ha='center')\n",
    "    ax_summary.set_ylabel(\"Spearman's \", fontsize=16)\n",
    "    ax_summary.spines['top'].set_visible(False)\n",
    "    ax_summary.spines['right'].set_visible(False)\n",
    "    ax_summary.yaxis.set_ticks_position('left')\n",
    "    ax_summary.set_title('Summary of B-Factor Correlations', fontsize=18, pad=20)\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label='Spearman with REP1 B-Factor'),\n",
    "        Line2D([0], [0], marker='x', color='w', markeredgecolor='gray', markersize=10, label='Spearman with REP2 B-Factor'),\n",
    "        Line2D([0], [0], marker='s', color='w', markerfacecolor='none', markeredgecolor='black', markersize=12, label='p < 0.05')\n",
    "    ]\n",
    "    ax_summary.legend(handles=legend_elements, \n",
    "                     loc='lower right',\n",
    "                     fontsize=12,\n",
    "                     frameon=True,\n",
    "                     framealpha=0.9)\n",
    "    \n",
    "    for ax in [ax1, ax2, ax3, ax4, ax5, ax6]:\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "########################################\n",
    "# 3) PROCESSING ALL SUBDIRECTORIES     #\n",
    "########################################\n",
    "\n",
    "# 3) PROCESSING ALL SUBDIRECTORIES\n",
    "def process_all_subdirectories(root_dir,\n",
    "                               summary_filename=\"summary.csv\",\n",
    "                               box_height_ratio=0.1,\n",
    "                               spacing_ratio=0.15,\n",
    "                               additional_space_ratio=0.295,\n",
    "                               box_padding_ratio=0.05,\n",
    "                               legend_separation_ratio=-0.75):\n",
    "    \"\"\"\n",
    "    Iterate through all immediate subdirectories of 'root_dir'. For each subdirectory,\n",
    "    if a file named 'summary_filename' is found, generate a plot, number it S21S40,\n",
    "    and save both individual and aggregate PDFs.\n",
    "    \"\"\"\n",
    "    big_figures = []  # List to store each generated figure\n",
    "    all_corrs   = []  # List to store Spearman  for each protein (subdirectory)\n",
    "\n",
    "    # Start numbering supplemental figures at S21\n",
    "    figure_number = 21\n",
    "\n",
    "    for entry in os.listdir(root_dir):\n",
    "        subdir_path = os.path.join(root_dir, entry)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "\n",
    "        # remove any old \"frustration\" PDFs\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            if filename.lower().endswith('.pdf') and \"frustration\" in filename.lower():\n",
    "                try:\n",
    "                    os.remove(os.path.join(subdir_path, filename))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        summary_filepath = os.path.join(subdir_path, summary_filename)\n",
    "        if not os.path.exists(summary_filepath):\n",
    "            print(f\"Skipping '{entry}': summary file not found.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # generate the frustration comparison figure\n",
    "            fig = plot_frustration_comparison(\n",
    "                summary_filepath,\n",
    "                box_height_ratio=box_height_ratio,\n",
    "                spacing_ratio=spacing_ratio,\n",
    "                additional_space_ratio=additional_space_ratio,\n",
    "                box_padding_ratio=box_padding_ratio,\n",
    "                legend_separation_ratio=legend_separation_ratio\n",
    "            )\n",
    "\n",
    "            # give it a numbered suptitle, e.g. \"Figure S21. 1abcD\"\n",
    "            fig.suptitle(\n",
    "                f\"Figure S{figure_number}. {entry}\",\n",
    "                fontsize=20,\n",
    "                weight='bold',\n",
    "                y=0.9\n",
    "            )\n",
    "            figure_number += 1\n",
    "\n",
    "            # save the individual PDF in the subdirectory\n",
    "            output_filename = f\"{entry}_frustration_comparison.pdf\"\n",
    "            output_path = os.path.join(subdir_path, output_filename)\n",
    "            fig.savefig(output_path, dpi=600, bbox_inches='tight')\n",
    "            print(f\"Plot saved successfully at: {output_path}\")\n",
    "\n",
    "            big_figures.append(fig)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping '{entry}' due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    # save all individual figures into one big PDF\n",
    "    if big_figures:\n",
    "        all_plots_path = os.path.join(root_dir, \"all_plots.pdf\")\n",
    "        with PdfPages(all_plots_path) as pdf:\n",
    "            for fig in big_figures:\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "        print(f\"All plots saved in one PDF at: {all_plots_path}\")\n",
    "\n",
    "    \n",
    "    # ---- NEW: Create and save histogram of the Spearman correlations ----\n",
    "    if all_corrs:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(all_corrs, bins=10, edgecolor='black')\n",
    "        plt.xlabel(\"Spearman's \", fontsize=14)\n",
    "        plt.ylabel(\"Frequency\", fontsize=14)\n",
    "        plt.title(\"Histogram of Spearman Correlation Coefficients\\n(Evolutionary vs Experimental Frustration)\", fontsize=16)\n",
    "        histogram_path = os.path.join(root_dir, \"correlation_histogram.pdf\")\n",
    "        plt.savefig(histogram_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Histogram saved successfully at: {histogram_path}\")\n",
    "    else:\n",
    "        print(\"No valid correlations computed to plot histogram.\")\n",
    "\n",
    "########################################\n",
    "# 4) MAIN EXECUTION BLOCK              #\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the root directory that contains subdirectories to process.\n",
    "    root_directory = \"\"  \n",
    "    process_all_subdirectories(root_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
